# 目次

* [機械学習の課題](#機械学習の課題)
* [性能指標](#性能指標)
* [線形回帰モデル](#線形回帰モデル)
* [非線形回帰モデル](#非線形回帰モデル)
* [分析手法(主成分分析、ｋ近傍法、k-means)](#分析手法)
* [パターン認識](#パターン認識)

# 機械学習の課題
([目次に戻る](#目次))


## 機械学習の基本

- 機械学習（Machine Learning）とは、画像・言語・音声・数値などの様々なデータの背景に潜むルールやパターンを獲得する手法。
- コンピュータに大量のデータを「学習」させることでルールやパターンを獲得し、それらを基に未知のデータを予測・解析する。
- 機械学習の基本理解｜訓練誤差と汎化誤差

  - 訓練誤差（Training Loss）：学習（訓練）データに対する予測と正解の誤差。学習データに対してどの程度正確に予測できるかを示す値。

  - 汎化誤差（Generalization Loss）：未知のデータに対する予測と正解の誤差。未知のデータに対してどの程度正確に予測できるかを示す値。

## 課題

- 過剰適合（Overfitting）：機械学習モデルが学習データに過剰に適合し、学習データに対しては高い精度な一方、未知のデータに対しては十分な精
  度が得られない現象。学習データの数に対してモデルの能力が過剰に高い（パラメータ数が多い）場合に主に発生する。

  - 過剰適合の原因：データの分布に対してモデルの次元が大きすぎる（パラメータ数が大きすぎる）or データ数が少なすぎる場合

- 過少適合（Underfitting）：機械学習モデルが学習データに十分に適合できず、学習データおよび未知のデータに対して十分な精度が得られない現象。
  学習データの数に対して手法の能力が過剰に低い（パラメータ数が少ない）場合に主に発生する。
  ※未知のデータ対応できない！！

  - 過少適合の原因：データの分布に対してモデルの次元が小さすぎる（パラメータ数が少なすぎる）

- データ数が少ない場合：過少適合が発生し、汎化誤差も大きくなる。過剰適合は、適切なモデルの次元であっても発生し、汎化誤差も大きくなる。
- データ数が多い場合：過少適合は発生するが、汎化誤差はある程度小さくなる。過剰適合は、モデルの次元が過剰であっても抑制される。

- 正則化（Regularization）：機械学習モデルの過剰適合を抑制する方法。

  - モデルのパラメータが複雑になりすぎないように（学習データに特化しすぎないように）、制約を加えた状態で学習させる。正則化の方法は様々であり、解きたい問題や手法によって異なる。
  - パラメータの値が大きくなると正則化項の値も大きくなる → 指標を最小化するためにはパラメータの値を小さくする必要あり
    → モデルが複雑になりすぎるのが抑制される
    → 過剰適合が抑制 & 汎化性能 UP

- 次元の呪い（The curse of dimensionality）：機械学習において、特徴量の次元数 UP モデルの作成に必要なデータ数が指数関数的に増加し、学習が難しくなる
  - 同じデータであっても、データの次元が大きくなればなるほどバラツキが大きくなり、特定の範囲（例：0.0-1.0）に収まるデータの数が減少
    → バラツキが大きいデータは、大量のデータがない限り機械学習でのモデル化が難しい（モデルのパラメータ数を上げると過剰適合が起こる）

## 深層学習（Deep learning）

- 深層学習: 機械学習の手法の 1 つ。人間の神経細胞を模倣した機械学習手法「ニューラルネットワーク」において、層の数を増や

  - メリット
    ・入力から自動的に特徴量を獲得 → 手間なし
    ・（十分な学習データの数がある場合）人手による特徴量よりも高度な特徴量を獲得でき、結果的に高精度な予測が可能となる
  - デメリット
    ・データ → 高度な特徴量獲得のためには数万〜数十万規模のデータセットが必要となり、データセット収集の負荷が大きい
    ・リソース → 学習には GPU などの計算資源が必要であり、かつ学習時間もかかる

  - 深層学習の適用範囲：適用範囲は多岐に渡り、カテゴリカルデータの分類、画像・言語の認識や生成など、ほぼ全ての機械学習の分野を網羅。
    ※ChatGPT などの大規模言語モデルも深層学習ベースであり、特に画像（動画）・言語系の領域で席巻している。
   ## 使用するデータセットとライブラリ

   ### データセット: ボストンの住宅データセット

   | 項番 | 変数名           | 記号名  | 備考                                                                        |
   | ---- | ---------------- | ------- | --------------------------------------------------------------------------- |
   | 1    | 犯罪発生率       | CRIM    | (人口単位)                                                                  |
   | 2    | 住宅区間の割合   | ZN      | 25000平方フィート以上の住宅区間の割合                                       |
   | 3    | 非小売業土地面積 | INDUS   | 非小売業の土地面積の割合（人口単位）                                        |
   | 4    | チャールズ川沿い | CHAS    | 川沿いなら1、そうでなければ0                                                |
   | 5    | 窒素感化物濃度   | NOX     | （pphm単位）                                                                |
   | 6    | 平均部屋数       | RM      | 例: 2, 3など（単位は部屋）                                                  |
   | 7    | 古い家屋の割合   | AGE     | 1940年よりも前に建てられた家屋の割合                                        |
   | 8    | 雇用圏までの距離 | DIS     | ボストンの主な5つの雇用圏までの重み付き距離                                 |
   | 9    | 幹線道路指数     | RAD     | 幹線道路へのアクセス指数                                                    |
   | 10   | 所得税率         | TAX     | 10000ドルあたりの所得税率                                                   |
   | 11   | 生徒数           | PTRATIO | 教師あたりの生徒の数（人口単位）                                            |
   | 12   | アフリカ系割合   | B       | 1000(Bk - 0.63)^2として計算される量（Bkはアフリカ系アメリカ人居住者の割合） |
   | 13   | 低所得者割合     | LSTAT   |                                                                             |
   | 14   | 住宅価格         | MEDV    | 中央値（単位は1000ドル）                                                    |

   ### 使用するライブラリ

   #### Scikit-learn
   - **特徴**: 様々な回帰・分類・クラスタリングアルゴリズムが実装されているPythonのオープンソースライブラリ。
   - **設計**: NumPyやSciPyと連携して動作。
   - **公式URL**: [Scikit-learn](https://scikit-learn.org/stable/)

   #### Pandas
   - **特徴**: Excelのような2次元テーブルを対象にしたデータ解析を支援するライブラリ。
   - **主な構造**: DataFrameがメインで、2次元のテーブルを表現。

   #### NumPy
   - **特徴**: 効率的な数値計算を行うための拡張モジュール。
   - **機能**: 型付き多次元配列（ベクトルや行列など）をサポートし、それらを操作するための数学関数ライブラリを提供。

   ---

   ## 非線形回帰モデル

   ### 特徴
   - **目的**: 複雑な非線形構造を内在する現象をモデリング。
   - **必要性**: データの構造を線形で捉えられる場合は限られるため、非線形な構造を捉える仕組みが必要。

   ### 基底展開法
   - **概要**: 基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線型結合を回帰関数として使用。
   - **パラメータ推定**: 線形回帰モデルと同様に最小二乗法や最尤法を利用。

   #### よく使われる基底関数
   - 多項式関数
   - ガウス型基底関数
   - スプライン関数 / Bスプライン関数

   ---

   ### モデルの複雑さ調整
   - **未学習 (Underfitting)**: 学習データに対して十分小さな誤差が得られない。
      - **対策**: モデルの表現力を高める。
   - **過学習 (Overfitting)**: 学習データに特化しすぎて汎化性能が低下。
      - **対策**:
         1. 学習データの数を増やす。
         2. 不要な基底関数を削除。
         3. 正則化法を利用。

   #### 正則化法
   - **目的**: モデルの複雑さを抑制。
   - **手法**:
      - L2ノルム (Ridge推定量): パラメータを0に近づける。
      - L1ノルム (Lasso推定量): 一部のパラメータを正確に0にする。

   ---

   ### モデル評価
   - **汎化性能**: 学習データ以外の新たな入力に対する予測性能を測定。
   - **交差検証 (Cross Validation)**:
      - データを複数分割し、学習と検証を繰り返すことでモデルの汎化性能を評価。
      - **グリッドサーチ**: 全てのチューニングパラメータの組み合わせを評価し、最適なモデルを選択。
      ## 未学習 (Underfitting) と過学習 (Overfitting)

      ### 未学習
      - **定義**: 学習データに対して十分小さな誤差が得られないモデル。
      - **対策**:
         - モデルの表現力が低いため、表現力の高いモデルを利用する。

      ### 過学習
      - **定義**: 学習データでは小さな誤差が得られるが、テストデータとの誤差が大きいモデル。
      - **対策**:
         1. 学習データの数を増やす。
         2. 不要な基底関数（変数）を削除して表現力を抑制する。
         3. 正則化法を利用して表現力を抑制する。

      ---

      ## モデルの複雑さを調整する方法

      ### 不要な基底関数を削除
      - **概要**: 基底関数の数、位置、バンド幅によりモデルの複雑さが変化。
      - **注意点**: 多くの基底関数を用意すると過学習の問題が発生するため、適切な基底関数を選択（例: クロスバリデーションで選択）。

      ### 正則化法 (罰則化法)
      - **概要**: モデルの複雑さに伴い大きくなる正則化項（罰則項）を課した関数を最小化。
      - **正則化項の種類**:
         - **L2ノルム**: Ridge推定量（パラメータを0に近づける）。
         - **L1ノルム**: Lasso推定量（一部のパラメータを正確に0にする）。
      - **正則化パラメータ**:
         - 小さい値: 制約が緩く、モデルが複雑化。
         - 大きい値: 制約が厳しく、モデルが単純化。

      ---

      ## 非線形回帰モデル

      ### 特徴
      - **目的**: 複雑な非線形構造を内在する現象をモデリング。
      - **必要性**: データの構造を線形で捉えられる場合は限られるため、非線形な構造を捉える仕組みが必要。

      ### 基底展開法
      - **概要**: 基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線型結合を回帰関数として使用。
      - **パラメータ推定**: 線形回帰モデルと同様に最小二乗法や最尤法を利用。
      - **よく使われる基底関数**:
         - 多項式関数
         - ガウス型基底関数
         - スプライン関数 / Bスプライン関数

      ---

      ### 未学習と過学習の比較

      | 状態       | 特徴                                           | 対策                                                 |
      | ---------- | ---------------------------------------------- | ---------------------------------------------------- |
      | **未学習** | 学習データに対して十分小さな誤差が得られない。 | モデルの表現力を高める。                             |
      | **過学習** | 学習データでは誤差小、テストデータとの誤差が大 | 学習データを増やす、不要な基底関数削除、正則化法利用 |
      ## 非線形回帰モデルの複雑さ調整

      ### モデルの複雑さを調整する2つの方法

      1. **不要な基底関数を削除**
         - 基底関数の数、位置、バンド幅によりモデルの複雑さが変化。
         - 過学習を防ぐため、適切な基底関数を選択（例: クロスバリデーションを利用）。

      2. **正則化法 (罰則化法)**
         - モデルの複雑さに応じて正則化項（罰則項）を課し、関数を最小化。
         - **正則化項の種類**:
           - **L2ノルム**: Ridge推定量（パラメータを0に近づける）。
           - **L1ノルム**: Lasso推定量（一部のパラメータを正確に0にする）。
         - **正則化パラメータ**:
           - 小さい値: 制約が緩く、モデルが複雑化。
           - 大きい値: 制約が厳しく、モデルが単純化。

      ---

      ### 正則化項の役割

      - **Ridge推定量**: パラメータを縮小し、0に近づける。
      - **Lasso推定量**: 一部のパラメータを正確に0にする（スパース推定）。

      ---

      ### モデルの複雑さと正則化の影響

      - **基底関数の数と正則化パラメータの関係**:
        - 基底関数が増加するとパラメータが増え、モデルが複雑化。
        - 正則化パラメータを調整することで、モデルのなめらかさを制御可能。

      | 基底関数数 | 正則化パラメータ | サンプル数 |
      | ---------- | ---------------- | ---------- |
      | 2          | 0                | 100        |
      | 10         | 0.1              | 100        |
      | 50         | 0                | 10,000     |

## 参考文献による補足

### 出典
- Christopher M. Bishop, _Pattern Recognition and Machine Learning_, 2006, pp.144, Chapter 3: Linear Models for Regression
### 正則化付き最小二乗法 (Regularized Least Squares)

### 概要
正則化付き最小二乗法では、**過剰適合**を抑えるために誤差関数に正則化項を追加します。この手法により、モデルの複雑さを抑えつつ学習データと未知のデータへの適合をバランスよく調整します。

---

### 最小化する誤差関数
正則化付きの誤差関数は次の形式を取ります：
$$
E_D(w) + \lambda E_W(w)
$$
ここで：
- $E_D(w)$：データに基づく誤差。
- $E_W(w)$：正則化項（ペナルティ項）。
- $\lambda$：データ誤差と正則化項の重要度を調整する係数。

---

### 正則化項
正則化項は重みベクトルの要素の二乗和として定義されます：
$$
E_W(w) = \frac{1}{2}w^T w
$$
この正則化項により、重みの値が大きくなりすぎるのを防ぎます。

---

### データ誤差
データ誤差は二乗誤差として以下の形式で表されます：
$$
E(w) = \frac{1}{2} \sum_{n=1}^{N} \{ t_n - w^T \phi(x_n) \}^2
$$
ここで：
- $t_n$：ターゲット値（正しい答え）。
- $\phi(x_n)$：入力 $x_n$ の特徴量。
- $w$：重みベクトル。

---

### 合計誤差関数
最終的な誤差関数は以下の形になります：
$$
E(w) = \frac{1}{2} \sum_{n=1}^{N} \{ t_n - w^T \phi(x_n) \}^2 + \frac{\lambda}{2} w^T w
$$
この式の第1項はデータ誤差、第2項は正則化項を表し、両者をバランスよく最小化することで、過剰適合を抑制しつつモデルの精度を向上させます。


   ### 未学習と過学習のバランス

   - **未学習**: 表現力が低く、学習データに対して誤差が大きい。
   - **過学習**: 表現力が高すぎ、学習データに特化しすぎる。
   - **適切なモデル**: 表現力が適切で、汎化性能が高い。

   ## モデルの汎化性能と評価方法

   ### 汎化性能
   - **定義**: 学習に使用したデータだけでなく、未知の新たなデータに対する予測性能を指す。
   - **良いモデルの条件**:
      - 学習誤差ではなく、汎化誤差（テスト誤差）が小さいモデル。
      - 汎化誤差は通常、学習データとは別に収集された検証データで測定。

   ### データ分割と汎化性能測定
   1. **学習データ**: モデルの学習に使用。
   2. **検証データ**: 学習済みモデルの精度を検証するために使用。

   #### 学習と検証の流れ
   - 学習データを用いてモデルのパラメータを推定。
   - 検証データを用いてモデルの汎化性能を測定。

   #### モデルの状態
   - **汎化しているモデル**: 訓練誤差とテスト誤差がどちらも小さい。
   - **過学習**: 訓練誤差は小さいが、テスト誤差が大きい。
   - **未学習**: 訓練誤差もテスト誤差もどちらも小さくならない。

   ### ホールドアウト法
   - データを学習用とテスト用の2つに分割して予測精度や誤り率を推定。
   - **注意点**:
      - 学習用データを多くすると、テスト用データが減り性能評価の精度が低下。
      - テスト用データを多くすると、学習データが減り学習精度が低下。

   ### クロスバリデーション
   - データを複数分割し、学習と検証を繰り返すことで汎化性能を評価。
   - **手順**:
      1. データを複数の分割に分ける（例: 5分割）。
      2. 各分割を検証データとして使用し、残りを学習データとしてモデルを学習。
      3. 各分割での精度を平均してモデルの汎化性能を評価。

   ### モデルの評価指標
   - **学習誤差**: 学習データに対する誤差。
   - **検証誤差**: 検証データに対する誤差。
   - **汎化性能**: 検証誤差が小さいほど高い。

   ### 適切なモデル選択
   - 適切な基底関数の数、位置、バンド幅、正則化パラメータを選択。
   - クロスバリデーションを用いて汎化性能が高いモデルを決定。
   ## 非線形回帰モデルとロジスティック回帰モデルの概要

   ### 非線形回帰モデル

   #### クロスバリデーション (交差検証)
   - データを学習用と評価用に分割し、モデルの汎化性能を評価。
   - 例: 5分割クロスバリデーション
      - データを5つに分割し、1つを検証用、残りを学習用としてモデルを学習。
      - 各分割での精度を平均してモデルの汎化性能を評価。

   #### グリッドサーチ
   - 全てのチューニングパラメータの組み合わせで評価値を算出。
   - 最も良い評価値を持つチューニングパラメータを採用。

   ---

   ### ロジスティック回帰モデル

   #### 特徴
   - **分類問題**を解くための教師あり学習モデル。
   - 入力データとパラメータの線形結合をシグモイド関数に入力し、出力はクラス1に分類される確率。

   #### シグモイド関数
   - 実数を入力し、出力は必ず0～1の値。
   - 確率を表現し、単調増加関数。

   #### 最尤推定
   - 確率分布としてベルヌーイ分布を仮定。
   - 尤度関数を最大化するようなパラメータを推定。

   #### 勾配降下法
   - 対数尤度関数を最小化するために反復的にパラメータを更新。
   - **確率的勾配降下法 (SGD)** を利用して効率的に最適解を探索。

   ---

   ### 分類モデルの評価指標

   #### 混同行列
   | 実際\予測 | Positive            | Negative            |
   | --------- | ------------------- | ------------------- |
   | Positive  | True Positive (TP)  | False Negative (FN) |
   | Negative  | False Positive (FP) | True Negative (TN)  |

   #### 指標
   - **正解率 (Accuracy)**: 全データ中、正しく分類された割合。
   - **再現率 (Recall)**: 実際にPositiveなものの中で、正しくPositiveと予測された割合。
   - **適合率 (Precision)**: Positiveと予測されたものの中で、実際にPositiveである割合。
   - **F値 (F1-Score)**: 再現率と適合率の調和平均。

   #### 注意点
   - クラスの偏りがある場合、単純な正解率は適切な評価指標ではない。
   - タスクに応じて再現率や適合率を重視する。

   ---

   ### 参考リンク
   - [勾配降下法の解説](https://qiita.com/masatomix/items/d4e5fb3b52fa4c92366f)
   - [F値の詳細](http://ibisforest.org/index.php?F%E5%80%A4)
## ロジスティック回帰モデル

### 評価指標

#### 混同行列
| モデルの予測結果 | Positive            | Negative            |
| ---------------- | ------------------- | ------------------- |
| **Positive**     | True Positive (TP)  | False Positive (FP) |
| **Negative**     | False Negative (FN) | True Negative (TN)  |

#### 再現率 (Recall)
- **定義**: 本当にPositiveなデータの中で、正しくPositiveと予測された割合。
- **特徴**: False Negativeを小さくすることに注力。
- **使用例**: 病気の検診など、見逃しを避けたい場合に有効。

#### 適合率 (Precision)
- **定義**: Positiveと予測されたデータの中で、本当にPositiveである割合。
- **特徴**: False Positiveを小さくすることに注力。
- **使用例**: スパムメール検出など、予測の正確性を重視する場合に有効。

#### F値 (F1-Score)
- **定義**: 再現率と適合率の調和平均。
- **特徴**: 再現率と適合率のバランスを評価する指標。
- **参考**: [F値の詳細](http://ibisforest.org/index.php?F%E5%80%A4)

---

### タイタニックデータを用いたロジスティック回帰モデル

#### データセット概要
- **データセット名**: タイタニックデータ
- **レコード数**: 891 (学習用) / 471 (テスト用)
- **カラム数**: 11
- **詳細**: Kaggleで提供される分類アルゴリズム用データセット。

#### 主な変数
| 項番 | 変数名      | 記号名       | 説明                                          |
| ---- | ----------- | ------------ | --------------------------------------------- |
| 1    | PassengerId | 乗客ID       | 1, 2, ...                                     |
| 2    | Survived    | 生存情報     | 1: 生存 / 0: 死亡                             |
| 3    | Pclass      | 社会階級     | 1: 高 / 2: 中 / 3: 低                         |
| 4    | Name        | 名前         | -                                             |
| 5    | Sex         | 性別         | 男性: male / 女性: female                     |
| 6    | Age         | 年齢         | 22.0, 38.0 など                               |
| 7    | SibSp       | 兄弟・配偶者 | 0, 1, 2 など                                  |
| 8    | Parch       | 親・子供数   | 0, 1 など                                     |
| 9    | Ticket      | チケット番号 | A/5 21171 など                                |
| 10   | Fare        | 運賃         | 7.2500 など                                   |
| 11   | Cabin       | 船室         | C85 など                                      |
| 12   | Embarked    | 乗船港       | C: Cherbourg / Q: Queenstown / S: Southampton |

---

## 主成分分析 (PCA)

### 概要
- **目的**: 多変量データを少数の指標に圧縮し、情報損失を最小化。
- **応用**: 次元削減による可視化や分析。

### 手法
1. **分散共分散行列の計算**:
   - データの分散を計算し、相関を把握。
2. **固有値・固有ベクトルの計算**:
   - 固有値: 各主成分の分散を表す。
   - 固有ベクトル: 主成分の方向を表す。
3. **寄与率と累積寄与率の計算**:
   - 寄与率: 各主成分がデータ全体の分散に占める割合。
   - 累積寄与率: 上位k個の主成分で説明できる情報量の割合。

### 設定
- **データセット名**: 乳がん検査データ
- **レコード数**: 569
- **カラム数**: 33
- **目的**: 32次元データを2次元に圧縮し、判別可能性を確認。
## 主成分分析とk近傍法 (kNN)

### 主成分分析 (PCA)
- **課題**: 32次元のデータを2次元に次元圧縮し、判別可能性を確認。
- **データセット**: 乳がん検査データ
   - **レコード数**: 569
   - **カラム数**: 33
   - **詳細**: Scikit-learnライブラリを使用。

---

### k近傍法 (kNN)
- **概要**: 分類問題のための機械学習手法。
   - 新しいデータを分類する際、近傍のデータ点を基にクラスを決定。
   - 例: k=3の場合、近傍の点が紫2個、黄1個なら紫クラスに分類。

#### 特徴
- **kの影響**:
   - 小さいk: 決定境界が複雑。
   - 大きいk: 決定境界が滑らか。
- **分類結果の変化**:
   - k=1 (最近傍法): 最も近い1点で分類。
   - k=3: 近傍3点の多数決で分類。
   - k=10: より多くの点を考慮して分類。

#### ハンズオン設定
- **課題**: 人口データを分類し、分類結果をプロット。
- **データセット**: 人口データ
   - **レコード数**: 不明
   - **カラム数**: 不明

---

### k-平均法 (k-means)
- **概要**: 教師なし学習のクラスタリング手法。
   - 特徴の似ているデータをk個のクラスタに分類。

#### アルゴリズム
1. 各クラスタ中心の初期値を設定。
2. 各データ点に対して、最も近いクラスタを割り当てる。
3. 各クラスタの平均ベクトル（中心）を計算。
4. 収束するまで2と3を繰り返す。

#### 特徴
- **初期値の影響**:
   - 初期値が近い: クラスタリングがうまくいかない場合がある。
   - 初期値が離れている: クラスタリングがうまくいく。
- **kの影響**:
   - k=3: 3つのクラスタに分類。
   - k=5: 5つのクラスタに分類。

---

### 距離計算
- **コサイン距離**: ベクトル間の角度を基に計算。
- **ユークリッド距離**: いわゆる通常の距離。
- **マンハッタン距離**: 各座標の差の絶対値の総和。
- **Lp距離**: Lp空間における一般化された距離。
- **マハラノビス距離**: データの散らばりを考慮した距離。

#### 用途
- **コサイン距離**: テキストマイニング、自然言語処理。
- **ユークリッド距離**: 画像処理、クラスタリング。
- **マンハッタン距離**: ルート検索。
- **マハラノビス距離**: 異常検知、次元削減。

## 深層学習の学習

- **エポック数**: あらかじめ設定した繰り返し回数分、反復的にパラメータ（ニューロンの重みなど）を更新。
- **誤差観察**: 訓練誤差と汎化誤差の状況をリアルタイムに観察。
   - 通常、訓練データの数が検証データより多いため、訓練誤差の方が汎化誤差よりも低い値となる。

### 適合状況の分類
1. **過少適合**:
    - 訓練誤差と汎化誤差の傾向は同じだが、汎化誤差が明らかに大きい。
    - **対策**: モデルのパラメータ数を増やして乖離を小さくする。
2. **過剰適合**:
    - 訓練誤差と汎化誤差の傾向が異なり、乖離が大きくなる。
    - **対策**: 正則化やモデルのパラメータ数を減らす。

### 適合状況の変化
- 反復的な学習の中で、過少適合から過剰適合へと変化することがある。
- グラフの状況を観察し、適切な正則化やパラメータ調整を行う必要がある。

---

## 機械学習モデリングプロセス

1. **問題設定**:
    - 機械学習を使う課題を明確化。
2. **データ選定**:
    - モデルに学習させるためのデータを選定。
3. **データの前処理**:
    - データをモデルが学習可能な形式に変換。
4. **モデルの選定**:
    - 線形回帰、ロジスティック回帰、SVM、PCA、k-means などから選択。
5. **モデルの学習**:
    - パラメータ推定やハイパーパラメータの選定を実施。
6. **モデルの評価**:
    - モデル精度を測定し、必要に応じて調整。

- **教師あり学習**:
   - 線形回帰、非線形回帰、ロジスティック回帰。
- **教師なし学習**:
   - 主成分分析、k-means アルゴリズム。
- **分類タスク**:
   - 最近傍法、K-近傍アルゴリズム、サポートベクターマシン。
- **性能指標**:
   - 分類タスク: 正解率、適合率、再現率、F値、ROC曲線、AUC。
   - 回帰タスク: MSE、RMSE、MAE、決定係数。

## 参考図書による補足 
参考論文：[https://www.ism.ac.jp/editsec/toukei/pdf/58-2-141.pdf]
- Vapnikの原理
ある問題を解くとき，その問題よりも難しい問題を途中の段階で解いてはならないという原理
プログラマの教訓的によく使われる
途中で難しい問題があるならそれを回避する方法を考えた方が良いし，回避出来ないなら最終的なタスクを変えて回避した方がいい

サポートベクトルマシン(SVM)の提案者であるVapnikがStatistical Learning Theoryという教科書の中で書いたらしい．
- 確率密度の推定は困難な問題都知られており，これを回避することが統計的機械学習において非常に重要である．
- SVMはこの原理を踏襲し成功した典型的な例である
- SVMはデータ生成の確率分布を推定するという一般的かつ困難な問題を解くことなく，[パターン認識]に必要な決定境界のみ学習する

# 性能指標
([目次に戻る](#目次))

### 分類タスク
- **混同行列**:
   | 実際\予測 | Positive | Negative |
   | --------- | -------- | -------- |
   | Positive  | TP       | FN       |
   | Negative  | FP       | TN       |

- **指標の計算方法**:
   - **正解率 (Accuracy)**: \((TP + TN) / (TP + FP + FN + TN)\)
   - **適合率 (Precision)**: \(TP / (TP + FP)\)
   - **再現率 (Recall)**: \(TP / (TP + FN)\)
   - **F値 (F1-Score)**: \(2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\)

- **ROC曲線とAUC**:
   - ROC曲線: 再現率と偽陽性率の関係を視覚化。
   - AUC: ROC曲線の下の面積。1に近いほど良いモデル。

   ### 回帰タスク
   - **指標の計算方法**:
      - **MSE (平均二乗誤差)**: $$\frac{1}{N} \sum_{i=1}^{N} (y_i - t_i)^2$$
      - **RMSE (平方平均二乗誤差)**: $$\sqrt{\text{MSE}}$$
      - **MAE (平均絶対誤差)**: $$\frac{1}{N} \sum_{i=1}^{N} |y_i - t_i|$$
      - **決定係数**: $$1 - \frac{\sum_{i=1}^{N} (y_i - t_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}$$

   - **特徴**:
      - MSE/RMSE: 誤差が小さいほど良い。
      - MAE: 外れ値の影響が小さい。
      - 決定係数: 1に近いほど精度が高い。
      ## 性能指標

      ### 回帰タスクの評価指標

      1. **平均二乗誤差 (MSE)**  
         $$
         \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - t_i)^2
         $$  
         - **特徴**: 誤差の二乗平均を計算。外れ値の影響を受けやすい。

      2. **平方平均二乗誤差 (RMSE)**  
         $$
         \text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - t_i)^2}
         $$  
         - **特徴**: MSEの平方根を取った値。元のデータの単位に戻す。

      3. **平均絶対誤差 (MAE)**  
         $$
         \text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - t_i|
         $$  
         - **特徴**: 誤差の絶対値の平均を計算。外れ値の影響を受けにくい。

      4. **決定係数 (R²)**  
         $$
         R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - t_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}
         $$  
         - **特徴**: モデルの説明力を評価。1に近いほど良いモデル。

   ### 機械学習の定義

   - **タスク T**: アプリケーションにさせたいこと。
   - **性能指標 P**: タスクの性能を測定する指標。
   - **経験 E**: 過去のデータや経験。

**定義**: コンピュータプログラムがタスク Tを性能指標 Pで測定し、経験 Eにより性能が改善される場合、プログラムは学習していると言える (トム・ミッチェル, 1997)。


# 線形回帰モデル
([目次に戻る](#目次))

   - **概要**: 入力とパラメータの線形結合を出力する教師あり学習モデル。
   - **数式**

   $$  y = w_0 + w_1x_1 + w_2x_2 + \dots + w_mx_m$$

     - y: 予測値  
     - w: パラメータ  
     - x: 説明変数  

   ### パラメータ推定

   - **最小二乗法**:  
     平均二乗誤差 (MSE) を最小化するパラメータを探索。  
     $$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - t_i)^2$$
     - 勾配が0になる点を求めることで最適なパラメータを計算。

   ### データ分割と汎化性能

   1. **学習データ**: モデルの学習に使用。
   2. **検証データ**: 学習済みモデルの精度を評価。

   - **目的**: モデルの汎化性能 (未知データへの適用能力) を測定。
   - **手法**:  
     - **ホールドアウト法**: データを学習用とテスト用に分割。  
     - **クロスバリデーション**: データを複数分割し、学習と検証を繰り返す。

 ### 線形回帰モデルの評価

   - **評価指標**: MSE, RMSE, MAE, R²。
   - **重要性**: 学習データへの当てはまりだけでなく、未知データへの適用能力を重視。
   ## 線形回帰モデル

   ### 最尤法による回帰係数の推定
   - 誤差を正規分布に従う確率変数と仮定し、尤度関数の最大化を利用して回帰係数を推定可能。
   - 線形回帰の場合、最尤法による解は最小二乗法の解と一致。

   ## ハンズオン: ボストンの住宅データセット

   ### 設定
   - **目的**: ボストンの住宅データセットを用いて線形回帰モデルで分析。
   - **課題**: 部屋数が4、犯罪率が0.3の物件の価格を予測。

   ### データセット概要
   - **データセット名**: ボストンの住宅データセット
   - **レコード数**: 506
   - **カラム数**: 14
   - **詳細**: UCI Machine Learning Repository: Housing Data Set
   - **備考**: 現在、倫理的な問題から非推奨。

   ## 使用するライブラリ

   ### Scikit-learn
   - **特徴**: 様々な回帰・分類・クラスタリングアルゴリズムが実装されているPythonのオープンソースライブラリ。
   - **設計**: NumPyやSciPyと連携して動作。
   - **公式URL**: [Scikit-learn](https://scikit-learn.org/stable/)

   ### Pandas
   - **特徴**: Excelのような2次元テーブルを対象にしたデータ解析を支援するライブラリ。
   - **主な構造**: `DataFrame`がメインで、2次元のテーブルを表現。

   ### NumPy
   - **特徴**: 効率的な数値計算を行うための拡張モジュール。
   - **機能**: 型付き多次元配列（ベクトルや行列など）をサポートし、それらを操作するための数学関数ライブラリを提供。

   ---

# 非線形回帰モデルと評価方法
([目次に戻る](#目次))

   ### モデルの複雑さと調整方法
   - **未学習 (Underfitting)**: 学習データに対して十分小さな誤差が得られない。
      - **対策**: モデルの表現力を高める。
   - **過学習 (Overfitting)**: 学習データに特化しすぎて汎化性能が低下。
      - **対策**:
         1. 学習データを増やす。
         2. 不要な基底関数を削除。
         3. 正則化法を利用。

   ### 正則化法
   - **目的**: モデルの複雑さを抑制。
   - **手法**:
      - **L2ノルム (Ridge推定量)**: パラメータを0に近づける。
      - **L1ノルム (Lasso推定量)**: 一部のパラメータを正確に0にする。

   ### クロスバリデーション (交差検証)
   - **概要**: データを学習用と評価用に分割し、モデルの汎化性能を評価。
   - **手順**:
      1. データを複数分割（例: 5分割）。
      2. 各分割で学習と検証を繰り返す。
      3. 各分割の精度を平均してモデルの汎化性能を評価。

   ### グリッドサーチ
   - **目的**: 最適なチューニングパラメータを探索。
   - **手法**:
      - 全てのパラメータの組み合わせで評価値を算出。
      - 最も良い評価値を持つパラメータを採用。

   ---

   ## ロジスティック回帰モデル

   ### 特徴
   - **分類問題**を解くための教師あり学習モデル。
   - 入力データとパラメータの線形結合をシグモイド関数に入力し、出力はクラス1に分類される確率。

   ### シグモイド関数
   - **定義**: 実数を入力し、出力は必ず0～1の値。
   - **特徴**: 確率を表現し、単調増加関数。

   ### 最尤推定
   - **目的**: 尤度関数を最大化するようなパラメータを推定。
   - **手法**: 勾配降下法や確率的勾配降下法 (SGD) を利用。

   ### 分類モデルの評価指標
   - **混同行列**:
      | 実際\予測 | Positive            | Negative            |
      | --------- | ------------------- | ------------------- |
      | Positive  | True Positive (TP)  | False Negative (FN) |
      | Negative  | False Positive (FP) | True Negative (TN)  |

   - **指標**:
      - **正解率 (Accuracy)**: 全データ中、正しく分類された割合。
      - **再現率 (Recall)**: 実際にPositiveなものの中で、正しくPositiveと予測された割合。
      - **適合率 (Precision)**: Positiveと予測されたものの中で、実際にPositiveである割合。
      - **F値 (F1-Score)**: 再現率と適合率の調和平均。



   ---
   








# 分析手法
- 主成分分析、k近傍法、k-means
([目次に戻る](#目次))
   ## 主成分分析 (PCA)

   ### 概要
   - **目的**: 多変量データを少数の指標に圧縮し、情報損失を最小化。
   - **応用**: 次元削減による可視化や分析。

   ### 手法
   1. **分散共分散行列の計算**: データの分散を計算し、相関を把握。
   2. **固有値・固有ベクトルの計算**: 主成分の方向と分散を表す。
   3. **寄与率と累積寄与率の計算**:
       - 寄与率: 各主成分がデータ全体の分散に占める割合。
       - 累積寄与率: 上位k個の主成分で説明できる情報量の割合。

   ---

   ## k近傍法 (kNN)

   ### 特徴
   - **分類問題**のための機械学習手法。
   - 新しいデータを分類する際、近傍のデータ点を基にクラスを決定。

   ### kの影響
   - 小さいk: 決定境界が複雑。
   - 大きいk: 決定境界が滑らか。

   ---

   ## k-平均法 (k-means)

   ### 特徴
   - **教師なし学習**のクラスタリング手法。
   - 特徴の似ているデータをk個のクラスタに分類。

   ### アルゴリズム
   1. 各クラスタ中心の初期値を設定。
   2. 各データ点に対して、最も近いクラスタを割り当てる。
   3. 各クラスタの平均ベクトル（中心）を計算。
   4. 収束するまで2と3を繰り返す。

## 参考図書による補足
- 参考著書：見て試してわかる機械学習アルゴリズムの仕組み 機械学習図鑑
## クラスタリング結果の評価方法について

クラスタリング結果の良し悪しは、**クラスタ内平方和 (Within-Cluster Sum of Squares: WCSS)** を計算することで、定量的に評価することができます
（このWCSSはクラスタ数を増やせば、小さくなるので、同じクラスタ数での比較の場合に利用できます）。

WCSSは、すべてのクラスタについて、所属するデータ点とクラスタ重心の距離の平方和を計算し、それらについて和をとったもので、この値が小さいほど、良いクラスタリングということができます。

クラスタ重心と所属するデータ点との距離が小さいほど、つまり、データ点がクラスタ重心近くに凝集しているクラスタリングほど、WCSSは小さくなります。
   ---

   ## 距離計算
   - **コサイン距離**: ベクトル間の角度を基に計算。
   - **ユークリッド距離**: 通常の距離。
   - **マンハッタン距離**: 各座標の差の絶対値の総和。
   - **マハラノビス距離**: データの散らばりを考慮した距離。

   ### 用途
   - **コサイン距離**: テキストマイニング、自然言語処理。
   - **ユークリッド距離**: 画像処理、クラスタリング。
   - **マンハッタン距離**: ルート検索。
   - **マハラノビス距離**: 異常検知、次元削減。
## CV 値とグリッドサーチ

### CV 値
- **ホールドアウト法**で検証した場合の精度が70%、**クロスバリデーション (CV)** で65%だった場合でも、汎化性能の推定としてはCVを利用することが推奨されます。
- **チューニングパラメータ**は固定して評価を行います。

---

### グリッドサーチ
- **概要**: 全てのチューニングパラメータの組み合わせで評価値を算出。
- **目的**: 最も良い評価値を持つチューニングパラメータを選択。
- **採用**: 「良いモデルのパラメータ」として最適な組み合わせを採用します。

---

## ロジスティック回帰モデル

### 分類タスク
- **分類問題**: 入力データからクラスに分類する問題。
- **データ形式**:
   - **入力**: \(m\) 次元のベクトル (特徴量)。
   - **出力**: 0 または 1 (目的変数)。
- **例**: タイタニックデータ、IRISデータなど。

---

### ロジスティック回帰モデルの概要
- **教師あり学習モデル**: 教師データから学習。
- **シグモイド関数**を用いて、クラス1に分類される確率を出力。

#### シグモイド関数
- **定義**: 実数を入力し、出力は必ず0～1の値。
- **特徴**: 確率を表現し、単調増加関数。
- **パラメータの影響**:
   - パラメータ \(a\) を増加させると、曲線の勾配が増加。
   - バイアスの変化は段差の位置に影響。

---

### 最尤推定
- **確率分布**: ベルヌーイ分布を仮定。
- **尤度関数**: データが得られる確率を最大化するパラメータを推定。
- **手法**: 勾配降下法や確率的勾配降下法 (SGD) を利用。

---

### 分類モデルの評価指標
- **混同行列**:
   | 実際\予測 | Positive            | Negative            |
   | --------- | ------------------- | ------------------- |
   | Positive  | True Positive (TP)  | False Negative (FN) |
   | Negative  | False Positive (FP) | True Negative (TN)  |

- **指標**:
   - **正解率 (Accuracy)**: 全データ中、正しく分類された割合。
   - **再現率 (Recall)**: 実際にPositiveなものの中で、正しくPositiveと予測された割合。
   - **適合率 (Precision)**: Positiveと予測されたものの中で、実際にPositiveである割合。
   - **F値 (F1-Score)**: 再現率と適合率の調和平均。

---

## 主成分分析 (PCA)

### 概要
- **目的**: 多変量データを少数の指標に圧縮し、情報損失を最小化。
- **応用**: 次元削減による可視化や分析。

### 手法
1. **分散共分散行列の計算**: データの分散を計算し、相関を把握。
2. **固有値・固有ベクトルの計算**: 主成分の方向と分散を表す。
3. **寄与率と累積寄与率の計算**:
    - 寄与率: 各主成分がデータ全体の分散に占める割合。
    - 累積寄与率: 上位 \(k\) 個の主成分で説明できる情報量の割合。

---

## k近傍法 (kNN)

### 特徴
- **分類問題**のための機械学習手法。
- 新しいデータを分類する際、近傍のデータ点を基にクラスを決定。

### kの影響
- 小さい \(k\): 決定境界が複雑。
- 大きい \(k\): 決定境界が滑らか。

---

## k-平均法 (k-means)

### 特徴
- **教師なし学習**のクラスタリング手法。
- 特徴の似ているデータを \(k\) 個のクラスタに分類。

### アルゴリズム
1. 各クラスタ中心の初期値を設定。
2. 各データ点に対して、最も近いクラスタを割り当てる。
3. 各クラスタの平均ベクトル（中心）を計算。
4. 収束するまで2と3を繰り返す。

---

## 距離計算

### 種類
- **コサイン距離**: ベクトル間の角度を基に計算。
- **ユークリッド距離**: 通常の距離。
- **マンハッタン距離**: 各座標の差の絶対値の総和。
- **マハラノビス距離**: データの散らばりを考慮した距離。

### 用途
- **コサイン距離**: テキストマイニング、自然言語処理。
- **ユークリッド距離**: 画像処理、クラスタリング。
- **マンハッタン距離**: ルート検索。
- **マハラノビス距離**: 異常検知、次元削減。
   ### 回帰タスクの評価指標

   1. **平均二乗誤差 (MSE)**  
      $$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - t_i)^2$$
      - **特徴**: 誤差の二乗平均を計算。外れ値の影響を受けやすい。

   2. **平方平均二乗誤差 (RMSE)**  
     $$ \ text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - t_i)^2}$$
      - **特徴**: MSEの平方根を取った値。元のデータの単位に戻す。

   3. **平均絶対誤差 (MAE)**  
      $$\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - t_i|$$
      - **特徴**: 誤差の絶対値の平均を計算。外れ値の影響を受けにくい。

   4. **決定係数 (R²)**  
      $$R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - t_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}$$
      - **特徴**: モデルの説明力を評価。1に近いほど良いモデル。

   ---

   ### 機械学習の定義

   - **タスク T**: アプリケーションにさせたいこと。
   - **性能指標 P**: タスクの性能を測定する指標。
   - **経験 E**: 過去のデータや経験。

   **定義**: コンピュータプログラムがタスク Tを性能指標 Pで測定し、経験 Eにより性能が改善される場合、プログラムは学習していると言える (トム・ミッチェル, 1997)。

   ---

   ### 線形回帰モデル

   - **概要**: 入力とパラメータの線形結合を出力する教師あり学習モデル。
   - **数式**:  
     \[
     y = w_0 + w_1x_1 + w_2x_2 + \dots + w_mx_m
     \]  
     - \(y\): 予測値  
     - \(w\): パラメータ  
     - \(x\): 説明変数  

   ---

   ### パラメータ推定

   - **最小二乗法**:  
     平均二乗誤差 (MSE) を最小化するパラメータを探索。  
     \[
     \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - t_i)^2
     \]  
     - 勾配が0になる点を求めることで最適なパラメータを計算。

   ---

   ### データ分割と汎化性能

   1. **学習データ**: モデルの学習に使用。
   2. **検証データ**: 学習済みモデルの精度を評価。

   - **目的**: モデルの汎化性能 (未知データへの適用能力) を測定。
   - **手法**:  
     - **ホールドアウト法**: データを学習用とテスト用に分割。  
     - **クロスバリデーション**: データを複数分割し、学習と検証を繰り返す。

   ---

   ### 線形回帰モデルの評価

   - **評価指標**: MSE, RMSE, MAE, R²。
   ### 非線形回帰モデルの評価とクロスバリデーション

   #### クロスバリデーション (交差検証)
   - **目的**: モデルの汎化性能を評価するためにデータを分割して学習と検証を繰り返す。
   - **手順**:
      1. データを複数分割（例: 5分割）。
      2. 各分割で学習と検証を繰り返す。
      3. 各分割の精度を平均してモデルの汎化性能を評価。
   - **CV値**: 各分割で得られた精度の平均値を指し、モデルの汎化性能を示す。

   #### グリッドサーチ
   - **概要**: 全てのチューニングパラメータの組み合わせで評価値を算出。
   - **目的**: 最も良い評価値を持つチューニングパラメータを選択。
   - **採用**: 「良いモデルのパラメータ」として最適な組み合わせを採用。

   #### 注意点
   - **ホールドアウト法との比較**:
      - 例: ホールドアウト法で70%の精度、CVで65%だった場合でも、汎化性能の推定としてはCVを利用することが推奨される。
   - **チューニングパラメータ**: 固定して評価を行う。

   ---

   ### ロジスティック回帰モデルの概要

   #### 特徴
   - **分類問題**を解くための教師あり学習モデル。
   - 入力データとパラメータの線形結合をシグモイド関数に入力し、出力はクラス1に分類される確率。

   #### シグモイド関数
   - **定義**: 実数を入力し、出力は必ず0～1の値。
   - **特徴**: 確率を表現し、単調増加関数。

   #### 最尤推定
   - **目的**: 尤度関数を最大化するようなパラメータを推定。
   - **手法**: 勾配降下法や確率的勾配降下法 (SGD) を利用。

   #### 勾配降下法
   - **概要**: 反復学習によりパラメータを逐次的に更新する手法。
   - **学習率 (η)**: モデルのパラメータの収束しやすさを調整するハイパーパラメータ。

   #### 分類モデルの評価指標
   - **混同行列**:
      | 実際\予測 | Positive            | Negative            |
      | --------- | ------------------- | ------------------- |
      | Positive  | True Positive (TP)  | False Negative (FN) |
      | Negative  | False Positive (FP) | True Negative (TN)  |

   - **指標**:
      - **正解率 (Accuracy)**: 全データ中、正しく分類された割合。
      - **再現率 (Recall)**: 実際にPositiveなものの中で、正しくPositiveと予測された割合。
      - **適合率 (Precision)**: Positiveと予測されたものの中で、実際にPositiveである割合。
      - **F値 (F1-Score)**: 再現率と適合率の調和平均。

   ---

   ### タイタニックデータを用いたロジスティック回帰モデル

   #### データセット概要
   - **データセット名**: タイタニックデータ
   - **レコード数**: 891 (学習用) / 471 (テスト用)
   - **カラム数**: 11
   - **詳細**: Kaggleで提供される分類アルゴリズム用データセット。

   #### 主な変数
   | 項番 | 変数名      | 記号名       | 説明                                          |
   | ---- | ----------- | ------------ | --------------------------------------------- |
   | 1    | PassengerId | 乗客ID       | 1, 2, ...                                     |
   | 2    | Survived    | 生存情報     | 1: 生存 / 0: 死亡                             |
   | 3    | Pclass      | 社会階級     | 1: 高 / 2: 中 / 3: 低                         |
   | 4    | Name        | 名前         | -                                             |
   | 5    | Sex         | 性別         | 男性: male / 女性: female                     |
   | 6    | Age         | 年齢         | 22.0, 38.0 など                               |
   | 7    | SibSp       | 兄弟・配偶者 | 0, 1, 2 など                                  |
   | 8    | Parch       | 親・子供数   | 0, 1 など                                     |
   | 9    | Ticket      | チケット番号 | A/5 21171 など                                |
   | 10   | Fare        | 運賃         | 7.2500 など                                   |
   | 11   | Cabin       | 船室         | C85 など                                      |
   | 12   | Embarked    | 乗船港       | C: Cherbourg / Q: Queenstown / S: Southampton |

   #### 課題
   - 年齢が30歳で男性の乗客が生き残れるかを予測する。
   - 特徴量抽出を行い、モデルを構築する。
   ### 確率的勾配降下法 (SGD)

   - **課題**: データ量 \(n\) が巨大な場合、オンメモリに載せる容量不足や計算時間の増大が問題となる。
   - **解決策**: 確率的勾配降下法 (SGD) を利用。
      - データを一つずつランダムに（「確率的に」）選んでパラメータを更新。
      - 勾配降下法でパラメータを1回更新するのと同じ計算量で、パラメータを \(n\) 回更新可能。
      - 効率的に最適解を探索可能。

   ### 分類の評価方法

   1. **正解率 (Accuracy)**  
       - **定義**: 正解した数 / 予測対象となった全データ数。
       - **課題**: クラスに偏りがある場合、単純な正解率は適切な評価指標ではない。

   2. **再現率 (Recall)**  
       - **定義**: 本当にPositiveなものの中で、Positiveと予測できた割合。
       - **特徴**: False Negativeを小さくすることに注力。
       - **使用例**: 病気の検診など、見逃しを避けたい場合。

   3. **適合率 (Precision)**  
       - **定義**: Positiveと予測されたものの中で、本当にPositiveである割合。
       - **特徴**: False Positiveを小さくすることに注力。
       - **使用例**: スパムメール検出など、予測の正確性を重視する場合。

   4. **F値 (F1-Score)**  
       - **定義**: 再現率と適合率の調和平均。
       - **特徴**: 再現率と適合率のバランスを評価する指標。

   ---

   ### タイタニックデータを用いたロジスティック回帰モデル

   - **データセット名**: タイタニックデータ
   - **レコード数**: 891 (学習用) / 471 (テスト用)
   - **カラム数**: 11
   - **課題**: 年齢が30歳で男性の乗客が生き残れるかを予測。
   
   ### 混同行列 (Confusion Matrix)

   - **概要**: 各検証データに対するモデルの予測結果を4つの観点で分類し、それぞれの個数をまとめた表。

   | モデルの予測結果 | Positive           | Negative            |
   | ---------------- | ------------------ | ------------------- |
   | **Positive**     | True Positive (TP) | False Positive (FP) | # パターン認識 |
   
   ---

# パターン認識
([目次に戻る](#目次))

## キーワード
- **k近傍法**
  - `kd-tree`
  - 近似最近傍探索
- **距離計算**
  - コサイン距離
  - ユークリッド距離
  - マンハッタン距離
  - Lp距離
  - マハラノビス距離

## k近傍法とその計算量
- **アルゴリズム**
  - 識別したいサンプルと訓練サンプル間の距離を計算し、近傍k個の訓練サンプルが所属するクラスに分類。
  - クラス決定は近傍k個の訓練サンプルによる投票で行う。
- **最適な最近傍数(k)**
  - `k` を大きくすると滑らかな識別面が得られる。
  - `k` が大きすぎると識別精度が下がる。
- **計算量の課題**
  - 距離計算のコストが高い。
  - 訓練サンプル数に比例して計算量が増加。


## 距離計算方法と性質
1. **コサイン距離**
   - 2つのベクトルがなす角度のコサイン値。
   - 主にテキストマイニングや自然言語処理で利用。
2. **ユークリッド距離**
   - 通常の直線距離（L2距離とも呼ばれる）。
   - 主にクラスタリングや画像処理で利用。
3. **マンハッタン距離**
   - 各座標における差の絶対値の総和（L1距離とも呼ばれる）。
   - 特徴量の比較やルート検索で利用。
4. **Lp距離**
   - 一般化されたLp空間での距離。
   - 次元削減や特徴量の正規化で使用。
5. **マハラノビス距離**
   - データの散らばりを考慮した統計学的な距離。
   - 異常検知や特徴選択で使用。

## k近傍法の計算量の削減
### 近似最近傍探索
- **必要性**
  - 高次元空間では距離計算のコストが高いため。
- **アプローチ**
  - Tree/Partitioning (例: `kd-tree`)
  - Graph探索
  - Locality Sensitive Hashing (LSH)
  - データ量子化
- **用途**
  - k近傍法の距離計算、情報検索、画像認識、音声処理。

## kd-tree (kd木)
- **概要**
  - k次元空間内でデータを分類する空間分割データ構造。
- **構築**
  - 1つ目の次元で中央値を使ってデータを分割。
  - 次の次元に対して同様に分割。
- **特徴**
  - k近傍法の最近傍探索に利用。
  - 高次元データには適さない。

## 距離計算の模式図と数式
1. **コサイン距離**  
   - 数式:  
     $$ \cos(\mathbf{x}, \mathbf{y}) = 1 - \frac{\mathbf{x} \cdot \mathbf{y}}{|\mathbf{x}||\mathbf{y}|} $$

2. **ユークリッド距離**  
   - 数式:  
     $$ d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^n{(x_i - y_i)^2}} $$

3. **マンハッタン距離**  
   - 数式:  
     $$ d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^n{|x_i - y_i|} $$

4. **Lp距離**  
   - 数式:  
     $$ d(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^n{|x_i - y_i|^p} \right)^{1/p} $$

5. **マハラノビス距離**  
   - 数式:  
     $$ d(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^\top S^{-1} (\mathbf{x} - \mathbf{y})} $$  
     ここで、$S^{-1}$ は共分散行列の逆行列。

   ---





























