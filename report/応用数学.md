# 目次

1. [第１章:線形代数](#第1章:線形代数)
2. [第２章:確率・統計](#確率・統計)
3. [第３章:情報理論](#情報理論)

## 1. 第 1 章:線形代数

### スカラー

- 普通の数
- 四則演算可能
- ベクトルの係数として利用可

### ベクトル

- 大きさと向き
- 矢印として図示される
- スカラーのセットで表示される → 行列

### 行列

- スカラーの表
- ベクトルを並べたもの
- ベクトルの変換や連立方程式の解を求めるために使用

### 単位行列

- 対角に 1 が並び、それ以外 0 の正方行列
- 行列の単位元になる
- $AA^{-1}=A^{-1}A = I(単位行列)$ ※積の交換法則が成立!

$$\begin{pmatrix} 1 &0&0 \\ 0&1&0 \\ 0&0&1 \end{pmatrix}$$

### 逆行列

行列の逆数のようなもの

- 行列 A に対して、A とその逆行列 A⁻¹ の積が単位行列になる行列
- 逆行列が存在するためには、行列 A が正則である必要がある（行列式が 0 でない）
- 逆行列の求め方には、ガウス・ジョルダン法や余因子行列を用いる方法がある
- 逆行列は線形方程式の解を求める際に使用

### 行列式

→ 正方行列の「大きさ」みたいなもの

#### 2 つの横ベクトル

行列 $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ を 2 つの横ベクトル $\vec{v}_1, \vec{v}_2$ の組み合わせと考える。

$$A = \begin{pmatrix} a & b \\ c & d \end{pmatrix} = \begin{pmatrix} \vec{v}_1 \\ \vec{v}_2 \end{pmatrix}$$

このとき、行列式 $|A|$ は、$\vec{v}_1$ と $\vec{v}_2$ によって作られる平行四辺形の面積を表し、逆行列の有無を判別する。

$$|A| = \left| \begin{matrix} a & b \\ c & d \end{matrix} \right| = \left| \begin{matrix} \vec{v}_1 \\ \vec{v}_2 \end{matrix} \right|$$

#### 3 つ以上のベクトル

3 つ以上のベクトルからなる行列式は展開できる。

$$\left| \begin{matrix} \vec{v}_1 \\ \vec{v}_2 \\ \vec{v}_3 \end{matrix} \right| = \left| \begin{matrix} a & b & c \\ d & e & f \\ g & h & i \end{matrix} \right| = \left| \begin{matrix} a & b & c \\ 0 & e & f \\ 0 & h & i \end{matrix} \right| + \left| \begin{matrix} 0 & b & c \\ d & e & f \\ 0 & h & i \end{matrix} \right| + \left| \begin{matrix} 0 & b & c \\ 0 & e & f \\ g & h & i \end{matrix} \right|$$

$$= a \left| \begin{matrix} e & f \\ h & i \end{matrix} \right| - d \left| \begin{matrix} b & c \\ h & i \end{matrix} \right| + g \left| \begin{matrix} b & c \\ e & f \end{matrix} \right|$$

#### n 個のベクトルからなる行列式の特徴

- **同じ行ベクトルが含まれていると行列式はゼロ**

$$\left| \begin{matrix} \vec{v}_1 \\ \vdots \\ \vec{v}_1 \\ \vec{v}_n \end{matrix} \right| = 0$$

- **1 つのベクトルが λ 倍されると行列式は λ 倍される**

$$\left| \begin{matrix} \vec{v}_1 \\ \vdots \\ \lambda \vec{v}_i \\ \vec{v}_n \end{matrix} \right| = \lambda \left| \begin{matrix} \vec{v}_1 \\ \vdots \\ \vec{v}_i \\ \vec{v}_n \end{matrix} \right|$$

- **他の成分が全部同じで i 番目のベクトルだけが違った場合、行列式の足し合わせになる**

$$\left| \begin{matrix} \vec{v}_1 \\ \vdots \\ \vec{v}_i + \vec{w} \\ \vec{v}_n \end{matrix} \right| = \left| \begin{matrix} \vec{v}_1 \\ \vdots \\ \vec{v}_i \\ \vec{v}_n \end{matrix} \right| + \left| \begin{matrix} \vec{v}_1 \\ \vdots \\ \vec{w} \\ \vec{v}_n \end{matrix} \right|$$

- **行を入れ替えると符号が変わる**

$$\left| \begin{matrix} \vec{v}_1 \\ \vdots \\ \vec{v}_i \\ \vdots \\ \vec{v}_j \\ \vdots \\ \vec{v}_n \end{matrix} \right| = - \left| \begin{matrix} \vec{v}_1 \\ \vdots \\ \vec{v}_j \\ \vdots \\ \vec{v}_i \\ \vdots \\ \vec{v}_n \end{matrix} \right|$$

- (例)

$$A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$$
$$ad - bc$$
$$A' = \begin{pmatrix}c & d \\ a & b \end{pmatrix}$$
$$cb - ad$$
$$ = -(ad - bc)$$

計算方法

$$a_{11}\begin{vmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{vmatrix} - a_{21}\begin{vmatrix} a_{12} & a_{13} \\ a_{32} & a_{33} \end{vmatrix} + a_{31}\begin{vmatrix} a_{12} & a_{13} \\ a_{22} & a_{23} \end{vmatrix}$$

#### 例題

$\begin{vmatrix} 5 & 2 & -1 \\ 4 & 1 & 0 \\ 3 & -1 & 1 \end{vmatrix} = 5\begin{vmatrix} 1 & 0 \\ -1 & 1 \end{vmatrix} - 4\begin{vmatrix} 2 & -1 \\ -1 & 1 \end{vmatrix} + 3\begin{vmatrix} 2 & -1 \\ 1 & 0 \end{vmatrix}$

$= 5 \times (1-0)- 4 \times (2-1)* 3 \times (0-(-1))
  = 4$

### 固有値

- 行列: A
- ベクトル: x
- スカラー: λ

上記について、Ax = λx が成り立つとき

- ベクトル x → 固有ベクトル
- 係数 λ → 固有値

### 固有値分解

以下のような
固有値を対角に並べた

- 正方行列 $A$
- ベクトル V

$$
\Lambda = \begin{pmatrix}
\lambda_1 & 0 &\dots & \\
0 & \lambda_2 & \\
& & \ddots
\end{pmatrix}
$$

$$
V = \begin{pmatrix}
\vec{v}_1 & \vec{v}_2 & \cdots
\end{pmatrix}
$$

固有値 $\lambda_1, \lambda_2, \cdots$

固有ベクトル $\vec{v}_1, \vec{v}_2, \cdots$

$A = V\Lambda V^{-1}$

と変形できる。
3 つの行列の積に変換すること → 固有値分解
※累乗計算が楽にできる

### 特異値分解

正方行列以外で固有値分解のようなことを行う
$$M\vec{v} = \sigma\vec{u}$$
$$M^T\vec{u} = \sigma\vec{v}$$
このような単位ベクトルがあるならば特異値分解できる。

$$M = USV^{-1}$$
$$MV = US \qquad M^TU = VS^T$$
$$M = USV^{-1} \qquad M^T = VS^TU^{-1}$$
$$MM^T = USV^{-1}VS^TU^{-1} = USS^TU^{-1}$$

つまり $MM^T$ を固有値分解すれば、その左特異ベクトルと特異値の 2 乗が求められることがわかる。

## 第 2 章 確率・統計

### 種類

- 頻度確率(客観確率)
  発生する頻度
- ベイズ確率(主観確率)
  信念の度合い

### 定義

$$P(A) = \frac{n(A)}{n(U)}$$

### 条件付き確率

ある事象 $X=x$ が与えられた下で、$Y=y$ となる確率

例: 雨が降っている条件下で交通事故に遭う確率

$$P(Y=y|X=x) = \frac{P(Y=y, X=x)}{P(X=x)}$$

### 同時確率

お互いの発生には因果関係のない事象 A と B が同時に発生する確率

### 確率変数 & 確率分布

- 事象と結び付けられた数値 & ・事象の発生する確率の分布
- 事象そのものを指すと解釈する場合も多い & ・離散値であれば表に示せる

例

| 事象     | 裏 0 表 4 | 裏 1 表 3 | 裏 2 表 2 | 裏 3 表 1 | 裏 4 表 0 |
| -------- | --------- | --------- | --------- | --------- | --------- |
| 確率変数 | 4         | 3         | 2         | 1         | 0         |
| 回数     | 75        | 300       | 450       | 300       | 75        |
| 確率     | 1/16      | 4/16      | 6/16      | 4/16      | 1/16      |

### 期待値

- 平均のようなもの
- 期待値の性質
  確率変数が一定の場合、期待値=確率変数
  確率変数に加算・乗算すると、期待値も同様の加算・乗算した結果になる

| 事象          | X1    | X2    | ... | Xn    |
| ------------- | ----- | ----- | --- | ----- |
| 確率変数 f(X) | f(x1) | f(x2) | ... | f(xn) |
| 確率 P(X)     | P(x1) | P(x2) | ... | P(xn) |

- 期待値 E(f)
  $$=\Sigma P(X=x_k) f(X=x_k) (k=1からnまで)$$

連続する値の場合は微分取る

- 期待値 E(f)
  $$=\int P(X=x) f(X=x) dx$$

ベイズの定理
条件付き確率を用いて事後確率を計算する公式
与えられた事象 (A) と (B) に対して、ベイズの定理は以下のように表されます

---

### 分散

- 分散 Var(f)

  - データの散らばり具合
  - 期待値からのズレの平均
    $$E((f(X=x)-E(f))^2)$$
    $$E((f^2(X=x)-(E(f))^2$$

- 共分散 Cov(f, g)

  - 2 つのデータ系列の傾向の違い。
    - 正の値: にてる
    - 負の値: にてない(逆傾向)
    - 0: 無関係
    - [X の偏差\*Y の偏差]の平均

  $$E\left[(f(X) - E[f])(g(Y) - E[g])\right] $$
 $$ = E[fg] - E[f]E[g]$$

- 標準偏差
  - 単位問題: 分散は単位が異なる。 ※例）$m:長さ \to m^2:面積$

$$\sigma = \sqrt{\text{Var}(f)} = \sqrt{E\left[(f(X) - E[f])^2\right]}$$
分散の平方根で単位を戻す。

### 確率分布

- ベルヌーイ分布
  - コイントスのイメージ
  - 裏と表で出る割合が等しくなくても扱える
    $$P(x|\mu) = \mu^x (1 - \mu)^{1-x}$$
- マルチヌーイ (カテゴリカル) 分布
  - さいころを転がすイメージ
  - 各面の出る割合が等しくなくても扱える
- 二項分布
  - ベルヌーイ分布の多試行版
    $$P(x|\lambda, n) = \frac{n!}{x!(n-x)!} \lambda^x (1 - \lambda)^{n-x}$$
- ガウス分布
  - 釣鐘型の連続分布
    $$\mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{1}{2\sigma^2} (x - \mu)^2 \right)$$

### 集合

- 数学的には下記の様に表される。

  - $S = \{a, b, c, d, e, f, g\}$

- また集合 S について、a が含まれていることは下記のように表される。

  - $a \in  S$

    ※a はこれ以上分解できない要素

- 和集合 $A \cup B$
- 共通部分 $A \cap B$

- 絶対補

- 集合 $A$ の絶対補集合 $\bar{A}$ は、全体集合 $U$ から $A$ を除いた部分を表す。

$$U \setminus A = \bar{A} $$

- 相対補

  - 集合 $A$ の相対補集合 $B \setminus A$ は、集合 $B$ から $A$ を除いた部分を表す。

$$B \setminus A = \{ x \in B \mid x \notin A \}$$

## 第 3 章 情報理論

### 自己情報量

- どれぐらい珍しい？
- 対数の底が 2 のとき，単位はビット(bit)
- 対数の底がネイピアの e のとき，単位は(nat)
  $$I(x) = -\log(P(x)) = \log(W(x))$$
  ※on/off スイッチで情報量を表す →log でスイッチ数がわかる

### シャノンエントロピー

- 微分エントロピーともいうが，微分しているわけではない(自己情報量の平均値)
- 自己情報量の期待値

$$
H(x) = E[I(x)]
= -E[\log P(x)]
= -\sum P(x) \log{P(x)}
$$

### カルバック・ライブラー ダイバージェンス

- 同じ事象・確率変数における異なる確率分布 P,Q の違いを表す

$$
D_{KL}(P||Q)
=E_{x \sim P} \left[ \log \frac{P(x)}{Q(x)} \right]
=E_{x \sim P} \left[ \log P(x) - \log Q(x) \right]
$$

※$log\frac{P(x)}{Q(x)}$について

$$
I(Q(x)) - I(P(x)) = (-\log(Q(x))) - (-\log(P(x))) = \log \frac{P(x)}{Q(x)}
$$

### 交差エントロピー

- KL ダイバージェンスの一部分を取り出したもの
- Q についての自己情報量を P の分布で平均している

$$
D_{KL}(P||Q) = \sum_x P(x) (-\log(Q(x))) - (-\log(P(x)))
H(P, Q) = H(P) + D_{KL}(P||Q)
H(P, Q) = -E_{x\sim P} \log Q(x)
= -\sum_x P(x) \logQ(x)
$$

### 自己情報量とエントロピーまとめ

- 自己情報量[bit]：𝐼 𝑥 = − log2 𝑝 𝑥
  - 特定の事象が持つ情報量を定量化した値
  - 直感的には、事象を観測した時の驚きの度合いの大きさを表す
    → 宝くじの場合 1 等が出る事象は「驚きの度合いが大きい」=自己情報量も高い
- エントロピー[bit]：𝐻 = − ∑𝑥∈𝑋 𝑝 𝑥 log2 𝑝 𝑥
  - 事象全体の「自己情報量の期待値」
  - 直感的には、事象全体の不確実性の大きさや予測の難しさを表す- 例：宝くじとコイントスのエントロピー
    → コイントスは表裏共に 50%で完全にランダム=不確実性が大きい
    宝くじはハズレが 90%=不確実性が小さい（ハズレと予測すれば、90%は正解できる）
    ※ 𝑥 ：事象、 𝑝 𝑥 ： 𝑥 の発生確率

### 相互情報量

- 相互情報量[bit]
  ：𝐼 𝑋; 𝑌 = ∑𝑥∈𝑋,𝑦∈𝑌 𝑝 𝑥, 𝑦 log2
  - 事象 𝑋 の観測によって、事象 𝑌 の不確実性（エントロピー）を減少させられる度合い
  - 直感的には、事象 𝑋 と 𝑌 の依存度合いや影響の度合いを表す

### 条件付きエントロピーと結合エントロピー

- 条件付きエントロピー[bit]：𝐻 𝑌|𝑋 = − ∑𝑥∈𝑋 𝑝 𝑥 ∑𝑦∈𝑌 𝑝 𝑦|𝑥 log2 𝑝 𝑦|𝑥

  - 事象 𝑋 が観測された上で残った事象 𝑌 の不確実性（エントロピー）
  - 直感的には、事象 𝑋 を知った上での事象 𝑌 の予測の難しさを表す

- 結合エントロピー[bit]：𝐻 𝑋, 𝑌 = − ∑𝑥∈𝑋,𝑦∈𝑌 𝑝 𝑥, 𝑦 log2 𝑝 𝑥, 𝑦
  - 2 つ以上の事象が同時に発生する全てのパターンの不確実性（エントロピー）の総和
  - 直感的には、2 つ以上の事象の全ての同時発生パターンの多様性や複雑さを表す

### JS ダイバージェンス

- 2 つの確率分布の類似性を測る尺度
- 2 つの確率分布の位置が近く、かつ形状が類似していれば値が小さく、異なれば値が大きくなる（0~1 の範囲）
- 例：地域ごとのアイスクリームの販売量
  - 東京都と大阪府のアイスクリームの販売量の確率分布の JS ダイバージェンスを算出
  - JS ダイバージェンスが小さい場合：東京都と大阪府の販売量の傾向が似ている → 同じマーケティング戦略を使える
  - JS ダイバージェンスが大きい場合：東京都と大阪府の販売量の傾向が異なる → それぞれ異なるマーケティング戦略が必要
- 類似性を測る尺度である「KL ダイバージェンス」に基づくが、KL ダイバージェンスと比べて
  下記の違いがある
  - 対称性：類似性を測る際の基準となる確率分布を入れ替えても同じ値となる（ 𝐽𝑆 𝑃ԡ𝑄 = 𝐽𝑆 𝑄ԡ𝑃 ）
  - 値が有限：無限大に発散することがなく、値が 0~1.0 に収まる（底が 2 の場合）

### KL ダイバージェンス

- 2 つの確率分布の類似性を測る尺度
  - 2 つの確率分布の位置が近く、かつ形状が類似していれば値が小さく、異なれば値が大きくなる（0 以上）
  - JS ダイバージェンスが 2 つの確率分布の距離や形状の違いを評価するイメージである一方で、KL ダイバージェンス
    は一方の確率分布をもう一方の確率分布で近似した場合の情報損失を表すイメージ
- JS ダイバージェンスと比べて下記の違いがある
  - 非対称性：類似性を測る際の基準となる確率分布を入れ替えると値が異なる（ 𝐾𝐿 𝑃ԡ𝑄 ≠ 𝐾𝐿 𝑄ԡ𝑃 ）
  - 一方の確率分布をもう一方で近似する考え方であるため、P を Q で近似した場合と Q を P で近似した場合の結果は異なる
  - 機械学習の分野で頻繁に使用される（P を真の分布、Q を予測分布として、KL ダイバージェンスの値を最小化する）
  - 値が有限ではない： 𝑃 𝑥 > 0 かつ 𝑄 𝑥 = 0 の条件で、KL ダイバージェンスの値が無限大となる
  - 無限大となると計算不能となってしまうため、KL ダイバージェンスを分析等に使う際は注意する必要あり

### クロスエントロピーと KL ダイバージェンス

- クロスエントロピー：2 つの確率分布のエントロピーを測る尺度
  - 2 つの確率分布 P と Q において、P の分布を Q の分布がどの程度正確に表現（近似）できているかを示す
  - P で Q を正確に表現できていれば値が小さくなり、表現できていなければ値が大きくなる
- クロスエントロピーと KL ダイバージェンスの関係性
  - クロスエントロピー・KL ダイバージェンスは、ともに 2 つの確率分布の類似性を測る尺度と捉えることができ、
    密接な関係性を持つ
  - KL ダイバージェンスは、2 つの確率分布 P・Q が与えられた場合、下記式より算出可能
- 式より、分布 P を分布 Q でどれだけ正確に表現（近似）できているかを定量的に評価可能
  - クロスエントロピー H(P,Q)は、Q が P と完全に同じ場合に P のエントロピーとなり、結果として KL は 0（最小値）となる
  - クロスエントロピーは KL と同様に機械学習の分野で頻繁に使用される（P を真の分布、Q を予測分布として、クロスエント
    ロピーの値を最小化する）

### 機械学習での応用例

- クロスエントロピー
  - Deep learning を用いた分類問題などで頻繁に使用される
  - 例：Deep learning による文字画像の分類
  - 畳み込みニューラルネットワークを用いて、文字の画像のクラス（0,1,2…）を予測。損失関数にクロスエントロピーを使用。
- KL ダイバージェンス
  - Deep learning を用いた画像生成などで頻繁に使用される
  - 例：Generative Adversarial Network による画像生成
