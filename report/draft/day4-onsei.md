- [音声認識](#音声認識)
  - [機械学習のための音声データの扱い](#機械学習のための音声データの扱い)
    - [音声データとAI (1/2)](#音声データとai-12)
    - [音声データとAI (2/2)](#音声データとai-22)
    - [そもそも、音声データとは?](#そもそも音声データとは)
      - [音が聞こえる仕組み](#音が聞こえる仕組み)
      - [音波](#音波)
      - [周波数と角周波数](#周波数と角周波数)
    - [フーリエ変換の概算(補足)](#フーリエ変換の概算補足)
  - [フーリエ変換の公式](#フーリエ変換の公式)
  - [スペクトログラムの特徴](#スペクトログラムの特徴)
- [CTC](#ctc)
  - [音声認識の概要](#音声認識の概要)
  - [基本的な音声認識処理の流れ](#基本的な音声認識処理の流れ)
    - [特徴量抽出： 音声信号を認識しやすい表現に変換する](#特徴量抽出-音声信号を認識しやすい表現に変換する)
    - [従来の音声認識モデルの構造](#従来の音声認識モデルの構造)
    - [3つの構成要素](#3つの構成要素)
      - [音響モデル](#音響モデル)
    - [発音辞書](#発音辞書)
    - [言語モデル](#言語モデル)
    - [従来手法の問題点](#従来手法の問題点)
    - [End-to-Endモデルの登場](#end-to-endモデルの登場)
    - [CTC (Connectionist Temporal Classification)](#ctc-connectionist-temporal-classification)
    - [CTCにおける重要な発明](#ctcにおける重要な発明)
    - [前提](#前提)
    - [ブランクの導入](#ブランクの導入)
    - [縮約（Contraction）の手順](#縮約contractionの手順)
    - [損失関数](#損失関数)
    - [確率計算の詳細](#確率計算の詳細)
    - [効率的な計算：前向き・後ろ向きアルゴリズム](#効率的な計算前向き後ろ向きアルゴリズム)
    - [パスの分類](#パスの分類)
    - [確率の和の法則により：](#確率の和の法則により)
- [DCGAN](#dcgan)
    - [GANについて](#ganについて)
    - [応用技術の紹介](#応用技術の紹介)
  - [2プレイヤーのミニマックスゲームとは？](#2プレイヤーのミニマックスゲームとは)
  - [GANの価値関数はバイナリークロスエントロピー](#ganの価値関数はバイナリークロスエントロピー)
  - [最適化方法](#最適化方法)
  - [なぜGeneratorは本物のようなデータを生成するのか？](#なぜgeneratorは本物のようなデータを生成するのか)
  - [ステップ1: 価値関数を最大化する$D(x)$の値は？](#ステップ1-価値関数を最大化するdxの値は)
  - [ステップ1: 価値関数を最大化する$D(x)$の値は？](#ステップ1-価値関数を最大化するdxの値は-1)
  - [ステップ2: 価値関数はいつ最小化するか？](#ステップ2-価値関数はいつ最小化するか)
  - [ステップ2: 価値関数はいつ最小化するか？](#ステップ2-価値関数はいつ最小化するか-1)
  - [GANの問題点](#ganの問題点)
  - [Wasserstein GAN (WGAN)](#wasserstein-gan-wgan)
  - [Wasserstein GAN (WGAN)](#wasserstein-gan-wgan-1)
  - [CycleGAN](#cyclegan)
  - [CycleGAN](#cyclegan-1)
- [Conditional GAN](#conditional-gan)
  - [問題1](#問題1)
  - [問題2](#問題2)
  - [*Generator*: G(Z)G(Z)](#generator-gzgz)
  - [CGANのネットワーク](#cganのネットワーク)
  - [Generator: G(Z∣Y)G(Z|Y)](#generator-gzygzy)
  - [重要なポイント](#重要なポイント)
    - [具体例（MNIST）](#具体例mnist)
- [Pix2Pix](#pix2pix)
  - [Pix2pix の概要](#pix2pix-の概要)
    - [簡単なおさらい：GAN（1/2）](#簡単なおさらいgan12)
    - [簡単なおさらい：GAN（2/2）](#簡単なおさらいgan22)
    - [Pix2pix：概要](#pix2pix概要)
    - [Pix2pix：学習データ](#pix2pix学習データ)
    - [Pix2pix：ネットワーク](#pix2pixネットワーク)
    - [Pix2pix：工夫](#pix2pix工夫)
- [A3C](#a3c)
  - [A3Cとは](#a3cとは)
    - [A3Cによる非同期（Asynchronous）学習の詳細](#a3cによる非同期asynchronous学習の詳細)
    - [並列分散エージェントで学習を行うA3Cのメリット](#並列分散エージェントで学習を行うa3cのメリット)
      - [安定化について](#安定化について)
    - [A3Cの難しいところとA2Cについて](#a3cの難しいところとa2cについて)
    - [A3Cのロス関数](#a3cのロス関数)
    - [分岐型Actor-Criticネットワーク](#分岐型actor-criticネットワーク)
    - [方策勾配法の基本](#方策勾配法の基本)
    - [θを勾配法で最適化する](#θを勾配法で最適化する)
  - [A3Cの特徴](#a3cの特徴)
    - [A3Cのアルゴリズムの特徴としては、勾配を推定する際に：](#a3cのアルゴリズムの特徴としては勾配を推定する際に)
    - [A3CのAtari 2600における性能検証](#a3cのatari-2600における性能検証)
  - [距離学習 (Metric learning)](#距離学習-metric-learning)
    - [距離学習とは](#距離学習とは)
  - [Deep Metric Learning](#deep-metric-learning)
  - [Siamese Network](#siamese-network)
  - [Triplet Network](#triplet-network)
- [MAML(メタ学習)](#mamlメタ学習)
  - [MAMLが解決したい課題](#mamlが解決したい課題)
    - [深層学習モデル開発に必要なデータ量](#深層学習モデル開発に必要なデータ量)
  - [MAMLのコンセプト](#mamlのコンセプト)
  - [MAMLの学習手順](#mamlの学習手順)
  - [MAMLの効果](#mamlの効果)
  - [MAMLの課題と対処](#mamlの課題と対処)
- [グラフ畳み込み(GCN)](#グラフ畳み込みgcn)
    - [教科書の数式とは違うように見えるのは？](#教科書の数式とは違うように見えるのは)
  - [Spatialな場合](#spatialな場合)
    - [Spatialな場合（どんな手順で？）](#spatialな場合どんな手順で)
- [Grad-CAM, LIME, SHAP](#grad-cam-lime-shap)
  - [ディープラーニングモデルの解釈性](#ディープラーニングモデルの解釈性)
    - [CAM](#cam)
    - [Grad-CAM](#grad-cam)
    - [LIME](#lime)
    - [参考文献から補足](#参考文献から補足)
    - [SHAP](#shap)
- [Docker](#docker)
  - [コンテナ型仮想化](#コンテナ型仮想化)
  - [Dockerとは](#dockerとは)
  - [Dockerの用途](#dockerの用途)
  - [基本的な Docker コマンド](#基本的な-docker-コマンド)
  - [Dockerfile の作成方法](#dockerfile-の作成方法)
  - [GPU 環境におけるDockerの使用](#gpu-環境におけるdockerの使用)
  - [コンテナオーケストレーション](#コンテナオーケストレーション)
    - [参考文献補足](#参考文献補足)

# 音声認識
([目次に戻る](#目次))

## 機械学習のための音声データの扱い

### 音声データとAI (1/2)

音声データを処理する能力を持つAIの研究・開発が近年多くなされている背景には、以下の理由があります。

* 利便性の向上
* 業務の生産性の向上
* 他の技術と組み合わさることができる

**活用事例:**

* スマートスピーカー
* 音声アシスタント
* 会議などで使われる自動議事録AI

### 音声データとAI (2/2)

音声認識をタスクとしたデータ分析コンペも多数開催されています。

* **Kaggle Freesound Audio Tagging 2019:**
    短い音声データからギターや犬の鳴き声などタグ付けするタスク。
* **Kaggle BirdCLEF2021: Processing audio data:**
    鳥の鳴き声の音声データから、各鳴き声に対応する鳥の種類を推測するタスク。

$\rightarrow$ 音声データをどうやって扱うか？

### そもそも、音声データとは?

#### 音が聞こえる仕組み

音はどのように耳に伝わるか？

* 物体の振動による空気の振動
* 空気のない宇宙では音は聞こえない

**音が聞こえるイメージ:**

音源 $\rightarrow$ 空気中（振動が伝わる）$\rightarrow$ 空気中 = 音が聞こえる

#### 音波

空気の振動による音の波。ある地点における波形。

**音波の特性:**

* **振幅:** 音の大きさ
* **波長:** 音の高さ

**音の特徴:**

* 振幅が大きい $\rightarrow$ 大きい音
* 波長が大きい $\rightarrow$ 低い音
* 1波長分の時間 = 1周期

#### 周波数と角周波数

**定義:**

* **周波数:** 一秒あたりの振動数（周期数）
* **角周波数:** 周波数を回転する角度で表現

**例:**

1秒間で3回振動 = 周波数は3

**ラジアンの確認:**

* $\pi = 180^\circ$
* $\frac{\pi}{2} = 90^\circ$
* $\frac{\pi}{4} = 45^\circ$

**角周波数の段階的表現:**

* $\frac{\pi}{4}$ ラジアン
* $\frac{\pi}{2}$ ラジアン
* $\pi$ ラジアン

### フーリエ変換の概算(補足)

* 全ての波形は正弦波・余弦波で表せる

$$
f(x) = a + \sum_{n=1}^{\infty} a_n \cos nx + b_n \sin nx
$$

（マクローリン展開より）

* オイラーの公式：$e^{i\theta} = \cos\theta + i\sin\theta$などを用いて、

## フーリエ変換の公式

$$
f(x) = \int_{-\infty}^{\infty} \left[ \int_{-\infty}^{\infty} f(t)e^{-i\omega t} dt \right] e^{i\omega x} d\omega
$$

* $f(x)$：波形
* $\int_{-\infty}^{\infty} f(t)e^{-i\omega t} dt$：ある波の振幅
* $e^{i\omega x} d\omega$：ある周波数の波

（※ $f(t)$はある条件を満たす）

ある波の振幅：
$$
F(\omega) = \int_{-\infty}^{\infty} f(t)e^{-i\omega t} dt
$$

## スペクトログラムの特徴
横軸：時間
縦軸：周波数
輝度：振幅
スペクトル → スペクトログラム（時間軸を追加）

参考： https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%9A%E3%82%AF%E3%83%88%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0

# CTC
([目次に戻る](#目次))
CTC (Connectionist Temporal Classification)

## 音声認識の概要
音声認識 (ASR: Automatic Speech Recognition) とは、

- 入力信号を音声特徴ベクトルに変換、その音声特徴ベクトルの系列から対応する単語列を推定するタスク
- 近年、深層学習技術の進展に伴い音声認識モデルの精度は向上しており、様々な製品に応用

## 基本的な音声認識処理の流れ
音声信号 → [特徴量抽出] → 音声特徴量 → [音声認識モデル] → 認識結果

### 特徴量抽出： 音声信号を認識しやすい表現に変換する

主にフーリエ変換を用いて音声信号を周波数ごとの音の強度に分解
人間の聴覚器官・発声器官の生物学的知見に基づく様々な処理

音声認識モデル： 音声認識結果の候補の確率を計算

「この音声は"天気"と言っている確率が0.8である」というように計算

### 従来の音声認識モデルの構造
音声特徴量 → [音響モデル] → [発音辞書] → [言語モデル] → 認識結果

### 3つの構成要素

#### 音響モデル
音声特徴量と音素列の間の確率を計算するモデル

> 音素：/a/や/i/といった母音、/k/や/s/といった子音から構成される音の最小単位

音を聴いて「これは"あ(/a/)"かな？それとも"い(/i/)"かな？」といった音素ごとの確率を求める

### 発音辞書

音素列と単語との対応を扱うモデル
「おはよう：/o/h/a/y/o」のような単語とその発音（音素列）が記述されたリスト

### 言語モデル

ある単語に対して、その単語が発話される確率を計算
文脈的にその単語が表れやすいかの確率を計算

### 従来手法の問題点

- 実装の複雑さ： 
  3つのモジュールの出力をうまく統合して音声認識結果を出力する「デコーダ」の実装が困難

- モジュール間の統合： 
  複数のモジュールの統合処理が複雑

### End-to-Endモデルの登場
2015年ごろから音響モデル、発音辞書、言語モデルを**1つのDNNで表現するEnd-to-Endモデル（E2Eモデル）**の研究が活発化

利点： 構成がシンプルで比較的簡単に実装可能
CTCはこのようなEnd-to-Endモデルの1つ

### CTC (Connectionist Temporal Classification)
CTCはEnd-to-Endモデルの中でも比較的初期に提案されたモデルで、従来手法のように隠れマルコフモデル（HMM）を使用せずにディープニューラルネットワーク（DNN）だけで音響モデルを構築する手法として提案 [Graves+,2006]。

### CTCにおける重要な発明
ブランク（blank）と呼ばれるラベルの導入
前向き・後ろ向きアルゴリズム（forward-backward algorithm）を用いたDNNの学習

### 前提
CTCでは基本的には音声のような時系列情報を扱うため、RNN（recurrent neural network）やLSTM（Long Short Term Memory）のような時系列を考慮したDNNを用います

### ブランクの導入
RNNに音声系列を入力すると、フレーム数だけ出力が得られます。
例： 8フレームの音声系列をRNNに入力し、出力値（確率）が最も高いラベル（音素）をフレーム毎に出力した結果：
[a, −, −, b, b, −, c, c]
> ここで「−」はブランクを表しています。

### 縮約（Contraction）の手順
- CTCでは、次の手順でフレーム単位のラベル系列を最終的なテキスト系列に変換します：
- 連続して出現している同一ラベルを1つのラベルにまとめる
- ブランク「−」を削除する

例：
[a, −, −, b, b, −, c, c] 
→ [a, −, b, −, c]  (連続ラベルをまとめる)
→ [a, b, c]        (ブランクを削除)
この縮約を関数BB
Bで表すと：

B(a,−,−,b,b,−,c,c)=[a,b,c] B(a, −, −, b, b, −, c, c) = [a, b, c]B(a,−,−,b,b,−,c,c)=[a,b,c]
ブランクを導入する理由
1. 同一ラベルが連続するテキスト系列を表現するため

ブランクが存在しない場合：[a,a,a,a,b,b,b]→[a,b][a, a, a, a, b, b, b] [a, b] [a,a,a,a,b,b,b]→[a,b]に縮約されてしまう

[a,a,b][a, a, b]
[a,a,b]を表現できない

解決策：[a,a,−,a,a,b,b][a, a, −, a, a, b, b]
[a,a,−,a,a,b,b]のようにブランクを挿入


2. 厳密にアライメントを決定させないため

音素の境界は曖昧なことが多い
単語の間にはポーズ（間）などの非音声区間も存在
ブランクの出力を許可することで、モデルが無理なアライメント推定を行わず音声認識結果が正解することのみを目的とした学習可能

- 前向き・後ろ向きアルゴリズムを用いたRNNの学習
問題設定
8フレームの音声系列で、最終的なテキスト列が[a,b,c][a, b, c]
[a,b,c]となるようなRNNの出力は複数存在：

[a,−,b,b,b,c,−,−][a, −, b, b, b, c, −, −]
[a,−,b,b,b,c,−,−]
[−,−,a,−,b,b,−,c][−, −, a, −, b, b, −, c]
[−,−,a,−,b,b,−,c]
など

つまり：
B(a,−,b,b,b,c,−,−)=[a,b,c]B(a, −, b, b, b, c, −, −) = [a, b, c]B(a,−,b,b,b,c,−,−)=[a,b,c]
B(−,−,a,−,b,b,−,c)=[a,b,c]B(−, −, a, −, b, b, −, c) = [a, b, c]B(−,−,a,−,b,b,−,c)=[a,b,c]

- 事後確率の計算
入力音声系列xx
xに対して縮約後の出力テキスト系列がl=[a,b,c]l = [a, b, c]
$$
l = [a, b, c] \text{ となる事後確率は：}
$$

$$
P(l|x) = P([a, -, b, b, b, c, -, -]|x) + P([-, -, a, -, b, b, -, c]|x) + \cdots
$$

$$
= \sum_{\pi \in B^{-1}(l)} P(\pi|x) \tag{1}
$$
ここで：

P([a,−,b,b,b,c,−,−]∣x)P([a, −, b, b, b, c, −, −]|x)
P([a,−,b,b,b,c,−,−]∣x)：入力xx
xに対してRNNの（縮約前の）出力が[a,−,b,b,b,c,−,−][a, −, b, b, b, c, −, −]
[a,−,b,b,b,c,−,−]となる確率

B−1(l)B^{-1}(l)
B−1(l)：「縮約するとテキスト系列になるような縮約前のラベル系列の集合」


### 損失関数
CTCにおいて最小化すべき損失関数は：
LCTC=−log⁡P(l∗∣x)(2)L_{CTC} = -\log P(l^*|x) \tag{2}LCTC​=−logP(l∗∣x)(2)
ここでl∗l^*
l∗は正解テキスト系列です。

### 確率計算の詳細
P(l∗∣x)P(l^*|x)
P(l∗∣x)を計算する際には、以下のようなグラフを考えます：

フレーム: 1  2  3  4  5  6  7  8
ラベル:   a  −  −  b  b  −  c  c  (赤パス)
         −  −  a  −  b  b  −  c  (青パス)
各パスの確率は、パス上で各ラベルが出力される確率の積として計算：
$$
P([a,−,b,b,b,c,−,−]\,|\,x) = y_1^a \times y_2^{-} \times y_3^b \times y_4^b \times y_5^b \times y_6^c \times y_7^{-} \times y_8^{-} \tag{3}
$$
ここで $y_t^k$ は、フレーム $t$ でラベル $k$ が出力される確率を表します。

一般的な計算式は次の通りです：

$$
P(l^*\,|\,x) = \sum_{\pi \in B^{-1}(l^*)} P(\pi\,|\,x) = \sum_{\pi \in B^{-1}(l^*)} \prod_{t=1}^T y_t^{\pi_t} \tag{4}
$$

ここで：

TT
T：フレーム数

πt\pi_t
πt​：パスπ\pi
πのフレームtt
tにおけるラベル

### 効率的な計算：前向き・後ろ向きアルゴリズム
縮約してl∗=[a,b,c]l^* = [a, b, c]
l∗=[a,b,c]となるパスは大量にあるため、全てを愚直に計算するのは非効率です。そのため実際のCTCでは、**前向き・後ろ向きアルゴリズム（forward-backward algorithm）**が用いられます。

### パスの分類
π∈B−1(l∗)\pi \in B^{-1}(l^*)
π∈B−1(l∗)なるパスを、あるフレームtt
tにおいてどの頂点を通るかに注目して分類します。

### 確率の和の法則により：
P(l∗∣x)=∑π∈B−1(l∗)P(π∣x)=∑s=1∣l′∣∑π∈B−1(l∗),πt=4=ls′P(π∣x)(5)P(l^*|x) = \sum_{\pi \in B^{-1}(l^*)} P(\pi|x) = \sum_{s=1}^{|l'|} \sum_{\pi \in B^{-1}(l^*), \pi_{t=4} = l'_s} P(\pi|x) \tag{5}P(l∗∣x)=π∈B−1(l∗)∑​P(π∣x)=s=1∑∣l′∣​π∈B−1(l∗),πt=4​=ls′​∑​P(π∣x)(5)
ここでl′l'
l′は
拡張ラベルと呼ばれ、正解ラベル系列にブランクを挿入した系列を表します。


# DCGAN
([目次に戻る](#目次))

### GANについて

- GANの構造
  ミニマックスゲームと価値関数

- GANの最適化方法
  本物のようなデータを生成できる理由

- DCGANについて
  具体的なネットワーク構造

###  応用技術の紹介

- GAN(Generative Adversarial Nets)とは
  生成器と識別器を競わせて学習する生成&識別モデル

> Generator: 乱数からデータを生成
> Discriminator: 入力データが真データ（学習データ）であるかを識別
## 2プレイヤーのミニマックスゲームとは？

* 1人が自分の勝利する確率を最大化する作戦を取る。
* もう一人は相手が勝利する確率を最小化する作戦を取る。

* GANでは価値関数$V(D,G)$に対し、$D$が最大化、$G$が最小化を行う。
$$
\min_G \max_D V(D,G)
$$
$$
V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$

* バイナリークロスエントロピーと似ていますか？
$$
L = -\sum y \log \hat{y} + (1-y)\log(1-\hat{y})
$$

---

## GANの価値関数はバイナリークロスエントロピー

* 単一データのバイナリークロスエントロピー
$$
L = -y \log \hat{y} + (1-y)\log(1-\hat{y})
$$
    * $y$: 真値（ラベル）
    * $\hat{y}$: 予測値（確率）

* 真データを扱う時: $y=1, \hat{y}=D(x) \implies L = -\log D(x)$
* 生成データを扱う時: $y=0, \hat{y}=D(G(z)) \implies L = -\log(1 - D(G(z)))$

* 2つを足し合わせる
$$
L = (-\log D(x)) + [\log(1-D(G(z)))]
$$
$$
V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$

* 複数データを扱うために期待値を取る
* 期待値: 何度も試行する際の平均的な結果値 $\sum xP(x)$

## 最適化方法

* **Generatorのパラメータ $\theta_g$ を固定**
    * 真データと生成データを$m$個ずつサンプル
    * $\theta_d$を勾配上昇法(Gradient Ascent)で更新
    $$
    \theta_d \leftarrow \theta_d + \eta \frac{1}{m} \sum_{i=1}^m [\log D(x^{(i)}) + \log(1 - D(G(z^{(i)})))]
    $$
    （$\eta$は勾配更新を$k$回更新）

* **Discriminatorのパラメータ $\theta_d$ を固定**
    * 生成データを$m$個ずつサンプル
    * $\theta_g$を勾配降下法(Gradient Descent)で更新
    $$
    \theta_g \leftarrow \theta_g - \eta \frac{1}{m} \sum_{i=1}^m [\log(1 - D(G(z^{(i)})))]
    $$
    （$\eta$を1回更新）
## なぜGeneratorは本物のようなデータを生成するのか？

* 生成データが本物のような状況とは
    * $p_g = p_{data}$ であるはず

* 価値関数が $p_g = p_{data}$ の時に最適化されていることを示せばよい
$$
\min_G \max_D V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$

* 二つのステップにより確認する
    1. $G$を固定し、価値関数が最大値を取るときの$D(x)$を算出
    2. 上記の$D(x)$を価値関数に代入し、$G$が価値関数を最小化する条件を算出

---

## ステップ1: 価値関数を最大化する$D(x)$の値は？

* Generatorを固定
$$
V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$
$$
= \int_x p_{data}(x) \log D(x) dx + \int_z p_z(z) \log(1 - D(G(z))) dz
$$
$$
= \int_x p_{data}(x) \log D(x) + p_g(x) \log(1 - D(x)) dx
$$
$y=D(x), a=p_{data}(x), b=p_g(x)$と置けば
$$
a \log(y) + b \log(1-y)
$$

$a \log(y) + b \log(1-y)$の極値を求めよう。

---

## ステップ1: 価値関数を最大化する$D(x)$の値は？

$a \log(y) + b \log(1-y)$ を $y$ で微分
$$
\frac{a}{y} - \frac{b}{1-y} = 0
$$
$$
\frac{a}{y} = \frac{b}{1-y}
$$
$$
a(1-y) = by
$$
$$
a - ay = by
$$
$$
a = (a+b)y
$$
$$
y = \frac{a}{a+b}
$$

$y = D(x), a = p_{data}(x), b = p_g(x)$ なので
$$
D(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}
$$

* 価値関数が最大値を取るときの$D(x)$が判明

## ステップ2: 価値関数はいつ最小化するか？

* 価値関数の $D(x)$ を $\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$ で置き換え
$$
V = \mathbb{E}_{x \sim p_{data}(x)} \left[ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g(x)} \left[ \log \left( 1 - \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right) \right]
$$
$$
= \mathbb{E}_{x \sim p_{data}(x)} \left[ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g(x)} \left[ \log \frac{p_g(x)}{p_{data}(x) + p_g(x)} \right]
$$

* 二つの確率分布がどれぐらい近いのか調べる必要がある
* 有名な指標としてJSダイバージェンスがある
$$
JS(p_1 || p_2) = \frac{1}{2} (\mathbb{E}_{x \sim p_1} [\log \frac{p_1}{p_2}] + \mathbb{E}_{x \sim p_2} [\log \frac{p_2}{p_1}])
$$
* JSダイバージェンスは非負で、分布が一致する時のみ0の値を取る

---

## ステップ2: 価値関数はいつ最小化するか？

* 価値関数を変形
$$
V = \mathbb{E}_{x \sim p_{data}(x)} \left[ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g(x)} \left[ \log \frac{p_g(x)}{p_{data}(x) + p_g(x)} \right]
$$
$$
= \mathbb{E}_{x \sim p_{data}(x)} \left[ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g(x)} \left[ \log \frac{p_g(x)}{p_{data}(x) + p_g(x)} \right] - 2 \log 2 + 2 \log 2
$$
$$
= 2 JS(p_{data} \ || \ p_g) - 2 \log 2
$$

* $\min_G V$ は $p_{data} = p_g$ のときに最小値となる ($-2 \log 2 \approx -1.386$)
* GANの学習により$G$は本物のようなデータを生成できる
## GANの問題点

* **モード崩壊 (mode collapse)**
    * 識別器をだましやすい特定の画像形式（モード）を学習
    * 生成画像の多様性が消滅


* **学習が不安定**
    * 勾配消失問題

* **意味のない損失**
    * 識別器と生成器は学習ごとに更新される
    * 学習ごとの損失関数と画像品質に相関なし

---

## Wasserstein GAN (WGAN)

Wasserstein GAN (WGAN) は、分布間の距離を定義するWasserstein DistanceをGANの損失関数に導入して安定な学習を可能にします。

* **GANの目的**


* **従来のGANの損失関数**
    * Jensen-Shannon (JS) divergence
    * 勾配消失問題

* **WGANの損失関数**
    * Wasserstein Distance (Earth-Mover's distance)
    * 有意味な勾配

Wasserstein GAN (2017)  
[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)

## Wasserstein GAN (WGAN)

Wasserstein GAN (WGAN) の損失関数は、学習に応じて生成画像の質の向上とともに安定して減少します。

* **WGANの損失関数**

    * Wasserstein Distance
        $$
        L_D^{WGAN} = -\mathbb{E}_{x \sim p_{data}}[D(x)] - \mathbb{E}_{z \sim p_z}[D(G(z))]
        $$
        $$
        L_G^{WGAN} = -\mathbb{E}_{z \sim p_z}[\log D(G(z))]
        $$

    * K-Lipschitz continuous ($K=1$)
        $$
        \frac{|D(x_1) - D(x_2)|}{||x_1 - x_2||} \leq K
        $$

    * 勾配クリッピング (WGAN)
        $-c \leq W \leq c \quad (c=0.01)$

    * 勾配ペナルティ (WGAN-GP)
        $\nabla D \sim 1$

Wasserstein GAN (2017)  
[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)

Improved Training of Wasserstein GANs (2017)  
[https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)

## CycleGAN

CycleGANやPix2Pixは、GANを用いて画像様式を変換する生成モデルです。

* **画像変換**: 画像を入力として異なる画像を生成

* **Pix2Pix [1]**
    * ペア画像必要
    * 教師あり学習
    * 一方向変換
    
* **CycleGAN [2]**
    * ペア画像不要
    * 教師なし学習
    * 双方向変換

## CycleGAN

CycleGANは、2つの画像集合間を双方向に変換でき、4つのニューラルネットワーク構造を持ちます。

* **データセット**: 異なる特徴を持つ画像集合 $\{A\}$, $\{B\}$


* **2つの識別器** ($D_A$, $D_B$)

    * **敵対的損失** (adversarial loss)
    $$
    L_{adv}^{D_A} = \mathbb{E}_{a \sim P_{data}[A]} [(D_A(a) - 1)^2] + \mathbb{E}_{b \sim P_{data}[B]} [(D_A(G_B(b)))^2]
    $$

* **2つの生成器** ($G_A$, $G_B$)

    * **敵対的損失** (adversarial loss)
    $$
    L_{adv}^{G_B} = \mathbb{E}_{a \sim P_{data}[A]} [(D_B(G_A(a)) - 1)^2]
    $$

    * **復元損失** (cycle-consistency loss)
    $$
    L_{cyc}^{G_B, G_A} = \mathbb{E}_{a \sim P_{data}[A]} [||G_A(G_B(a)) - a||]
    $$

    * **同一性損失** (identity loss)
    $$
    L_{id}^{G_A} = \mathbb{E}_{a \sim P_{data}[A]} [||G_A(a) - a||]
    $$

# Conditional GAN
([目次に戻る](#目次))

## 問題1
- 敵対的生成ネットワーク（GAN）の一種である条件付き敵対的生成ネットワーク（Conditional GAN，CGAN）について正しいものを選べ。

(a) Conditional GANは，画像生成時に与えられる潜在変数に制約条件を加えることで，従来のGANよりも生成したい画像の質を向上することができる
(b) Conditional GANは，画像生成時に条件パラメータを与え，生成したい画像のクラスを指定できる一方で，従来のGANでは生成する画像のクラスは指定できない
(c) Conditional GANも従来のGANと同様に，Discriminatorが処理するタスクはGeneratorにより生成された画像かそうではないかを識別する分類問題である
(d) Conditional GANは，画像生成時に条件パラメータを与えることで，従来のGANよりも生成したい画像の解像度を向上することができる

- 解答
(b) が正解

- 解説
- Conditional GANの主な特徴は、条件パラメータを与えることで生成したい画像のクラスを指定できること- 従来のGANでは潜在変数ZZZのみから画像を生成するため、どのようなクラスの画像が生成されるかを制御不可
CGANでは条件パラメータYYYを追加することで、特定のクラスの画像を意図的に生成することが可能になります。

## 問題2
- Conditional GANのネットワークとして正しいものを選べ。
ただし，ネットワークはDiscriminator学習時のものとする。
G: Generator
D: Discriminator
X: 真の画像
Y: 条件パラメータ
Z: 潜在変数

- 解答
(a) が正解 ( Discriminatorの前に条件Yを受け取ってる)

- 解説
従来のGANのネットワーク
Z → [G] → G(Z) → [D] → 真偽判定
X ────────────────→ [D] → 真偽判定
従来のGANでは：

## *Generator*: G(Z)G(Z)
G(Z) - 生成分布を近似

生成例（MNIST）: ランダムに生成

## CGANのネットワーク
Z, Y → [G] → G(Z|Y) → [D] ← Y → 真偽判定
X, Y ─────────────────→ [D] ←─── → 真偽判定
CGANでは：

## Generator: G(Z∣Y)G(Z|Y)
G(Z∣Y) -
条件付き生成分布を近似
Discriminatorは画像とその条件パラメータの両方を入力として受け取る
生成例（MNIST）: 条件パラメータで数字（0-9）を指定可能

## 重要なポイント
Generatorは潜在変数ZZ
Zと条件パラメータYY
Yの両方を入力として受け取る

Discriminatorは画像（真の画像XXXまたは生成画像G(Z,Y)G(Z,Y)
G(Z,Y)）と条件パラメータYYYを入力として受け取る

これにより、Discriminatorは「その画像が指定された条件に対して真の画像か生成画像か」を判定する

### 具体例（MNIST）
条件パラメータとして数字のクラス（0-9）を指定することで：

条件パラメータ = 0 → 「0」の画像を生成
条件パラメータ = 1 → 「1」の画像を生成
...
条件パラメータ = 9 → 「9」の画像を生成

> Reference
> GANの提案論文
> Goodfellow, Ian J. et al. "Generative Adversarial Nets." NIPS (2014).
> Conditional GANの提案論文
> Mirza, Mehdi and Simon Osindero. "Conditional Generative Adversarial Nets." ArXiv abs/1411.1784 (2014): n. pag.

# Pix2Pix
([目次に戻る](#目次))
## Pix2pix の概要

（GANの学習後にご視聴ください）

### 簡単なおさらい：GAN（1/2）

* **生成器（Generator）と識別器（Discriminator）を競わせて学習する生成&識別モデル**
    * Generator：乱数からデータを生成
    * Discriminator：入力データが真データであるかどうか識別
* **GeneratorとDiscriminatorのミニマックスゲーム**
    * Generatorは自分の勝利する確率を最大化する
    * DiscriminatorはGeneratorが勝利する確率を最小化する
    * 上記を交互に繰り返す

### 簡単なおさらい：GAN（2/2）

[Image of GANのネットワーク]

* **GANのネットワーク**
    * $Z$: 乱数
    * $G(z)$: 生成されたデータ
    * $x$: 真のデータ
    * $True\ or\ False$: 識別器の出力
    * $\theta_g$: Generatorのパラメータ
    * $\theta_d$: Discriminatorのパラメータ

* **Conditional GAN（条件付きGAN）**
    * GANの生成したいデータに**条件**をつける
    * 条件はラベルで指定 $\rightarrow$ 「犬」という条件で、犬の画像を生成する
    * 基本的なネットワークはGANと同様
* **各プレイヤーの役割（条件ラベル$x$の場合）**
    * Generator：$x$の画像を生成
    * Discriminator：以下のように識別
        * （Gが生成した犬の画像 $G(x, z)$，$x$ラベル）$\rightarrow$ False
        * （Gが生成した犬の画像 $G(x, z)$，$x$以外のラベル）$\rightarrow$ False
        * （真のラベル$x$の画像 $y$，$x$ラベル）$\rightarrow$ True
        * （真のラベル$x$の画像 $y$, $x$以外のラベル）$\rightarrow$ False

### Pix2pix：概要

* **役割**
    * CGANと同様の考え方
    * 条件としてラベルではなく**画像**を用いる
    * 条件画像が入力され、何らかの**変換を施した**画像を出力する
    * 画像の**変換方法**を学習
* **各プレイヤーの役割（条件画像$x$）**
    * Generator：条件画像$x$をもとにある画像 $G(x, z)$を生成
    * Discriminator：
        * (条件画像$x \rightarrow$ Generatorが生成した画像 $G(z|x)$)の変換と
        * (条件画像$x \rightarrow$ 真の変換が施された画像 $y$)の変換が正しい変換かどうか識別する

### Pix2pix：学習データ

条件画像 $x$ と真の何らかの変換が施された画像 $y$ のペアが学習データ

* **例:**
    * 着色
    * RGB化
    * 建物のエッジ抽出

### Pix2pix：ネットワーク

* **Pix2pixのネットワーク**
    * $Z$: 乱数
    * $G(z|x)$: 生成された画像
    * $y$: 真の画像
    * $True\ or\ False$: 識別器の出力
    * $x$: 条件画像
    * $\theta_g$: Generatorのパラメータ
    * $\theta_d$: Discriminatorのパラメータ

### Pix2pix：工夫

**U-Net**

* Generatorに使用
* 物体の位置を抽出
    * 入力 $\rightarrow$ 出力
    * Encoder (ダウンサンプリング)
    * Decoder (アップサンプリング)
    * 画像の特徴 $\rightarrow$ セマンティックセグメンテーション
    * スキップ接続: 物体の位置情報伝達
    * セマンティックセグメンテーション
    * 物体の位置が抽出される
    * ピクセル単位の分類が可能
    * 入力画像と出力画像でサイズは一致

# A3C
([目次に戻る](#目次))
Asynchronous Advantage Actor-Critic（A3C）

## A3Cとは
強化学習の学習法の一つ；DeepMindのVolodymyr Mnih（ムニ）のチームが提案

複数のエージェントが同一の環境で非同期に学習する
"A3C"の名前の由来：3つの"A"

- Asynchronous  
  複数のエージェントによる非同期な並列学習
- Advantage  
  複数ステップ先を考慮して更新する手法
- Actor  
  方策によって行動を選択
- Critic
  状態価値関数に応じて方策を修正

- Actor-Criticとは
  行動を決めるActor（行動器）を直接改善しながら、方策を評価するCritic（評価器）を同時に学習させるアプローチ
> 引用： Sutton, Berto, "Reinforcement Learning – an introduction." 1998

### A3Cによる非同期（Asynchronous）学習の詳細

複数のエージェントが並列に自律的に、rollout（ゲームプレイ）を実行し、勾配計算を行う
その勾配情報をもって、好き勝手なタイミングで共有ネットワークを更新する
各エージェントは定期的に自分のネットワーク（local network）の重みをglobal networkの重みと同期する
共有ネットワーク = パラメータサーバ

> 引用： https://pylessons.com/A3C-reinforcement-learning/
> 参考論文： https://arxiv.org/abs/1602.01783

### 並列分散エージェントで学習を行うA3Cのメリット
① 学習が高速化
② 学習を安定化

#### 安定化について
経験の自己相関が引き起こす学習の不安定化は、強化学習の長年の課題
DQNはExperience Replay（経験再生）機構を用いてこの課題を解消

バッファに蓄積した経験をランダムに取り出すことで経験の自己相関を低減
しかし、経験再生は基本的にはオフポリシー手法でしか使えない


A3Cはオンポリシー手法であり、サンプルを集めるエージェントを並列化することで自己相関を低減することに成功した

### A3Cの難しいところとA2Cについて
- A3Cの課題
  Python言語の特性上、非同期並列処理を行うのが面倒
  パフォーマンスを最大化するためには、大規模なリソースを持つ環境が必要

- A2Cについて
  A3Cの後にA2Cという手法が発表された
  A2Cは同期処理を行い、Pythonでも実装しやすい
  各エージェントが中央指令部から行動の指示を受けて、一斉に1ステップ進行し、中央指令部は各エージェントから遷移先状態の報告を受けて次の行動を指示する
  性能がA3Cに劣らないことがわかったので、その後よく使われるようになった

### A3Cのロス関数
ロス関数の構成

- アドバンテージ方策勾配
- 価値関数ロス
- 方策エントロピー

$$
Total loss=−アドバンテージ方策勾配+α⋅価値関数ロス−β⋅方策エントロピー\text{Total loss}
$$
$$
= -\text{アドバンテージ方策勾配} + \alpha \cdot \text{価値関数ロス} - \beta \cdot \text{方策エントロピー}Total loss=−アドバンテージ方策勾配+α⋅価値関数ロス−β⋅方策エントロピー
$$ 
βはハイパーパラメータ
βは探索の度合いを調整するハイパーパラメータ

### 分岐型Actor-Criticネットワーク
- 一般的なActor-Critic
  方策ネットワークと価値ネットワークを別々に定義し、別々のロス関数（方策勾配ロス/価値ロス）でネットワークを更新

- A3C（パラメータ共有型のActor-Critic）
  1つの分岐型のネットワークが、方策と価値の両方を出力し、たった1つの「トータルロス関数」でネットワークを更新
- A3Cアルゴリズムのアドバンテージ方策勾配項

### 方策勾配法の基本
- 方策勾配法では、θ
- θをパラメータに持つ方策πθ
- πθ​に従ったときの期待収益ρθ
- ρθ​が最大になるように、θ

### θを勾配法で最適化する

方策勾配定理により、パラメータの更新に用いられる勾配∇θρθ\nabla_\theta \rho_\theta
∇θ​ρθ​は、以下の式で表される


- ベースラインの効果
b(s)b(s)
b(s)は価値の「ベースライン」：
これを引くことで、推定量の分散が小さくなり学習の安定化が期待できる

## A3Cの特徴
式1の中のQπθ(s,a)−b(s)Q^{\pi_\theta}(s, a) - b(s)
Qπθ​(s,a)−b(s)にアドバンテージ関数を設定する

### A3Cのアルゴリズムの特徴としては、勾配を推定する際に：

b(s)b(s)
b(s)の推定には価値関数Vπθ(s)V^{\pi_\theta}(s)
Vπθ​(s)を使用

Q(s,a)Q(s,a)
Q(s,a)の指定には、kk
kステップ先読みした収益（式2）を用いる
つまり、式3の期待値が式1の勾配と等価

### A3CのAtari 2600における性能検証

- 実験設定

Atari 2600において、人間のスコアに対して規格化した深層強化学習の結果
A3C：16 CPUコアのみ使用、GPU使用なし（1~4日間の訓練）
他のエージェント：Nvidia K40 GPUを使用（8~10日間の訓練）

- 結論：より短い訓練時間でGPUなしでも、A3Cのスコアが顕著に高い
手法平均中央値訓練時間A3C高い性能高い性能1~4日
1日間の訓練で、CPUのみでも、Dueling D-DQNの精度に到達

参考： https://arxiv.org/pdf/1602.01783

- A3Cアルゴリズムの方策エントロピー項
  - 方策のランダム性の高い（= エントロピーが大きい）方策にボーナスを与えることで、方策の収束が早すぎて局所解に停滞する事態を防ぐ効果がある
  - 方策エントロピー項の追加は、方策関数の正則化効果が期待できる

- 方策のランダムさの指標
  例： ある状態ss
  sの入力について出力される行動の採用確率が


(a1,a2,a3,a4)=[0.25,0.25,0.25,0.25](a_1, a_2, a_3, a_4) = [0.25, 0.25, 0.25, 0.25]
(a1​,a2​,a3​,a4​)=[0.25,0.25,0.25,0.25]の場合

(a1,a2,a3,a4)=[0.85,0.05,0.05,0.05](a_1, a_2, a_3, a_4) = [0.85, 0.05, 0.05, 0.05]
(a1​,a2​,a3​,a4​)=[0.85,0.05,0.05,0.05]の場合

**前者のほうが方策のエントロピーが大きい状態である**

A3Cの実装の参考（TensorFlow Blogより）
https://blog.tensorflow.org/2018/07/deep-reinforcement-learning-keras再試行Claudeは間違えることがあります。回答内容を必ずご確認ください。リサーチbeta Sonnet 4

## 距離学習 (Metric learning)
([目次に戻る](#目次))

ディープラーニング技術を用いた距離学習は、人物同定 (Person Re-Identification) をはじめ、顔認識、画像分類、画像検索、異常検知など幅広いタスクに利用される技術

この解説プリントでは、距離学習の基本的なアイディア、そして深層距離学習の代表的な手法である Siamese Network [Hadsell+, “Dimensionality Reduction by Learning an Invariant Mapping”, CVPR (2006)]、および、Triplet Network [Hoffer+,“Deep metric learning using Triplet network”, arXiv:1412.6622 (2014)] について説明していきます。

### 距離学習とは

- 距離学習ではデータ間の **metric**、すなわち「データ間の距離」を学習
-データ間の距離を適切に測ることができれば、距離が近いデータ同士をまとめてクラスタリング$^1$ができた り、他のデータ要素から距離が遠いデータを異常と判定することで異常検知したりと様々な応用が可能となります。距離学習自体は古くからある手法ですが、近年のディープラーニングの発展とともに、ディープラーニング技術を利用した距離学習の手法が数多く提案されています。このような手法は、特に**深層距離学習** (deep metric learning) と呼ばれています。

一般に、画像や音声などの多次元データはニューラルネットワークを用いることにより、**次元削減** (データ圧縮) することが出来ます。例えば、畳み込みニューラルネットワーク (Convolutional Neural Network: CNN) を用いた画像分類の場合、$28 \times 28$ サイズの画像をニューラルネットワーク (CNN) に入力することで $10$ 次元のベクトルに圧縮することができます。つまり、元々 $28 \times 28 = 784$ 個の数値で表現されていた画像の情報を $10$ 個の変数に圧縮したと解釈できます。

CNN から出力されるベクトルは、入力データの重要な情報を抽出したベクトルと考えられるため、一般に**特徴ベクトル** (feature vector) と呼ばれます。

入力      出力
入力データ -> CNN -> 特徴ベクトル
次元    10次元


ただし、未学習のネットワークにデータを入力しても出力されるのは無意味なベクトルであり、特徴ベクトルに意味を持たせるには何らかの手法で CNN を学習する必要があります。深層距離学習はその1つの方法であり、2つの特徴ベクトル間の距離がデータの類似度を反映するようにネットワークを学習します。具体的には、

* 同じクラスに属する (= 類似) サンプルから得られる特徴ベクトルの距離は小さくする
* 異なるクラスに属する (= 非類似) サンプルから得られる特徴ベクトルの距離は大きくする

といった具合です。特徴ベクトルの属する空間は**埋め込み空間** (embedding space)$^2$と呼ばれますが、この埋め込み空間内で類似サンプルの特徴ベクトルは近くに、非類似サンプルの特徴ベクトルは遠くに配置されるように学習を行うことになります。つまり、CNN を深層距離学習の手法を用いて学習していくと、類似サンプル (から得られる特徴ベクトル) は埋め込み空間内で密集していき、逆に非類似サンプルは離れる

入力       学習
入力データ -> CNN -> 埋め込み空間 (学習前)
|
V
損失関数
|
V
埋め込み空間 (学習後)

- この図では、特徴ベクトルが $2$ 次元のベクトル $(x_1, x_2)$ であるとして、特徴ベクトルを埋め込み空間上の点として可視化
- 学習後のような埋め込み空間を構成することができれば、画像分類やクラスタリングが容易になるのは一目瞭然
- 画像分類などのタスクに対する精度の向上は、ニューラルネットワークモデル自体の構造を工夫 (複雑化) することでも達成可
- ネットワークの構造自体は工夫しなくても、類似度を反映した埋め込み空間を構成できるように学習を行うだけで精度向上が可能になる、というのが深層距離学習のアプローチとなります。

では、深層距離学習ではどのようにしてサンプル間の類似度を反映した埋め込み空間を構成するのでしょうか？ 以下では、深層距離学習の代表的な手法である Siamese network と Triplet network について説明してきます。
## Deep Metric Learning

## Siamese Network
- 入力：2つのデータ
- 同一クラスなら距離を縮め、異なるクラスなら距離を離す
- **Contrastive Loss** を使用

## Triplet Network
- 入力：Anchor + Positive + Negative
- **Triplet Loss**：
  - $D(a, p) + \text{margin} < D(a, n)$ を満たすように学習
  - より安定した**埋め込み空間**の学習可能

# MAML(メタ学習)
([目次に戻る](#目次))

## MAMLが解決したい課題

* 訓練に必要なデータ量が多い
    * 人手のアノテーションコスト
    * データ自体を準備できるかどうか
* 少ないデータの問題点
    * 過学習が発生しやすい
    $\rightarrow$ 少ないデータで学習させたい

深層学習モデルの開発に必要なデータ量を削減したい

### 深層学習モデル開発に必要なデータ量

| データセット名        | 画像の枚数 |
| :-------------------- | :--------- |
| MNIST                 | 7万枚      |
| ImageNet (ILSVRC2012) | 約120万枚  |
| Open Image Dataset V6 | 約900万枚  |
| MegaFace              | 約570万枚  |

## MAMLのコンセプト

タスクに共通する重みを学習し、新しいモデルの学習に活用

* **転移学習 (ファインチューニング)**
    * **事前学習**
        * オートエンコーダー (教師なし学習)
        * ImageNetを使った事前学習モデル (教師あり学習)
    * **モデルAの重みを初期値として学習**
        * タスクAのためのモデル
        * タスクAの重みを使用 (この部分のみ学習)

* **MAML (メタ学習)**
    * モデル全体の重みを学習

共通重みからファインチューニング
+-----------------+
| タスクAのためのモデル |
+-----------------+
共通重みからファインチューニング
+-----------------+
| タスクBのためのモデル |
+-----------------+
共通重みからファインチューニング
+-----------------+
| タスクCのためのモデル |
+-----------------+
共通重みからファインチューニング
+-----------------+
| タスクDのためのモデル |
+-----------------+

    * タスク共通の重みを学習

## MAMLの学習手順

タスクごとの学習を行った結果を共通重みに反映させ学習

1.  **共通重み $\theta$ をランダムに初期化**

    **Outer Loop:** $\theta$ が収束するまで繰り返し
    2.  タスク集合 $T$ からタスク $T_i$ を取り出し
        **Inner Loop:** タスクの個数分繰り返し
        3.  タスク $T_i$ に重み $\theta$ を最適化し $\theta'_i$ を得る
    4.  ($\theta'_1, \theta'_2, ...$) を集める
    5.  集めた重みで共通重み $\theta$ を更新 (SGD)

## MAMLの効果

Few-Shot learningで既存手法を上回る精度を実現

Omniglotデータセット(50種類の文字)を使ったクラス分類

|             | 5クラス  | 20クラス |
| :---------- | :------- | :------- |
| サンプル1枚 | (データ) | (データ) |
| サンプル5枚 | (データ) | (データ) |

この他にも、回帰問題、強化学習などでも効果が確認された

C. Finn+, arXiv:1703.03400

## MAMLの課題と対処

* MAMLは計算量が多い
    * タスクごとの勾配計算と共通パラメータの勾配計算の2回が必要
    * 実用的にはInner loopのステップ数を大きくできない
* 計算コストを削減する改良案 (近似方法)
    * First-order MAML: 2次以上の勾配を無視し計算コストを大幅低減
    * Reptile: Inner loopの逆伝搬を行わず、学習前後のパラメータの差を利用

タスクごとの学習と共通パラメータの学習で計算量が多い

# グラフ畳み込み(GCN)
([目次に戻る](#目次))

* **何のために？**
    $\rightarrow$ 用途は様々！（だから概要をつかみにくい...）
    今回は特徴をはっきりさせるため
    **もとの関数にフィルターをかける！**
    $y_t = g_t * x_t$

* **特徴をはっきりさせる？**
    * これを2次元画像に対して使う $\rightarrow$ CNN
    * これをグラフ（ネットワーク）に対して使う $\rightarrow$ GCN

畳み込みで関数の特徴を際立たせている！

### 教科書の数式とは違うように見えるのは？

私たちは...
な式を読めるようになって、活用したい！

しかし、畳み込みについて、書籍を開いても、web上の情報を検索しても
という式しか出てこない...なぜ？

「因果的な時不変システム」を取り扱う際に便利な形式だから！

「過去の出来事が、現在にどれだけ影響しているのか」といったことをモデル化する際に便利！

一般的（より広い意味）に考えると...
$\rightarrow$ 元の関数に「重み」をかけて、強調するところと、捨象するところの区別をするということがキモ！

$t-\tau$ という部分が気になるところだが、より重要なことは『元の関数に「重み」をかけて、強調したり、捨象したりしている』という構造！

ここが「重み」（のようなもの）

**畳み込みの一般的な形**

連続的な場合:
$$
y_t = f * g_t = \int f(\tau) g(t, \tau) d\tau
$$

離散的な場合:
$$
y_m = f * g_m = \sum_n f(n) g(m, n)
$$
重み

**具体例** （$n$は0から4まで変化させる）

$f_n = (0, 2, 4, 6, 8)$

$g_{m, n} = \begin{cases} 1, & m-n = 1 \\ 3, & m-n = 0 \\ 5, & m-n = -1 \\ 0, & \text{上記以外} \end{cases}$

|  $n$  |   0   |   1   |   2   |   3   |   4   |
| :---: | :---: | :---: | :---: | :---: | :---: |
|  $f$  |   0   |   2   |   4   |   6   |   8   |

| $m-n$ |  -1   |   0   |   1   |
| :---: | :---: | :---: | :---: |
|  $g$  |   5   |   3   |   1   |

$m=0: 0 \times 1 + 1 \times 3 + 4 \times 5 = 23$
$m=1: 2 \times 1 + 4 \times 3 + 6 \times 5 = 44$
$m=2: 4 \times 1 + 6 \times 3 + 8 \times 5 = 62$

$y_m = (-, 23, 44, 62, -)$

## Spatialな場合

ノイズが目立たなくなり、形がはっきりする！
空間的の意

### Spatialな場合（どんな手順で？）

$93.8 \times (1/3) + 102.4 \times (1/3) + 96.7 \times (1/3) = 97.6$
このような空間的な操作を繰り返していく

- 重みの係数は用途に合わせて変更可能
- スペクトルに分解することで、特徴的な成分が明らかに！

# Grad-CAM, LIME, SHAP
([目次に戻る](#目次))
## ディープラーニングモデルの解釈性

* ディープラーニング活用の難しいことの1つは「ブラックボックス性」
* 判断の根拠を説明できないという問題があります。
* 実社会に実装する際に「なぜ予測が当たっているのか」を説明できないことが問題となります。
* モデルの解釈性に注目し、「ブラックボックス性」の解消を目指した研究が進められています。

### CAM
（Class Activation Mapping）

* 2016年のCVPRにて発表されたCNNの判断根拠可視化の手法です。
* GAP(Global Average Pooling)を使用するモデルに適用できる手法です。
* GAPは学習の過学習を防ぐ、正則化の役割として使われてきましたが、GAPがCNNが潜在的に注目している部分を可視化できるようにする性質を持っていることが分かりました。

### Grad-CAM
（Gradient-weighted Class Activation Mapping）

* CNNモデルに判断根拠を持たせ、モデルの予測根拠を可視化する手法です。
* 名称の由来は”Gradient” = 「勾配情報」です。
* 最後の畳み込み層の予測クラスの出力値に対する勾配を使用します。
* 勾配が大きいピクセルに重みを増やすことで、予測クラスの出力に大きく影響する重要な場所を特定します。
* CAMはモデルのアーキテクチャにGAPがないと可視化できなかったのに対し、Grad-CAMはGAPがなくても可視化できます。また、出力層が画像分類でなくてもよく、様々なタスクで使えます。

### LIME
(Local Interpretable Model-agnostic Explanations)

* 特定の入力データに対する予測について、その判断根拠を解釈・可視化するツールです。
    * 表形式データ：「どの変数が予測に効いたのか」を可視化します。
    * 画像データ：「画像のどの部分が予測に効いたのか」を可視化します。
* 単純で解釈しやすいモデルを用いて、複雑なモデルを近似することで解釈を行います。
    * 複雑なモデル：人間による解釈の困難なアルゴリズムで作った予測モデル（例：決定木のアンサンブル学習器、ニューラルネットワークなど）。
* LIMEへの入力は1つの個別の予測結果です。
    * 画像データ：1枚の画像の分類結果。
    * 表形式データ：1行分の推論結果。
* 対象サンプルの周辺のデータ空間からサンプリングして集めたデータセットを教師データとして、データ空間の対象範囲内でのみ有効な近似用モデルを作成します。近似用モデルから予測に寄与した特徴量を選び、解釈を行うことで、本来の難解なモデルの方を解釈したことと見なします。
* スーパーピクセル: 色や質感が似ている領域をグループ化すること。

### 参考文献から補足
• 参考：https://github.com/marcotcr/lime
• Pythonを用いたLIMEの実装ライブラリは、データ形式（表形式、テキスト、画像）によって
アルゴリズムが異なる ※コンセプトは同じ
(例) 表データから作成されたモデルは “LimeTabularExplainer”を使用して解釈する
LIMEを用いて、Google のインセプションモジュールによる画像認識を解釈している様子
（画像は論文より引用 ：https://arxiv.org/abs/1602.04938）


### SHAP
（SHapley Additive exPlanations）

* 機械学習モデルの予測結果を解釈するための手法です。
* 各特徴量がモデルの出力にどの程度寄与しているかを定量的に評価します。
* 協力ゲーム理論の概念であるshapley value(シャープレイ値)を機械学習に応用したものです。

# Docker
([目次に戻る](#目次))

## コンテナ型仮想化

仮想環境はハードウェア上で独立した複数の環境を実行する技術で、ホスト型、ハイパーバイザー型、コンテナ型の3種類が存在します。

| カテゴリ             | ホスト型                                                     | ハイパーバイザー型                                                                                                                             | コンテナ型                                                                                     |
| :------------------- | :----------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------- |
| **概要**             | 物理的なホスト OS 上で動作                                   | ハードウェア上で直接実行                                                                                                                       | 単一のOSカーネルで複数のコンテナ実行                                                           |
| **利点**             | ⚫ インストールの簡単さ<br>⚫ ホスト OS のデバイスドライバ利用 | ⚫ 異なる OS の同時実行<br>⚫ 高いパフォーマンスと安定性<br>⚫ ハードウェアと直接通信するためのオーバーヘッドが少ない<br>⚫ 完全なゲスト OS の隔離 | ⚫ ハードウェアの効率的な利用<br>⚫ 起動の速さ<br>⚫ リソースのオーバーヘッドの少なさ<br>⚫ 移植性 |
| **欠点**             | ⚫ パフォーマンスのオーバーヘッド<br>⚫ セキュリティ問題の影響 | ⚫ セットアップと管理が複雑<br>⚫ ハードウェアのリソースに直接依存<br>⚫ Type 2 の場合 ホスト OS のオーバーヘッドが存在                           | ⚫ 完全な隔離の不足<br>⚫ 特定の OS カーネルへの依存                                             |
| **ソフトウェアの例** | ⚫ VMware Workstation<br>⚫ Oracle VirtualBox                  | ⚫ VMware vSphere<br>⚫ Microsoft Hyper-V                                                                                                        | ⚫ Docker<br>⚫ LXC                                                                              |
| **備考**             | なし                                                         | **Type 1とType 2の違い:**<br>Type 1:<br>⚫ 高いパフォーマンスと安定性<br>Type 2<br>⚫ ホスト OS 上で動作<br>⚫ 設定の簡単さ                       | なし                                                                                           |

## Dockerとは

Dockerはコンテナ型仮想化ソリューションで、事実上の標準として利用されています。

* **Docker の定義と背景**
    * アプリケーションと依存関係をコンテナとしてパッケージ化
    * どの環境でも一貫した動作を保証
    * アプリケーションの依存関係をインフラから分離
* **Docker のアーキテクチャ**
    * **Docker エンジン**: コア技術で、Linuxの`cgroups`、`namespaces`を利用
    * **Docker イメージ**: アプリケーションと依存関係のスナップショット
    * **Docker コンテナ**: イメージから生成されるアプリケーションの実行環境
* **Docker の歴史と普及の背景**
    * 2013年: Docker Engineとして公開
    * Microsoftとの協力でWindows Serverに導入
    * 現在: データセンター、クラウドプロバイダーでの採用

Dockerイメージに封入された実行環境で作成したアプリが実行される

## Dockerの用途

Dockerは、アプリケーションの開発からデプロイメントまでの一貫性と効率性を向上させるために広く利用されています。

* **アプリケーションの開発とテスト**
    * 現代の開発の複雑さへの対応
    * Dockerによるワークフローの簡素化と加速
* **本番環境でのデプロイ**
    * コンテナの概念を導入することによる環境の隔離
    * 2013年のDocker導入以降の業界標準化
* **マイクロサービスアーキテクチャの実装**
    * 各マイクロサービスの独立したデプロイ
    * システムの柔軟性と耐障害性の向上
* **環境の統一と再現性の確保**
    * アプリケーションと依存関係のコンテナ化
    * 開発、テスト、本番環境の動作の一貫性

[Image of Dockerの用途における開発から本番環境までのワークフロー]
※ 実際には、ステージング環境・本番環境のDockerイメージはクラウド上でビルドすることがあります

## 基本的な Docker コマンド

Dockerの基本コマンドは、コンテナの作成、運用、管理を効率的に行うための不可欠です。

* **主要なDockerコマンド**
    * `docker build`: Dockerfileからイメージの作成（ソースコードやアプリケーションの依存関係を含むイメージ作成）
    * `docker run`: コンテナの起動（指定されたイメージから新しいコンテナを作成・実行、オプションや引数でカスタマイズ可能）
    * `docker ps`: 実行中のコンテナの一覧表示（`-a`オプションで停止中のコンテナも表示）
    * `docker images`: ローカルのイメージ一覧表示（ID、リポジトリ名、タグ、作成日時などの情報表示）
    * `docker pull`: Docker Hubや他のレジストリからイメージ取得（ローカルに存在しないイメージのダウンロード）
    * `docker push`: ローカルのイメージをレジストリにアップロード（自作イメージの公開・共有）
    * `docker start`: 停止しているコンテナの起動（既存の停止コンテナを再起動可能）
    * `docker stop`: 実行中のコンテナの停止（`docker start`で再起動可能）
    * `docker rm`: コンテナの削除（停止しているコンテナをシステムから完全削除、一度削除したコンテナは復元不可）

## Dockerfile の作成方法

Dockerfileは、コンテナイメージの作成手順を定義するための指示書であり、その命令とベストプラクティスを理解します。

* **主要な命令**
    * `FROM`: ベースイメージの指定（例: `FROM ubuntu:20.04`）
    * `RUN`: シェルコマンドの実行（例: `RUN apt-get update && apt-get install -y curl`）
    * `CMD`: デフォルトのコマンド指定（例: `CMD ["echo", "Hello, World!"]`）
    * `ENTRYPOINT`: デフォルトのアプリケーション指定（例: `ENTRYPOINT ["echo"]`）
    * `COPY`: ホストからコンテナへのファイルコピー（例: `COPY ./app /app`）
    * `ADD`: ファイルの追加 (tar解凍やURLからのダウンロードも可)（例: `ADD https://example.com/app.tar.gz /app`）
    * `WORKDIR`: 作業ディレクトリの設定（例: `WORKDIR /app`）
    * `ENV`: 環境変数の設定（例: `ENV MY_ENV_VARIABLE=value`）

* **Dockerfile とは**
    * Dockerコンテナイメージを作成するためのスクリプトファイル
    * ベースイメージの選択、アプリケーションのコードの追加、依存関係のインストール、環境変数の設定などが記述されます。
* **基本構文**
    * 命令は大文字で始まり、引数が続く
    * 例: `INSTRUCTION arguments`
* **ベストプラクティス**
    * `.dockerignore`ファイルを使用して不要なファイルを除外
    * キャッシュを効率的に使用するための配置
    * 不要なパッケージや一時ファイルの削除
    * 複数の`RUN`命令を連鎖させる
    * 公式イメージの使用を推奨

## GPU 環境におけるDockerの使用

Dockerを使用することで、GPUの計算リソースを効率的に活用したアプリケーションの開発とデプロイが簡単に行えます。

* **背景**
    * GPUはディープラーニングや高性能計算に広く利用
    * DockerがGPUサポートを強化
* **Docker での GPU サポート**
    * NVIDIA Container Toolkitを提供
    * コンテナランタイムライブラリやユーティリティを含む
    * Docker、LXC、Podmanなどのサポート
* **GPU アプリケーションのコンテナ化**
    * GPUを活用したアプリケーションをDockerで実行可能
    * 効率的なGPUリソースの利用
* **NVIDIA Container Toolkit の特徴**
    * GPU アクセラレーション: コンテナ内でのNVIDIA GPU直接利用
    * エコシステムのサポート: DockerやPodman等のサポート
    * 自動設定: NVIDIA GPU利用のための自動設定
* **深層学習モデルの開発とDocker**
    * 一貫した環境設定の簡易化
    * モデルの開発やテストの一貫性

※ 容量の大きなデータセットはイメージに含めないことも多い

## コンテナオーケストレーション

コンテナオーケストレーションは、大規模なアプリケーションなどで複数のコンテナ管理を実現するソリューションです。

* **コンテナオーケストレーション**
    * **定義**: コンテナのデプロイ、スケーリング、運用の自動化プロセス
    * **必要性**: 大規模アプリケーション・マイクロサービスの効果的な管理、コンテナダウン時の自動復旧
    * **代表的ツール**: Kubernetes（オーケストレーションニーズ対応）
* **主な機能:**
    * サービスディスカバリーとロードバランシング
    * ストレージオーケストレーション
    * 自動ロールアウト・ロールバック
    * 自動ビンパッキング
    * セルフヒーリング
    * シークレット・設定管理
* **代表的オーケストレーションツール:**
    * **Kubernetes**: オープンソース、大規模運用向け
    * `docker-compose`: Docker公式、開発環境・小規模デプロイ向け

### 参考文献補足
https://qiita.com/caunu-s/items/4fa0e0465ea83fcc06e4
類似のプログラムにpodmanがある
- PodmanとDockerの主な違いは以下の通りです：
デーモンの有無: Podmanはデーモンレスで動作し、Dockerは常駐デーモン（docker daemon）が必要です。 
- セキュリティ: Podmanは「rootless」モードをサポートしており、ユーザー権限でコンテナを実行できますが、Dockerは通常root権限で動作します。 
- コマンドラインインターフェース: PodmanはDockerと互換性のあるCLIを提供しており、ほとんどのDockerコマンドをそのまま使用できます。 

コンテナの管理: Podmanは複数のコンテナを一つのユニットとして管理する「pod」概念を持っていますが、Dockerは個別のコンテナを管理します。 

- 使用ケース: Podmanは特にセキュリティが重視される環境での使用が推奨され、Dockerは広く普及しているため、コミュニティやサポートが充しています