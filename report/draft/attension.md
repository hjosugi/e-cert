Attentionメカニズムは、深層学習モデル、特にTransformerアーキテクチャの中核をなす要素であり、入力シーケンスの異なる部分間の依存関係を捉えることを可能にします。ここでは、その数式を行列の知識を補足しながら詳細に解説します。

### Attention関数の定義

Attentionメカニズムの基本的な形式は、Query ($Q$), Key ($K$), Value ($V$) の3つの行列を用いて次のように表されます。

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
$$

この式は、大きく分けて2つの主要なステップで構成されます。

1.  **アテンションスコア（Attention Score）の計算と正規化**: $Q$ と $K$ を用いて、QueryがどのKeyにどれだけ注目すべきかを示すスコアを計算し、それを確率分布に変換します。
2.  **Valueの加重平均**: 正規化されたアテンションスコアを重みとして、$V$ の要素を線形結合（加重平均）し、最終的な出力を生成します。

### 各要素と数式の詳細な解説

#### 1. 行列 $Q, K, V$ の構造

まず、$Q, K, V$ はそれぞれベクトルを複数集めた行列です。

* **$Q$ (Query行列)**:
    * 形状: $n \times d_k$
    * $Q = \begin{pmatrix} \vec{q_1}^T \\ \vec{q_2}^T \\ \vdots \\ \vec{q_n}^T \end{pmatrix}$
    * ここで、$\vec{q_i}$ は $d_k$ 次元（つまり、$d_k$個の数値を持つ）の行ベクトルです。
    * $n$ はクエリの数、または通常、入力シーケンスの長さ（バッチ処理の場合はバッチ内のシーケンス長）です。各行が1つのクエリに対応します。
    * モデルが「何に注目したいか」を表す情報を含んでいます。例えば、翻訳タスクにおいて、現在の単語（Query）が入力文中のどの単語（Key）と関連が深いかを探すために使われます。

* **$K$ (Key行列)**:
    * 形状: $m \times d_k$
    * $K = \begin{pmatrix} \vec{k_1}^T \\ \vec{k_2}^T \\ \vdots \\ \vec{k_m}^T \end{pmatrix}$
    * ここで、$\vec{k_i}$ は $d_k$ 次元の行ベクトルです。
    * $m$ はキーの数、または通常、入力シーケンスの長さです。各行が1つのキーに対応します。
    * Value行列 $V$ の各要素 $\vec{v_i}$ に対応しており、「対応するValue $\vec{v_i}$ が何を表しているか」を示す「鍵」のような役割を果たします。

* **$V$ (Value行列)**:
    * 形状: $m \times d_v$
    * $V = \begin{pmatrix} \vec{v_1}^T \\ \vec{v_2}^T \\ \vdots \\ \vec{v_m}^T \end{pmatrix}$
    * ここで、$\vec{v_i}$ は $d_v$ 次元の行ベクトルです。
    * $m$ はキーの数と同じです。各行が1つのValueに対応します。
    * Queryが注目すべき「情報そのもの」を含んでいます。Keyによって特定された関連性の高いValueが抽出され、最終的な出力に貢献します。

#### 2. $Q K^T$ の計算（ドット積による類似度計算）

$$
Q K^T
$$

* **$K^T$ (Key行列の転置)**:
    * 形状: $d_k \times m$
    * $K^T = \begin{pmatrix} \vec{k_1} & \vec{k_2} & \dots & \vec{k_m} \end{pmatrix}$
    * 行と列が入れ替わります。$\vec{k_i}$ が列ベクトルになります。

* **$Q K^T$**:
    * 形状: $(n \times d_k) \times (d_k \times m) = n \times m$
    * この行列積の各要素 $(QK^T)_{ij}$ は、Queryベクトル $\vec{q_i}$ と Keyベクトル $\vec{k_j}$ の内積（ドット積）です。
    * $(QK^T)_{ij} = \vec{q_i} \cdot \vec{k_j} = \sum_{p=1}^{d_k} q_{ip} k_{jp}$
    * 内積は、2つのベクトルがどれだけ似ているか（方向が近いか、相関があるか）を示す指標としてよく使われます。値が大きいほど、Query $\vec{q_i}$ と Key $\vec{k_j}$ の関連性が高いことを意味します。この結果が「アテンションスコア」の元になります。

#### 3. スケーリング（Scaling）

$$
\frac{Q K^T}{\sqrt{d_k}}
$$

* **$\sqrt{d_k}$ (スケール因子)**:
    * $d_k$ はKeyベクトルの次元数です。
    * 内積 $Q K^T$ の値は、$d_k$ が大きくなるとともに無制限に大きくなる傾向があります。値が大きくなりすぎると、softmax関数に大きな値が入力されたときに、勾配が非常に小さくなり（飽和し）、学習が不安定になる可能性があります。
    * $\sqrt{d_k}$ で割ることで、内積の大きさを正規化し、勾配の消失（vanishing gradients）や爆発（exploding gradients）を防ぎ、学習を安定化させる効果があります。

#### 4. softmax関数の適用（正規化）

$$
\text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)
$$

* **softmax関数**:
    * softmaxは、入力された各要素を0から1の間の値に変換し、かつ全ての要素の合計が1になるようにする関数です。
    * $\text{softmax}(x_j) = \frac{e^{x_j}}{\sum_p e^{x_p}}$
    * ここでは、$Q K^T / \sqrt{d_k}$ の各行（つまり、各Queryに対するKeyとの関連性スコアのセット）に対してsoftmax関数を適用します。
    * 結果として得られる行列は、形状が $n \times m$ であり、各要素は「Query $\vec{q_i}$ が Key $\vec{k_j}$ に与える注目度合い（アテンションウェイト）」を表す確率値となります。この行列を**アテンションマップ**と呼ぶこともあります。

#### 5. Valueの加重平均（Attention出力の生成）

$$
\text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
$$

* **行列積**:
    * 形状: $(n \times m) \times (m \times d_v) = n \times d_v$
    * softmaxの結果（アテンションウェイト行列）と $V$ 行列を掛け算します。
    * この行列積は、各Queryに対応するアテンションウェイトを使って、$V$ の行ベクトル（Value）を加重平均する操作に相当します。
    * 例えば、出力行列の $i$ 番目の行ベクトルは、Query $\vec{q_i}$ に対応するアテンションウェイトを使って、全てのValueベクトル $\vec{v_1}, ..., \vec{v_m}$ を重み付けして足し合わせたものになります。
    * これにより、Queryが最も注目すべきKeyに対応するValueの情報が強調され、それ以外のValueの情報は相対的に弱められた形で、最終的なAttentionの出力が生成されます。

### まとめ

Attentionメカニズムは、Query、Key、Valueという3つの概念を通じて、入力データ内で関連性の高い部分を動的に特定し、その情報を抽出する洗練された方法を提供します。行列演算を多用することで、この複雑なプロセスが効率的かつ並列に実行され、大規模な深層学習モデルの訓練を可能にしています。

このメカニズムにより、モデルは固定長の隠れ状態にすべての情報を圧縮するのではなく、必要に応じて入力シーケンス全体から関連情報を選択的に「参照」できるようになり、特に長距離の依存関係を持つデータ（長い文章など）の処理において、飛躍的な性能向上をもたらしました。



与えられた画像には、行列とベクトルの積に関する数式が手書きで書かれています。

$$
一般に、A = \begin{pmatrix} \vec{a_1} \\ \vec{a_2} \\ \vdots \\ \vec{a_m} \end{pmatrix} \text{と} \vec{x} = (x_1, \dots, x_m) \text{について、} \\
\vec{x} A = \sum_{i=1}^{m} x_i \vec{a_i} \text{になる。}
$$

### これは何を言っているのか？

これは「**行ベクトルと行列の積**」の計算方法の一つを示しています。特に、結果が行列の各行ベクトルを重み付けして足し合わせたものになる、という重要な性質を強調しています。

順を追って説明しましょう。

#### 1. 行列 $A$ と行ベクトル $\vec{x}$ の定義

* **行列 $A$**:
    * $A$ は、$m$ 個の行ベクトル $\vec{a_1}, \vec{a_2}, \dots, \vec{a_m}$ が縦に並んだ行列です。
    * それぞれの $\vec{a_i}$ は、同じ次元数を持つ行ベクトルです。例えば、$\vec{a_i} = (a_{i1}, a_{i2}, \dots, a_{ik})$ のように、$k$ 個の要素を持つとします。
    * つまり、$A$ は全体として $m \times k$ の形状の行列です。

    $$A = \begin{pmatrix}
    \vec{a_1} \\
    \vec{a_2} \\
    \vdots \\
    \vec{a_m}
    \end{pmatrix}
    $$
    は以下と同じ
    $$
    \begin{pmatrix}
    a_{11} & a_{12} & \dots & a_{1k} \\
    a_{21} & a_{22} & \dots & a_{2k} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mk}
    \end{pmatrix}
    $$

* **行ベクトル $\vec{x}$**:
    * $\vec{x}$ は、$m$ 個の要素 $x_1, x_2, \dots, x_m$ を持つ行ベクトルです。
    * つまり、$\vec{x}$ は $1 \times m$ の形状のベクトルです。

    $$
    \vec{x} = (x_1, x_2, \dots, x_m)
    $$

#### 2. 行列の積 $\vec{x}A$ の計算ルール

行ベクトルと行列を掛け算する際の基本的なルールは、「**（行ベクトル）の要素数 ＝ （行列）の行数**」であることです。この場合、どちらも $m$ なので、積を計算できます。

積の結果は、行ベクトルの各要素と行列の対応する行を「掛け合わせて足し合わせる」という操作になります。

$$
\vec{x} A = (x_1, x_2, \dots, x_m)
\begin{pmatrix}
\vec{a_1} \\
\vec{a_2} \\
\vdots \\
\vec{a_m}
\end{pmatrix}
$$

この積の計算は、次のように展開されます。

* $\vec{x}$ の最初の要素 $x_1$ を $A$ の最初の行ベクトル $\vec{a_1}$ に掛けます。
* $\vec{x}$ の次の要素 $x_2$ を $A$ の次の行ベクトル $\vec{a_2}$ に掛けます。
* ...これを最後の要素 $x_m$ と行ベクトル $\vec{a_m}$ まで繰り返します。
* 最後に、これらの結果をすべて足し合わせます。

#### 3. 計算結果 $\sum_{i=1}^{m} x_i \vec{a_i}$

この計算プロセスを数式で書いたのが $\sum_{i=1}^{m} x_i \vec{a_i}$ です。
これは以下のような意味です。

$$\sum_{i=1}^{m} x_i \vec{a_i} = x_1 \vec{a_1} + x_2 \vec{a_2} + \dots + x_m \vec{a_m}$$

各 $x_i \vec{a_i}$ は、スカラー（単なる数値）$x_i$ とベクトル $\vec{a_i}$ の積です。これは、ベクトル $\vec{a_i}$ の各要素を $x_i$ 倍することを意味します。

例えば、$\vec{a_i} = (a_{i1}, a_{i2}, \dots, a_{ik})$ ならば、$x_i \vec{a_i} = (x_i a_{i1}, x_i a_{i2}, \dots, x_i a_{ik})$ となります。

最終的に、これらのスカラー倍されたベクトルをすべて足し合わせることで、結果は元の行ベクトル $\vec{a_i}$ と同じ次元数 $k$ を持つ新しい行ベクトルになります。

### 具体例で見てみよう

簡単な例で考えてみましょう。

$A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ と $\vec{x} = (5, 6)$ とします。
この場合、$\vec{a_1} = (1, 2)$ と $\vec{a_2} = (3, 4)$ です。

$\vec{x} A$ を計算すると：

$$\vec{x} A = (5, 6) \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$$

行列の積のルールに従って計算すると：

* 左側の行列（行ベクトル $\vec{x}$）の行と、右側の行列（$A$）の列を掛け合わせます。
* 結果の新しい行列は、行ベクトル $\vec{x}$ の行数（1）と行列 $A$ の列数（2）を持つので、$1 \times 2$ の行ベクトルになります。

$$
\begin{aligned}
\vec{x} A &= ( (5 \times 1 + 6 \times 3), (5 \times 2 + 6 \times 4) ) \\
&= ( (5 + 18), (10 + 24) ) \\
&= (23, 34)
\end{aligned}
$$

次に、画像で示されている公式 $\sum_{i=1}^{m} x_i \vec{a_i}$ で計算してみましょう。
この場合、$m=2$ なので、$x_1 \vec{a_1} + x_2 \vec{a_2}$ となります。

* $x_1 = 5, \vec{a_1} = (1, 2)$
* $x_2 = 6, \vec{a_2} = (3, 4)$

$$
\begin{aligned}
\sum_{i=1}^{2} x_i \vec{a_i} &= x_1 \vec{a_1} + x_2 \vec{a_2} \\
&= 5 \times (1, 2) + 6 \times (3, 4) \\
&= (5 \times 1, 5 \times 2) + (6 \times 3, 6 \times 4) \\
&= (5, 10) + (18, 24) \\
&= (5 + 18, 10 + 24) \\
&= (23, 34)
\end{aligned}
$$

ご覧の通り、どちらの計算方法でも同じ結果が得られます。この公式は、行列の積が持つ、**「行ベクトルが行列の各行を線形結合する」**という性質を非常に簡潔に表現しているのです。

### なぜこれが重要なのか？

この性質は、機械学習、特にニューラルネットワークにおいて非常に重要です。

* **線形変換**: 行列の積は、ベクトルの線形変換と見なすことができます。この公式は、入力ベクトル（$\vec{x}$）が、行列の行ベクトル（$\vec{a_i}$）をどのように組み合わせて新しいベクトルを生成するかを示しています。
* **特徴の組み合わせ**: 例えば、$\vec{x}$ が入力データ、$A$ がニューラルネットワークの層の重み行列だとすると、出力 $\vec{x}A$ は入力特徴を様々な方法で組み合わせた新しい特徴表現として解釈できます。各 $x_i$ は入力の一部分を、$\vec{a_i}$ はその部分から抽出されるべきパターンや特徴を表していると考えることができます。

このように、この一見単純な数式は、行列演算の深い意味と、それがどのようにデータ変換や特徴学習に応用されるかを示すものです。