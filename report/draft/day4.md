)で更新
 
 
Discriminatorの更新を複数回行うごとに、Generatorの更新を1回行う(毎回ペアで行うわけではない)
生成データと真データがそっくりな状況とは、
この時に価値関数が最適化されており、
は最小値となっている
DCGAN(Deep Convolutional GAN)
GANを利⽤した画像⽣成モデル
いくつかの構造制約により⽣成品質を向上
Generator
Pooling層の代わりに転置畳み込み層(Transposed Convolution)を使用し、乱数を画像にアップサンプリング
最終層はtanh(-1~1)、その他はReLU関数で活性化
Discriminator
Pooling層の代わりに畳み込み層を使用
Leaky ReLU関数で活性化、最終層はsigmoid(0~1)
共通事項
中間層に全結合層を使わない
バッチノーマライゼーションを適用
応用技術
Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars: 1枚の顔画像から動画像(Avatar)を高速に生成するモデル
初期化部と推論部からなる
初期化: ⼈物の特徴を抽出、1アバターにつき⼀回の計算コスト
推論: 所望の動きを付ける、時間フレーム分だけの計算コスト
初期化の計算コストは重いが、推論の計算コストが小さいのでリアルタイムで推論できる
推論部は緻密な輪郭と粗い顔画像を別々に生成し結合することで、計算コストを下げている
Conditional GAN(CGAN)
GANの生成したいデータに条件をつける
条件はラベルで指定(例: 「犬」という条件で犬の画像を生成する)
基本的なネットワークはGANと同様
ただし、DおよびG双方に新しい入力(条件パラメーター)が追加となる
Pix2pix
CGANと同様の考え方
条件としてラベルではなく、画像を用いる
条件画像が入力され、何らかの変換を施した画像を出力する
つまり、画像の変換方法を学習
条件画像xと真の何らかの変換が施された画像yのペアが学習データとなる
CGANと同様に、DおよびG双方に新しい入力(条件画像)が追加となる
Pix2pixにおける工夫
GeneratorでU-Netを使用し、物体の位置を抽出
U-Netはセマンティックセグメンテーションを実現するEncoder-decoder
スキップ接続で物体の位置情報をDecoderに伝達
損失関数にL1正則化項を追加
画像の高周波部分(色の変化が顕著な部分)を学習し、Generatorが生成した画像がぼやけることを防ぐ
PatchGAN
条件画像をパッチに分けて，各パッチにPix2pixを適応
正確な高周波成分の強調による視覚的一致性の向上
L1正則化項の効果を向上
深層強化学習
A3C(Asynchronous Advantage Actor-Critic)
強化学習の学習法の一つ
特徴は複数のエージェントが同一の環境で非同期に学習すること
Asynchronous: 複数のエージェントによる非同期な並列学習
Advantage: 複数ステップ先を考慮して更新する手法
Actor: 方策によって行動を選択
Critic: 状態価値関数に応じて方策を修正
Actor-Criticとは、行動を決めるActor(行動器)を直接改善しながら、方策を評価するCritic(評価器)を同時に学習させるアプローチ
A3Cによる非同期学習の詳細
複数のエージェントが並列に自律的に、rollout(ゲームプレイ)を実行し、勾配計算を行う
その勾配情報をもって、好き勝手なタイミングで共有ネットワーク(パラメーターサーバー)を更新する
各エージェントは定期的に自分のネットワーク(local network) の重みをglobal networkの重みと同期する
並列分散エージェントで学習を行うA3Cのメリット
学習が高速化
学習を安定化
経験の自己相関が引き起こす学習の不安定化は、強化学習の長年の課題
A3Cはオンポリシー手法(直接方策を評価する)であり、サンプルを集めるエージェントを並列化することで自己相関を低減することに成功
A3Cの課題
Python言語の特性上、非同期並行処理を行うのが面倒
パフォーマンスを最大化するためには、大規模なリソースを持つ環境が必要
A3Cのロス関数
一般的なActor-Criticでは、方策ネットワークと価値ネットワークを別々に定義し、別々のロス関数（方策勾配ロス/価値ロス）でネットワークを更新する
これに対し、A3Cはパラメーター共有型のActor Criticであり、１つの分岐型のネットワークが、方策と価値の両方を出力し、たった１つの「トータルロス関数」でネットワークを更新
トータルロス関数は、アドバンテージ方策勾配、価値関数ロス、方策エントロピーの組み合わせ
A2C
A3Cの後に、同期処理を行うA2Cが発表された
同期処理なので、Pythonでも実装しやすい
性能がA3Cに劣らないことがわかったので、その後よく使われるようになった
様々な研究
Metric-learning(距離学習)
距離学習ではデータ間の metric、すなわち「データ間の距離」を学習する
データ間の距離を適切に測ることができれば、距離が近いデータ同士をまとめてクラスタリングができたり、他のデータ要素から距離が遠いデータを異常と判定することで異常検知したりと様々な応用が可能となる
深層距離学習(deep metric learning)は、ディープラーニング技術を利用した距離学習の手法
CNN等で出力された特徴ベクトル(feature vector)の属する空間は埋め込み空間(embedding space)と呼ばれるが、この埋め込み空間内で類似サンプルの特徴ベクトルは近くに、非類似サンプルの特徴ベクトルは遠くに配置されるように学習を行う
Siamese network
2つのサンプルをペアで入力しそれらのサンプル間の距離を明示的に表現して調整する
CNNにペア画像を入力し、その出力である特徴ベクトルを得る
埋め込み空間内での2つの特徴ベクトルの距離Dが「最適」となるようにネットワークのパラメータを学習する
ペア画像が同じクラスの場合には距離Dが小さくなるように、逆に異なるクラス
の場合には大きくなるように損失関数Lを設計し、学習を行う
損失関数contrastive lossの最小化を図る
 
は入力サンプルのペアが同じクラスに属する場合には1, 異なるクラスの場合には0
はユークリッド距離(L2距離)を使うことが多い
の場合には第一項で近付ける方向に、
の場合には第二項で遠ざける(マージン
まで)方向に働く
Siamese networkの欠点は、同じクラスのデータに対しては
となるまで、つまり埋め込み空間中のある1点に埋め込まれるまで最適化し続けてしまう点
類似度を反映した良い埋め込み空間を得ることが難しい
Triplet network
Triplet networkでは3つのサンプルを一組で入力する
3つのサンプルは、基準となるサンプル
(アンカーサンプル)と、その類似サンプル
、および非類似サンプル
損失関数triplet lossの最小化を図る
の場合に損失関数が正の値を持ってペナルティーが課される構造になっている
これは非類似サンプルの特徴ベクトル
が、類似サンプルの特徴ベクトル
+マージン
より内側にある状態
Triplet networkのメリット(特にSiamese networkと比較して)
Triplet networkは、類似サンプルについて、
となるまでの最適化をしないので、類似度を反映した良い埋め込み空間を得ることができる
またTriplet networkは類似/非類似サンプルを用いるため、何が類似かがはっきりと示されている
この点、Siamese networkはコンテクストを考慮する必要がある
Triplet networkの課題
学習がすぐに停滞してしまう
学習データセットのサイズが増えてくると考えうる入力の組み合わせが膨大になり、かつその殆どの組み合わせが学習が進むにつれてパラメータ更新に影響を及ぼさなくなるため
このため、学習に有効な入力セットを厳選する必要がある(triplet selection/triplet mining)
クラス内距離がクラス間距離より小さくなることを保証しない
4つのサンプルを使うQuadrupt lossという損失関数が提案されている
MAML(Model-Agnostic Meta-Learning, メタ学習)
深層学習モデルの開発に必要なデータ量を削減したい
MAMLのコンセプト: タスクに共通する重みを学習し、新しいモデルの学習に活用
タスクごとの学習を行った結果を共通重みに反映させ学習
学習手順
まず共通重み
をランダムに初期化
Inner loopでタスク集合から個別タスクを取り出し、そのタスクで最適化された重みを得る
これをタスクの個数分繰り返す
ここのタスクで得られた重みを集め、SGDで共通重み
を更新する
これをOuter loopとして、
が収束するまで繰り返す
効果があることは確認されているが、タスクごとの勾配計算と共通パラメータの勾配計算の2回が必要となるため、計算量が多い
また実用的にはInner loopのステップ数を大きくできない
これらについては、計算コストを削減する近似方法が提案されている
グラフ畳み込み(GCN)
元の関数にフィルターをかける
用途は様々だが、グラフ畳み込みにおいては特徴をはっきりさせるため
2次元画像に対してフィルターをかけるのがCNN
グラフ(ノードとエッジの結びついたデータ構造)に対してフィルターをかけるのがGCN
畳み込みで関数の特徴を際立たせている
畳み込みの一般的な形
連続的な関数に適用
離散的な関数に適用
 
や
が重み
元の関数に重みをかけて、強調するところと、捨象するところの区別をする
二つのアプローチ
Spatial(空間的)な場合
グラフにクラスタリングを繰り返す
ノイズが目立たなくなり、形をはっきりさせる
Spectral(スペクトル的)な場合
スペクトルに分解することで、特徴的な成分を明らかにする
グラフ畳み込みにフーリエ変換を応用する(グラフフーリエ変換)
計算量が多いのが課題
深層学習の説明性
ディープラーニング活用の難しいことの１つは 「ブラックボックス性」
判断の根拠を説明できない
モデルの解釈性 に注目し、 「ブラックボックス性」の解消 を目指した研究が進められている
CAM
Global Average Pooling(GAP)は通常学習の過学習を防ぐ、正則化の役割として使われてきたが、CNNが潜在的に注目している部分を可視化できるようにする役割も持っていることがわかった
CAMとは、出力層の重みを畳み込み特徴マップに投影することで、画像領域の重要性を識別する
実装としては、出力層につながっている重みのうち、どのクラスかを指定し、最後の畳み込み層の特徴マップとの内積をとる
Grad-CAM
CNNモデルに判断根拠を持たせ、モデルの予測根拠を可視化する手法
名称の由来は ”Gradient” = 「勾配情報」
最後の畳み込み層の予測クラスの出力値に対する勾配を使用
勾配が大きいピクセルに重みを増やす： 予測クラスの出力に大きく影響する重要な場所
特徴マップと重み係数を線形結合し、ReLU関数で出力する
CAMはモデルのアーキテクチャにGAPがないと可視化できなかったのに
対し、Grad CAM はGAPがなくても可視化できる
また、出力層が画像分類でなくてもよく、様々なタスクで使える
LIME(Local Interpretable Model-agnostic Explanations)
特定の入力データに対する予測について、その判断根拠を解釈・可視化するツール
表形式データ： 「どの変数が予測に効いたのか」
画像データ： 「画像のどの部分が予測に効いたのか」
単純で解釈しやすいモデルを用いて、複雑なモデルを近似することで解釈を行う
LIMEへの入力は１つの個別の予測結果
モデル全体の近似は複雑すぎる
対象サンプルの周辺のデータ空間からサンプリングして集めたデータセットを教師データとして、データ空間の対象範囲内でのみ有効な近似用モデルを作成
近似用モデルから 予測に寄与した特徴量を選び 、解釈を行うことで、本来の難解なモデルの方を解釈したことと見なす
SHAP
協力ゲーム理論の概念であるshapley value（シャープレイ値）を機械学習に応用
Shapey valueは、プレイヤーが協力し、それによって獲得した
報酬を分配する環境において、平均的な限界貢献度を意味する
機械学習においては、モデルから出力された予測値を、貢献度が異なる特徴量にどう分配するか
機械学習において、ある順序における、予測値への特徴量の貢献度を表す
開発・運用環境
Docker
コンテナ仮想化は仮想マシンに対してより軽量
コンテナ仮想化はアプリケーションの仮想化を実現する
Dockerは、コンテナ仮想化を用いてアプリケーションを開発・配置・実行するためのオープンプラットフォーム
Dockerはコンテナ仮想化を用いたOSレベルの仮想化によりアプリケーションを開発・実行環境から隔離し、アプリケーションの素早い提供を可能にする
その環境自体をアプリケーションと同じようにコード（イメージ）として管理可能にする
Dockerを開発・テスト・デプロイに用いることで「コードを書く」と「コードが製品として実行される」間の時間的ギャップを大きく短縮できる
以上の出所はWikipedia
Batch Normalization(バッチ正規化)
バッチ正規化ではバッチ単位で正規化する
ミニバッチに含まれる全サンプルの同一チャネルが同一分布に従う
しかし、バッチ正規化はミニバッチのサイズを大きく取れない場合には、効果が薄くなってしまう
また、ミニバッチのサイズが学習環境(演算器の性能)によって変わることもあり、何が効果なのか見えにくい
このため実際にはあまり活用したくない
代替的な手法としてLayer Normalization、Instance Normalizationがある
これらはミニバッチのサイズの影響を受けない
レイヤー正規化では、それぞれのサンプルのH×W×C全てのpixelを正規化する
入力データのスケールに対してロバスト
重み行列のスケールやシフトに関してロバスト
インスタンス正規化では、各サンプルの各チャネルごとに正規化する
コントラストの正規化に寄与する
画像のスタイル転送やテクスチャ合成タスクなどで利用される
参考文献
ディープラーニング入門 Chainer チュートリアル
ディープラーニングE資格エンジニア問題集
ゼロから作るDeep Learning
ゼロから作るDeep Learning 2 自然言語処理編
ゼロから作るDeep Learning 4 強化学習編
機械学習のエッセンス -実装しながら学ぶPython,数学,アルゴリズム- (Machine Learning)