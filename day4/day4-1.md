- [Section1 : 強化学習](#section1--強化学習)
  - [強化学習（Reinforcement Learning）](#強化学習reinforcement-learning)
  - [方策関数と方策勾配法](#方策関数と方策勾配法)
    - [方策関数とは？](#方策関数とは)
    - [方策勾配法](#方策勾配法)
    - [方策の良さ：$J(\\theta)$](#方策の良さjtheta)
      - [平均報酬](#平均報酬)
      - [割引報酬和](#割引報酬和)
    - [方策勾配定理（Policy Gradient Theorem）](#方策勾配定理policy-gradient-theorem)
  - [TD（Temporal Difference Learning）学習](#tdtemporal-difference-learning学習)
    - [TD学習の流れ](#td学習の流れ)
    - [時差誤差（TD誤差）](#時差誤差td誤差)
    - [更新式](#更新式)
    - [メリット](#メリット)
    - [TD学習の流れ](#td学習の流れ-1)
    - [時差誤差（Temporal Difference Error）](#時差誤差temporal-difference-error)
    - [価値関数の更新](#価値関数の更新)
    - [応用](#応用)
- [Section2 : AlphaGo](#section2--alphago)
  - [AlphaGoの学習フロー](#alphagoの学習フロー)
  - [PolicyNetの教師あり学習](#policynetの教師あり学習)
  - [ValueNetの学習](#valuenetの学習)
  - [モンテカルロ木探索](#モンテカルロ木探索)
  - [参考資料による学習](#参考資料による学習)
  - [Alpha Go (Lee)のモンテカルロ木探索](#alpha-go-leeのモンテカルロ木探索)
    - [評価](#評価)
    - [成長](#成長)
  - [AlphaGo (Lee) とAlphaGo Zeroの違い](#alphago-lee-とalphago-zeroの違い)
  - [Residual Network](#residual-network)
    - [バックアップResidual Networkの派生形](#バックアップresidual-networkの派生形)
  - [Alpha Go Zeroのモンテカルロ木探索](#alpha-go-zeroのモンテカルロ木探索)
    - [選択](#選択)
    - [評価と成長](#評価と成長)
    - [バックアップ](#バックアップ)
    - [Alpha Go Zeroの学習法](#alpha-go-zeroの学習法)
    - [学習](#学習)
    - [ネットワークの更新](#ネットワークの更新)
    - [コンピューター進化 vs 処理要求](#コンピューター進化-vs-処理要求)
  - [参考](#参考)
- [Section3 : 軽量化・高速化技術](#section3--軽量化高速化技術)
  - [分散深層学習](#分散深層学習)
  - [データ並列化](#データ並列化)
    - [同期型](#同期型)
    - [非同期型](#非同期型)
    - [モデル並列化](#モデル並列化)
    - [参照論文](#参照論文)
  - [GPUによる高速化](#gpuによる高速化)
    - [GPGPU開発環境](#gpgpu開発環境)
    - [モデルの軽量化とは](#モデルの軽量化とは)
      - [モデルの軽量化の利用](#モデルの軽量化の利用)
    - [軽量化の手法](#軽量化の手法)
    - [量子化（Quantization）](#量子化quantization)
    - [量子化の利点と欠点](#量子化の利点と欠点)
    - [精度の低下](#精度の低下)
    - [極端な量子化](#極端な量子化)
    - [省メモリ化](#省メモリ化)
    - [蒸留](#蒸留)
      - [教師モデルと生徒モデル](#教師モデルと生徒モデル)
    - [プルーニング](#プルーニング)
    - [モデルの軽量化まとめ](#モデルの軽量化まとめ)
- [Section4  応用技術](#section4--応用技術)
  - [Depthwise Convolution](#depthwise-convolution)
  - [MobileNet](#mobilenet)
  - [問題](#問題)
  - [WaveNetのメインアイディア](#wavenetのメインアイディア)
    - [時系列データに対して畳み込み（Dilated convolution）を適用する](#時系列データに対して畳み込みdilated-convolutionを適用する)
    - [図解](#図解)
    - [参照: https://arxiv.org/pdf/1609.03499.pdf](#参照-httpsarxivorgpdf160903499pdf)
    - [Pointwise Convolution](#pointwise-convolution)
  - [Dense Net](#dense-net)
  - [DenseNetとResNetの違い](#densenetとresnetの違い)
  - [DenseNet内で使用されるDenseBlockと呼ばれるモジュールでは成長率(Growth Rate)と呼ばれるハイパーパラメータが存在する。](#densenet内で使用されるdenseblockと呼ばれるモジュールでは成長率growth-rateと呼ばれるハイパーパラメータが存在する)
  - [Batch Norm](#batch-norm)
    - [Batch Normの問題点](#batch-normの問題点)
  - [Batch Norm以外の正規化](#batch-norm以外の正規化)
  - [Batch Norm.](#batch-norm-1)
  - [Layer Norm.](#layer-norm)
  - [Wavenetのメインアイディア](#wavenetのメインアイディア-1)
  - [Wavenet](#wavenet)
  - [問題](#問題-1)
    - [時系列データに対して畳み込み（Dilated convolution）を適用する](#時系列データに対して畳み込みdilated-convolutionを適用する-1)
    - [図解](#図解-1)
    - [参照: https://arxiv.org/pdf/1609.03499.pdf](#参照-httpsarxivorgpdf160903499pdf-1)

# Section1 : 強化学習
([目次に戻る](#目次))

## 強化学習（Reinforcement Learning）
強化学習は、**環境の中で行動を選択し、長期的に報酬を最大化できるエージェントを作る**ことを目的とした機械学習の一分野
報酬に基づいて行動戦略を改善し、**試行錯誤を通じて最適な行動方針**を学習

- 強化学習と通常の教師あり、教師なし学習との違い
- 目標が違う
  - 教師なし、あり学習では、データに含まれるパターンを見つけ出す
  およびそのデータから予測することが目標
  - 強化学習では、優れた方策を見つけることが目標

- 強化学習について
  - 冬の時代があったが、計算速度の進展により大規模な状態をもつ場合の強化学習を可能としつつある。
  - 関数近似法と、Q学習を組み合わせる手法の登場
    - Q学習
      行動価値関数を、行動する毎に更新することにより学習を進める方法
    - 関数近似法
      価値関数や方策関数を関数近似する手法のこと

## 方策関数と方策勾配法

### 方策関数とは？

- 方策ベースの強化学習手法において、**状態 $s$ における行動 $a$ の確率**を与える関数
- $$ \pi(a|s) $$：状態 $s$ において行動 $a$ を選択する確率

### 方策勾配法

- 方策関数を直接パラメータ化し、そのパラメータを最適化する手法
- 目的関数（方策の良さ）を最大化

### 方策の良さ：$J(\theta)$

$J(\theta$$ の定義は2通り

#### 平均報酬
$$ J(\theta) = \mathbb{E}_{s \sim d^{\pi}, a \sim \pi_{\theta}}[r(s,a)] $$

#### 割引報酬和
$$ J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right] $$

エージェントは方策に基づいて行動する

$\pi(S, \alpha)$: $V$や$Q$を基にどういう行動をとるか
$\Rightarrow$ 経験を活かす or チャレンジする など.
$\Rightarrow$ その瞬間、その瞬間の行動をどうするか?

$V_{\pi}(S)$: 状態関数
$Q_{\pi}(S, a)$: 状態+行動関数
$\left.\begin{array}{l} \text{ゴールまで今の方策を続けたときの} \\ \text{報酬の予測値が得られる}\end{array}\right\}$
$\Rightarrow$ やり続けたら最終的にどうなるか。


### 方策勾配定理（Policy Gradient Theorem）

- 方策のパラメータ更新は以下で行う：
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) Q^{\pi}(s,a) \right]
$$


## TD（Temporal Difference Learning）学習

| 手法           | 環境知識 | 更新対象     | ブートストラップ | 特徴                     |
| -------------- | -------- | ------------ | ---------------- | ------------------------ |
| 動的計画法     | 必要     | 状態価値関数 | 使える           | 最適解求まるが環境が必要 |
| モンテカルロ法 | 不要     | 状態価値関数 | 使えない         | 環境不要、学習不安定     |
| TD学習         | 不要     | 状態価値関数 | 使える           | 両者の利点を融合         |


### TD学習の流れ

1. 状態 $s$ において、方策 $\pi$ に基づき行動 $a$ を選択
2. 環境から報酬 $r$ と次の状態 $s'$ を得る
3. 価値関数 $V(s)$ を以下のように更新：

### 時差誤差（TD誤差）

$$
\delta = r + \gamma V(s') - V(s)
$$

### 更新式

$$
V(s) \leftarrow V(s) + \alpha \cdot \delta
$$

### メリット

- エピソード完了を待たずに学習可能
- モンテカルロより安定
- 環境モデルが不要


### TD学習の流れ
1. エージェントは状態 $s$ において、方策 $\pi$ に基づいて行動 $a$ を選択  
2. 環境は、報酬 $r$ と次の状態 $s'$ を返す  
3. 状態 $s$ の予測価値 $V(s)$ と、実際の報酬 $r + \gamma V(s')$ の差分（**時差誤差**）を計算し、$V(s)$ を更新する

### 時差誤差（Temporal Difference Error）

$$
\delta = r + \gamma V(s') - V(s)
$$

### 価値関数の更新

$$
V(s) \leftarrow V(s) + \alpha \cdot \delta
$$

- $\delta$ は、実際の報酬と予測報酬のギャップ
- $\alpha$：学習率。$\delta$ にかけることで安定性と収束の速さを調整

### 応用

- Q学習（Q-Learning）
  - 方策**オフ型**（off-policy）  
  - **学習時の行動選択と更新に使う行動が異なる**
  - 最適な行動価値関数を直接学習

更新式：

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s,a) \right]
$$

→ 最適方策を探索中の方策とは無関係に学習可能

- SARSA（State-Action-Reward-State-Action）
  - 方策**オン型**（on-policy）  
  - **現在の方策に従って次の行動を選択し、それに基づいて更新**

更新式：

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma Q(s', a') - Q(s,a) \right]
$$

→ リスクのある行動は避けられやすく、安全側に学習されやすい

# Section2 : AlphaGo
([目次に戻る](#目次))

## AlphaGoの学習フロー
出典: David Silver et al., Mastering the game of Go with deep neural networks and tree search,
      Nature 529, 484-489 (2016)
      https://www.nature.com/articles/nature16961

1. 教師あり学習によるRolloutPolicyとPolicyNetの学習
2. 強化学習によるPolicyNetの学習
3. 強化学習によるValueNetの学習

## PolicyNetの教師あり学習
- KGS Go Server (ネット囲碁対局サイト) の棋譜データから3000万局面分の教師データを用意し、教師と同じ着手を予測できるように学習を行った。
- 具体的には、教師が着手した手を1とし残りを0とした19×19次元の配列を教師とし、それを分類問題として学習した。
- この学習で作成したPolicyNetは57%ほどの精度である。
- 現状のPolicyNet同士の対局ではなく、PolicyPoolに保存されているものとの対局を使用する理由は、対局に幅を持たせて過学習を防ぐというのが主である。
- この学習をmini batch size 128で1万回行った。

## ValueNetの学習
PolicyNetを使用して対局シミュレーションを行い、その結果の勝敗を教師として学習した。
**教師データ作成の手順**
1. まずSL PolicyNet(教師あり学習で作成したPolicyNet)でN手まで打つ。
2. N+1手目をランダムに選択し、その手で進めた局面を$S(N+1)$とする。
3. $S(N+1)$からRL PolicyNet(強化学習で作成したPolicyNet)で終局まで打ち、その勝敗報酬をRとする。
- $S(N+1)$とRを教師データ対とし、損失関数を平均二乗誤差とし、回帰問題として学習した。
- この学習をmini batch size 32で5000万回行った
- N手までとN+1手からのPolicyNetを別々にしている理由は、過学習を防ぐためであると論文中では説明されている。

## モンテカルロ木探索
- コンピュータ囲碁ソフトでは現在もっとも有効とされている探索法。
- 他のボードゲームではminmax探索やその派生形の$\alpha\beta$探索を使うことが多いが、盤面の価値や勝率予測関数の精度が重要。しかし囲碁では盤面価値や勝率予測値を出すのが困難
- そこで、盤面評価値に頼らず終端局面(勝敗のみ)を使って探索できないか、という発想で生まれた
  囲碁の場合、他のボードゲームと違い最大手数はマス目の数ほぼ限定されるため、終端局面に到達しやすい。
- 具体的には、現局面から終端局面までPlayOutと呼ばれるランダムシミュレーションを多数回行い、その勝敗を集計して着手の優劣を決定
  また、該当手のシミュレーション回数が一定数を超えたら、その手を着手したあとの局面をシミュレーション開始局面とするよう、探索木を成長させる。
- **探索木の成長**を行うというのがモンテカルロ木探索の優れているところである。
- モンテカルロ木探索は木の成長を行うことによって、一定条件下において探索結果は最善手を返すということが理論的に証明されている。

## 参考資料による学習
- 【強化学習】モンテカルロ木探索を解説・実装
https://qiita.com/pocokhc/items/392a7f89c79f5e1e6bca#%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%B3%95monte-carlo-method-mc

モンテカルロ法をざっくり言うと、適当な回数ランダムに実行してみてその結果から結論を導くアルゴリズムです。
主に確率または期待値の近似値を求める手法として使われます。
（強化学習では予測値を求める手法としてモンテカルロ法がありますがそれとは別ものです）

円周率を求める事例が多いですが、カードの例でやり方を見てみます。

問題 ：デッキが30枚、初手に5枚ドローするカードゲームがあります。あるキーカードが初手に欲しいです。キーカードを何枚入れれば初手にキーカードが来るでしょうか。ただし、１回マリガン（手札を入れ替える）出来るとします。

キーカードが1枚入ってる場合の確率、2枚の確率…とそれぞれの確率を比較して枚数を決めたいところです。
ここで統計学として厳密に確率を求めようとする場合、かなり難しいと思います。
しかし、こういう近似値で十分な確率を求めたい場合はモンテカルロ法を使うと簡単に出せます。


for key_num in range(1, 20):
```python    
    simulation_num = 100000
    included_key_num = 0
    for i in range(simulation_num):

        # 指定枚数入ったデッキを作成(1がキーカード)
        deck = [0] * 30
        for i in range(key_num):
            deck[i] = 1

        # ランダムに５枚引く
        hands = random.sample(deck, 5)

        # キーカードが入っていない場合は引き直し（マリガン）
        if sum(hands) == 0:
            hands = random.sample(deck, 5)

        # キーカードが入っていればカウントを増やす
        if sum(hands) > 0:
            included_key_num += 1

    # モンテカルロ法: シミュレーションの結果から確率の近似値を出す
    prob = included_key_num / simulation_num
    print(f"キーカード数: {key_num}, 確率 {prob*100:.2f}%")
```

実行結果
```
キーカード数: 1, 確率 30.56%
キーカード数: 2, 確率 52.76%
キーカード数: 3, 確率 67.97%
キーカード数: 4, 確率 78.70%
キーカード数: 5, 確率 86.19%
キーカード数: 6, 確率 91.09%
キーカード数: 7, 確率 94.43%
キーカード数: 8, 確率 96.53%
キーカード数: 9, 確率 97.96%
キーカード数: 10, 確率 98.84%
~~
キーカード数: 19, 確率 100.00%
```

## Alpha Go (Lee)のモンテカルロ木探索
Alpha Goのモンテカルロ木探索は選択、評価、バックアップ、成長という4つのステップで構成される

### 評価
- 着手選択方策によって選ばれた手で進めた局面$s_L$がLeafNodeであればその局面$s_L$をValueNetで評価
- 局面$s_L$を開始局面とした終端局面までのシミュレーションを行い、勝敗値を出す。
- シミュレーション時にはRollOut方策を使用する。
### 成長
選択、評価、バックアップを繰り返し一定回数選択された手があったら、その手で進めた局面の合法手ノードを展開し、探索木を成長させる。

## AlphaGo (Lee) とAlphaGo Zeroの違い

1. 教師あり学習を一切行わず、強化学習のみで作成
2. 特徴入力からヒューリスティックな要素を排除し、石の配置のみにした
3. PolicyNetとValueNetを1つのネットワークに統合した
4. Residual Net (ResNet) を導入した
5. モンテカルロ木探索からRollOutシミュレーションをなくした



## Residual Network
- ネットワークにショートカット構造を追加して、勾配の爆発、消失を抑える効果を狙ったもの
- Residual Networkを使うことにより、100層を超えるネットワークでの安定した学習が可能となった
- 基本構造
  - Convolution → BatchNorm → ReLU → Convolution → BatchNorm → Add → ReLUのBlockを1単位にして積み重ね
- また、Resisual Networkを使うことにより層数の違うNetworkのアンサンブル効果が得られているという説もある

### バックアップResidual Networkの派生形

- Residual Blockの工夫
  - Bottleneck
    1x1 KernelのConvolutionを利用し、1層目で次元削減を行って3層目で次元を復元する3層構造にし、2層のものと比べて計算量はほぼ同じだが1層増やせるメリットがある、としたもの

  - PreActivation
    Residual Blockの並びをBatchNorm→ReLU→Convolution→BatchNorm→ReLU→Convolution→Addとすることにより性能が上昇するとしたもの

- Network構造の工夫
  - WideResNet
      ConvolutionのFilter数を$k$倍にしたResNet。1倍→$k$倍xブロック→2*$k$倍xブロックと段階的に幅を増やしていくのが一般的。Filter数を増やすことにより、浅い層数でも深い層数のものと同等以上の精度となり、
      またGPUをより効率的に使用できるため学習も早い

  - PyramidNet
    WideResNetで幅が広がった直後の層に過度の負担がかかり精度を落とす原因となっているとし、段階的にではなく、各層でFilter数を増やしていくResNet。

## Alpha Go Zeroのモンテカルロ木探索
Alpha Goのモンテカルロ木探索は選択、評価、バックアップという3つのステップで構成される

### 選択
Root局面にて着手選択方策 $p(s, a) + c\sqrt{\frac{\sum_b N(s, b)}{1 + N(s, a)}}$ に従って手を選択する。
選択された合法手$a$で進めた局面がLeafノードでなければ、そのノードの着手選択方策に従って選択を行い、局面を進める。
選択された合法手$a$で進めた局面がLeafノードであれば評価および成長ステップに移行する。

### 評価と成長
Leafノードまで進めた局面$s_L$をPolicyValueNetで評価する。RollOutは行わない。
また、局面$s_a$の合法手ノードを展開し木を成長させる。

### バックアップ
評価フェイズで評価した値を累積する。局面$s_a$でのValueNetの評価の累積値$W_v$が累積され、$N(s,a)$と$\Sigma N(s,b)$が1加算される。
それらの値から勝敗期待値 $Q(s, a) = \frac{W_v}{N(s, a)}$ が再計算される。これをRoot局面までさかのぼって更新する。

### Alpha Go Zeroの学習法
Alpha Goの学習は自己対局による教師データの作成、学習、ネットワークの更新の3ステップで構成される
- 自己対局による教師データの作成
- 現状のネットワークでモンテカルロ木探索を用いて自己対局を行う。
= まず30手までランダムで打ち、そこから探索を行い勝敗を決定する。
- 自己対局中の各局面での着手選択確率分布と勝敗を記録する。
- 教師データの形は (局面、着手選択確率分布、勝敗) が1セットとなる。

### 学習
- 自己対局で作成した教師データを使い学習を行う。
- NetworkのPolicy部分は教師の着手選択確率分布を用い、Value部分の教師に勝敗を用いる。
- 損失関数はPolicy部分はCrossEntropy、Value部分は平均二乗誤差。

### ネットワークの更新
学習後、現状のネットワークと学習後のネットワークとで対局テストを行い、学習後の
ネットワークの勝率が高かった場合、学習後のネットワークを現状のネットワークとする。

### コンピューター進化 vs 処理要求
- モデルの処理能力要求：**年10倍ペース**
- コンピューターの性能進化：**18〜24ヶ月で2倍**

## 参考

- [Reinforcement Learning: An Introduction - Sutton & Barto](http://incompleteideas.net/book/the-book.html)
- [AlphaGo論文 (Nature 2016)](https://www.nature.com/articles/nature16961)
# Section3 : 軽量化・高速化技術
([目次に戻る](#目次))

## 分散深層学習
- 深層学習は多くのデータを使用したり、パラメータ調整のため
に多くの時間を使用したりするため、高速な計算が求められる。
- 複数の計算資源(ワーカー)を使用し、並列的にニューラルネッ
トを構成することで、効率の良い学習を行いたい。
- データ並列化、モデル並列化、GPUによる高速技術は不可欠で
ある。

## データ並列化
- 親モデルを各ワーカーに複製
- 各ワーカーが異なるデータを処理

### 同期型
- 全ワーカーの勾配計算完了を待つ
- 勾配の平均で親モデルを一括更新
- **安定性高、処理速度やや遅め**

### 非同期型
- 各ワーカーが独立して更新
- パラメーターサーバーに push/pop(学習したらスタックに積んでく感じ？)
- **高速だが収束不安定**

 | 項目         | 同期型                                  | 非同期型                                                                  |
 | ------------ | --------------------------------------- | ------------------------------------------------------------------------- |
 | 処理スピード | 他ワーカーの計算を待つ→非同期型より遅い | 他ワーカーの計算を待たない→同期型より早                                   |
 | 学習の安定性 | 比較的安定                              | 最新のモデルのパラメータを利用できないため不安定 (Stale Gradient Problem) |
 | 現在の主流   | 主流                                    |                                                                           |
 | 精度         | 良いことが多い                          |

### モデル並列化
- モデルを分割し、各ワーカーが一部を担当
- 分割方法：
  - 層単位
  - **分岐単位（主流）**
- 大規模モデルで効率的（パラメータ数が多いほど効果大）

### 参照論文
- Large Scale Distributed Deep Networks
- Google社が2016年に出した論文
- Tensorflowの前身といわれている
- 並列コンピューティングを用いることで大規模なネットワークを
高速に学習させる仕組みを提案。
- 主にモデル並列とデータ並列(非同期型)の提案をしている。


## GPUによる高速化

- GPGPU (General-purpose on GPU)
    - 元々の目的であるグラフィック以外の用途で使用されるGPUの総称
- CPU
    - 高性能なコアが少数
    - 複雑で連続的な処理が得意
- GPU
    - 比較的低性能なコアが多数
    - 簡単な並列処理が得意
    - ニューラルネットの学習は単純な行列演算が多いため、高速化が可能。

### GPGPU開発環境

- CUDA
    - GPU上で並列コンピューティングを行うためのプラットフォーム
    - NVIDIA社が開発しているGPUのみで使用可能。
    - Deep Learning用に提供されているので、使いやすい
- OpenCL
    - オープンな並列コンピューティングのプラットフォーム
    - NVIDIA社以外の会社(Intel, AMD, ARMなど)のGPUからでも使用可能
    - Deep Learning用の計算に特化しているわけではない。

- Deep Learningフレームワーク(Tensorflow, Pytorch)内で実装されているので、使用する際は指定すれば良い

### モデルの軽量化とは
モデルの精度を維持しつつパラメータや演算回数を低減する手法の総称

|          | 高メモリ負荷                                  | 低メモリ負荷                                                                 |
| -------- | --------------------------------------------- | ---------------------------------------------------------------------------- |
| 演算性能 | 高い演算性能が求められる 通常はサーバーで利用 | 低い演算性能での利用が必要とされる IoT など エッジコンピューティングでの利用 |
| メモリ   | 高メモリ                                      | 低メモリ                                                                     |

> 山本康平, 橘素子, 前野蔵人. ディープラーニングのモデル軽量化技術. https://www.oki.com/jp/otr/2019/n233/pdf/otr233_r11.pdf

#### モデルの軽量化の利用
- モデルの軽量化はモバイル, IoT 機器、エッジコンピューティングでの利用において有用な手法

- モバイル端末や IoT はパソコンに比べ性能が大きく劣る
  >$\Rightarrow$ 主に計算速度と搭載されているメモリ

- モデルの軽量化は計算の高速化と省メモリ化を行うためモバイル, IoT 機器と相性が良い手法になる。

### 軽量化の手法

代表的な手法として下記の 3 つがある
- 量子化
- 蒸留
- プルーニング

### 量子化（Quantization）
- ネットワークが大きくなると大量のパラメータが必要なり学習や推論
に多くの メモリと演算処理が必要
- 通常のパラメータの 64 bit 浮動小数点を 32 bit など下位の精度に落
とすことでメモリと演算処理の削減を行う


### 量子化の利点と欠点

- 利点
  - 計算の高速化
  - 省メモリ化

- 欠点
  - 精度の低下

- 計算の高速化

  - 倍精度演算(64 bit)と単精度演算(32 bit)は演算性能が大きく違うため、量子化により精度を落とすことによりより多くの計算をすることができる。

- 深層学習で用いられるNVIDIA社製のGPUの性能

|                   | 単精度         | 倍精度        |
| ----------------- | -------------- | ------------- |
| NVIDIA Tesla V100 | 15.7 TeraFLOPS | 7.8 TeraFLOPS |
| NVIDIA Tesla P100 | 9.3 TeraFLOPS  | 4.7 TeraFLOPS |

https://www.nvidia.com/ja-jp/data-center/tesla-v100/
https://www.nvidia.co.jp/object/tesla-p100-jp.html

### 精度の低下

ニューロンが表現できる小数の有効桁が小さくなる
$\Downarrow$
モデルの表現力が低下する
$\Downarrow$
単精度の下限は$1.175494 \times 10^{-38}$、倍精度の下限は$2.225074 \times 10^{-308}$になる
$\Rightarrow$ 学習した結果ニューロンが$1.175494 \times 10^{-38}$未満の値になる場合など重みを表現できなくなる
$\Downarrow$
実際の問題では倍精度を単精度にしてもほぼ精度は変わらない

### 極端な量子化

極端な量子化を考える。表現できる値が0, 1 の1 bitの場合

$a = 0.1$が真値の時、関数$y(x) = ax$を近似する場合を考える際、学習によって$a$が0.1を得る必要がある
しかし、量子化によって$a$が表現できる値が0, 1 のため求められる式は
$y(x) = 0, y(x) = x$
のようになり、誤差の大きな式になってしまう。
$\Downarrow$
量子化する際は極端に精度が落ちない程度に量子化をしなければならない

### 省メモリ化
- 極端な量子化
  極端な量子化を考える。表現できる値が 0,1 の 1 bit の場合
  a = 0.1 が真値の時、関数 y(x) = ax を近似する場合を考える際、学習
  によって a が 0.1 を得る必要がある

  しかし、量子化によって a が表現できる値が 0,1 のため求められる式
  は
  y(x) = 0, y(x) = x のようになり、誤差の大きな式になってしまう。
  量子化する際は極端に精度が落ちない程度に量子化をしなければなら
  ない
  
### 蒸留
精度の高いモデルはニューロンの規模が大きい
(推論にメモリと演算処理多く必要)
$\Downarrow$
モデルの簡約化
- 学習済みの精度の高いモデルの知識を軽量なモデルへ継承させる
  知識の継承により、軽量でありながら複雑なモデルに匹敵する


#### 教師モデルと生徒モデル
蒸留は教師モデルと生徒モデルの2つで構成される
- 教師モデル
  予測精度の高い、複雑なモデルやアンサンブルされたモデル
- 生徒モデル
  教師モデルをもとに作られる軽量なモデル
> Geoffrey Hinton, Oriol Vinyals, Jeff Dean（2015）「Distilling the Knowledge in a Neural Network」,https://arxiv.org/abs/1503.02531.

- 教師モデルと生徒モデル
  教師モデルの重みを固定し生徒モデルの重みを更新していく
  誤差は教師モデルと生徒モデルのそれぞれの誤差を使い重みを更新し ていく
  
### プルーニング
大規模のネットワークの場合、大量のパラメーターが必要
$\Downarrow$
モデル精度に寄与少ないニューロンを削除することで、軽量化＆高速化


### モデルの軽量化まとめ

- **量子化**: 重みの精度を下げることにより計算の高速化と省メモリ化を行う技術
- **蒸留**: 複雑で精度の良い教師モデルから軽量な生徒モデルを効率よく学習を行う技術
- **プルーニング**: 寄与の少ないニューロンをモデルから削減し高速化と省メモリ化を行う技術

# Section4  応用技術
MobileNet
  - 論文タイトル
      - MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
  - 提案手法
      - ディープラーニングモデルは精度は良いが、その分ネットワークが深くなり計算量が増える。
      - 計算量が増えると、多くの計算リソースが必要で、お金がかかってしまう。
      - ディープラーニングモデルの軽量化・高速化・高精度化を実現 (その名の通りモバイルなネットワーク)
      - https://qiita.com/HiromuMasuda0228/items/7dd0b764804d2aa199e4

以下を組み合わせで軽量化
- Depthwise Convolution
- Pointwise Convolution

## Depthwise Convolution

* **仕組み**
    * 入力マップのチャンネルごとに畳み込みを実施
    * 出力マップのチャンネルはそれらと結合（入力マップのチャンネル数と同じになる）

* **図解**
    * 入力マップ: $H \times W \times C$
    * カーネル: $K \times K \times 1$
    * フィルタ数: 1
    * 出力マップ: $H \times W \times C$

## MobileNet

* 通常の畳み込みカーネルは全ての層にかかっていることを考えると計算量が大幅に削減可能
* 各層ごとの畳み込みなので層間の関係性は全く考慮されない。通常はPW畳み込みとセットで使うことで解決

## 問題
* MobileNetのアーキテクチャ
    * Depthwise Separable Convolutionという手法を用いて計算量を削減している。通常の畳み込みが空間方向とチャネル方向の計算を同時に行うのに対して、Depthwise Separable ConvolutionではそれらをDepthwise ConvolutionとPointwise Convolutionと呼ばれる演算によって個別に行う。
    * Depthwise Convolutionはチャネル毎に空間方向へ畳み込む。すなわち、チャネル毎に$D_K \times D_K \times 1$のサイズのフィルターをそれぞれ用いて計算を行うため、その計算量は（い）となる。
    * 次にDepthwise Convolutionの出力をPointwise Convolutionによってチャネル方向に畳み込む。すなわち、出力チャネル毎に$1 \times 1 \times M$サイズのフィルターをそれぞれ用いて計算を行うため、その計算量は（う）となる。

    * Depthwise Separable Convolutionという手法を用いて計算量を削減している。通常の畳み込みが空間方向とチャネル方向の計算を同時に行うのに対して、Depthwise Separable ConvolutionではそれらをDepthwise ConvolutionとPointwise Convolutionと呼ばれる演算によって個別に行う。
    * Depthwise Convolutionはチャネル毎に空間方向へ畳み込む。すなわち、チャネル毎に$D_K \times D_K \times 1$のサイズのフィルターをそれぞれ用いて計算を行うため、その計算量は（い）となる。
        * **解答**: (い) $D_K \cdot D_K \cdot M \cdot D_F \cdot D_F$
        * **解説**:
            * $D_K$: 畳み込みカーネルの縦横サイズ（例: $3 \times 3$なら$D_K=3$）
            * $M$: 入力チャネル数（Depthwise Convolutionでは各入力チャネルごとに1つのフィルタが適用されるため、出力チャネル数も$M$となる）
            * $D_F$: 特徴マップの縦横サイズ（畳み込み後の出力サイズ）
            * Depthwise Convolutionでは、$M$個の入力チャネルそれぞれに対して、$D_K \times D_K$のフィルタを適用します。各チャネルの畳み込みは、フィルタサイズ$D_K \times D_K$と出力特徴マップのサイズ$D_F \times D_F$の積に比例します。これが$M$個のチャネル分行われるため、総計算量は$D_K \cdot D_K \cdot M \cdot D_F \cdot D_F$となります。
    * 次にDepthwise Convolutionの出力をPointwise Convolutionによってチャネル方向に畳み込む。すなわち、出力チャネル毎に$1 \times 1 \times M$サイズのフィルターをそれぞれ用いて計算を行うため、その計算量は（う）となる。
        * **解答**: (う) $M \cdot N \cdot D_F \cdot D_F$
        * **解説**:
            * $M$: Depthwise Convolutionの出力チャネル数であり、Pointwise Convolutionの入力チャネル数となる。
            * $N$: Pointwise Convolutionの出力チャネル数（$N$個の$1 \times 1 \times M$フィルタを使用するため）。
            * $D_F$: 特徴マップの縦横サイズ（Depthwise Convolutionの出力と同じサイズ）。
            * Pointwise Convolutionでは、$D_F \times D_F$の各空間位置に対して、$M$個の入力チャネルと$N$個の出力チャネルを接続する$1 \times 1 \times M$フィルタを$N$個適用します。したがって、各空間位置での計算は$M \times N$となり、これが$D_F \times D_F$個の空間位置で繰り返されるため、総計算量は$M \cdot N \cdot D_F \cdot D_F$となります。

## WaveNetのメインアイディア

WaveNetは、音声生成や時系列データのモデリングに特化した深層学習モデルです。その主要なアイデアは、**Dilated convolution（拡張畳み込み）**という特殊な畳み込み層を使用することにあります。

### 時系列データに対して畳み込み（Dilated convolution）を適用する

* **Dilated convolution**
    * 通常の畳み込みが隣接する入力のみを参照するのに対し、Dilated convolutionは層が深くなるにつれて畳み込みのフィルタが**間隔を空けて**（"dilated"に）入力を参照します。これにより、広範囲の時系列情報を取り込むことができます。
    * 層が深くなるにつれて畳み込みリンクを飛ばすことで、少ない層数で非常に広い**受容野（Receptive Field）**を獲得することが可能になります。受容野とは、最終的な出力ユニットが影響を受ける入力領域の範囲を指します。
    * 受容野を効率的に増やすことができるという利点があります。これにより、長距離の依存関係を捉えることが得意になります。
    * 図（右）では、Dilated = 1, 2, 4, 8 となっている場合が示されており、層が深くなるにつれて参照する入力の間隔が指数関数的に広がっていく様子が分かります。

### 図解

* **左の図**: 既存の畳み込み層の視覚化です。通常の畳み込みでは、各層のニューロンが直前の層の隣接するニューロンのみに接続しているため、受容野を広げるには多くの層を重ねる必要があります。
* **右の図**: Dilated convolution層の視覚化です。層が深くなるにつれて、フィルタが参照する入力の間隔が広がっていくため、同じ層数でも従来の畳み込みよりもはるかに広い入力範囲（受容野）をカバーできます。

### 参照: https://arxiv.org/pdf/1609.03499.pdf

### Pointwise Convolution

* **仕組み**
    * $1 \times 1 \text{ conv}$ とも呼ばれる（正確には $1 \times 1 \times C$）
    * 入力マップのポイントごとに畳み込みを実施
    * 出力マップ（チャンネル数）はフィルタ数分だけ作成可能（任意のサイズが指定可能）

* **図解**
    * 入力マップ: $H \times W \times C$
    * カーネル: $1 \times 1 \times C$
    * フィルタ数: $M$
    * 出力マップ: $H \times W \times M$

## Dense Net
Dense Convolutional Network（DenseNet)

論文タイトル
  - Densely Connected Convolutional Networks. G. Huang et., al. 2016
  - https://arxiv.org/pdf/1608.06993.pdf
  - https://www.slideshare.net/harmonylab/densely-connected-convolutional-networks
-  概要
  -  畳込みニューラルネットワーク（CNN）アーキテクチャの一種
  -  ニューラルネットワークでは層が深くなるにつれ て、学習が難しくなるという問題
  -  Residual Network（以下、ResNet）などのCNNアーキテクチャでは前方の層から後方の層へアイデンティティ接続を介してパスを作ることで問題を対処
  - DenseBlockと呼ばれるモジュールを用いた、DenseNetもそのようなアーキテクチャの一つである。

## DenseNetとResNetの違い

* DenseNet内の**DenseBlock**では、**前方の各層からの出力全て**が後方の層への入力として用いられます。これは特徴の再利用を促進し、勾配の消失を防ぐ効果があります。
* ResNet内の**ResidualBlock**では、**前1層の入力のみ**後方の層へ入力されます。これは恒等写像を利用して残差学習を行うことで、深いネットワークの学習を容易にします。

## DenseNet内で使用されるDenseBlockと呼ばれるモジュールでは成長率(Growth Rate)と呼ばれるハイパーパラメータが存在する。

* DenseBlock内の各ブロック毎に$k$個ずつ特徴マップのチャンネル数が増加していく時、$k$を成長率と呼びます。この成長率が小さい場合でも、DenseNetは効率的に特徴を学習できるとされています。

## Batch Norm
* レイヤー間を流れるデータの分布を、ミニバッチ単位で平均が0・分散が1になるように正規化
* Batch Normalizationはニューラルネットワークにおいて学習時間の短縮や初期値への依存低減、過学習の抑制といった効果がある。

### Batch Normの問題点

* Batch Sizeが小さい条件下では、学習が収束しないことがあり、代わりにLayer Normalizationなどの正規化手法が使われることが多い。

## Batch Norm以外の正規化

* **Batch Norm**
    * ミニバッチに含まれるsampleの同一チャネルが同一分布に従うよう正規化
* **Layer Norm**
    * それぞれのsampleの全てのpixelsが同一分布に従うよう正規化
* **Instance Norm**
    * さらにchannelも同一分布に従うよう正規化

* **図解**
    * **Batch Norm**: ミニバッチ数N、チャネルC、高さと幅をまとめた$H \cdot W$というブロックで表現されており、ミニバッチ方向に正規化が行われることを示唆している。
    * **Layer Norm**: ミニバッチ数N、チャネルC、高さと幅をまとめた$H \cdot W$というブロックで表現されており、各サンプルのチャネルと空間全体にわたって正規化が行われることを示唆している。
    * **Instance Norm**: ミニバッチ数N、チャネルC、高さと幅をまとめた$H \cdot W$というブロックで表現されており、各サンプルの各チャネル内で正規化が行われることを示唆している。

* **参照**: https://arxiv.org/pdf/1803.08494.pdf

## Batch Norm.

* $H \times W \times C$ のsampleが$N$個あった場合に、$N$個の同一チャネルが正規化の単位である。
    * RGBの3チャネルのsampleが$N$個の場合は、それぞれのチャネルの平均と分散を求め正規化を実施（図の青い部分に対応）。チャネルごとに正規化された特徴マップを出力する。
* ミニバッチのサイズを大きく取れない場合には、効果が薄くなってしまう。

* **図解**:
    * 左の図では、$N$個のサンプル（各$H \times W \times C$）が積み重なっている様子が描かれている。そのうち、特定のチャネル（例えば赤、緑、青のどれか）において、$N$個のサンプル全てにわたる部分が青いブロックで強調されており、これがBatch Normで正規化の対象となる範囲を示している。
    * 中央の図は、$N=2$の場合のBatch Normの正規化対象の視覚化である。各サンプルの同一チャネル（例えば赤）が縦方向に結合されており、その範囲で正規化が行われることが示されている。
    * 右の図は、正規化された結果として、各チャネルがそれぞれ個別に正規化された特徴マップとして出力されることを示している。

* **参照**: https://arxiv.org/pdf/1803.08494.pdf

## Layer Norm.

* $N$個のsampleのうち一つに注目し、$H \times W \times C$ の全てのpixelが正規化の単位である。
    * RGBの3チャネルのsampleが1個の場合は、あるsampleを取り出し、全てのチャネルの平均と分散を求め正規化を実施（図の青い部分に対応）。特徴マップごとに正規化された特徴マップを出力する。
* ミニバッチの数に依存しないので、上記の（Batch Normの）問題を解決できると考えられる。

## Wavenetのメインアイディア

* 時系列データに対して畳み込み（Dilated convolution）を適用する
    * **Dilated convolution**
        * 層が深くなるにつれて畳み込みリンクを飛ばす
        * 受容野を効率的に増やすことができるという利点がある
        * 図（右）では、Dilated = 1, 2, 4, 8 となっている

* **図解**
    * 左の図: 既存の畳み込み層の視覚化。隣接する入力のみを参照している。
    * 右の図: Dilated convolution層の視覚化。層が深くなるにつれて間隔を空けた入力を参照しており、より広い受容野を持っていることが示されている。
    * 参照: https://arxiv.org/pdf/1609.03499.pdf

## Wavenet

* 深層学習を用いて結合確率を学習する際に、効率的に学習が行えるアーキテクチャを提案したことがWaveNetの大きな貢献の1つである。
* 提案された新しい**Convolution型アーキテクチャ**は（あ）と呼ばれ、結合確率を効率的に学習できるようになっている。
    * Dilated causal convolution
    * Depthwise separable convolution
    * Pointwise convolution
    * Deconvolution

## 問題
WaveNet

* （あ）を用いた際の大きな利点は、単純なConvolution layer と比べて（い）ことである。
    * パラメータ数に対する受容野が広い
    * 受容野あたりのパラメータ数が多い
    * 学習時に並列計算が行える
    * 推論時に並列計算が行える
    
    ## WaveNetのメインアイディア

WaveNetは、音声生成や時系列データのモデリングに特化した深層学習モデルです。その主要なアイデアは、**Dilated convolution（拡張畳み込み）**という特殊な畳み込み層を使用することにあります。

### 時系列データに対して畳み込み（Dilated convolution）を適用する

* **Dilated convolution**
    * 通常の畳み込みが隣接する入力のみを参照するのに対し、Dilated convolutionは層が深くなるにつれて畳み込みのフィルタが**間隔を空けて**（"dilated"に）入力を参照します。これにより、広範囲の時系列情報を取り込むことができます。
    * 層が深くなるにつれて畳み込みリンクを飛ばすことで、少ない層数で非常に広い**受容野（Receptive Field）**を獲得することが可能になります。受容野とは、最終的な出力ユニットが影響を受ける入力領域の範囲を指します。
    * 受容野を効率的に増やすことができるという利点があります。これにより、長距離の依存関係を捉えることが得意になります。
    * 図（右）では、Dilated = 1, 2, 4, 8 となっている場合が示されており、層が深くなるにつれて参照する入力の間隔が指数関数的に広がっていく様子が分かります。

### 図解

* **左の図**: 既存の畳み込み層の視覚化です。通常の畳み込みでは、各層のニューロンが直前の層の隣接するニューロンのみに接続しているため、受容野を広げるには多くの層を重ねる必要があります。
* **右の図**: Dilated convolution層の視覚化です。層が深くなるにつれて、フィルタが参照する入力の間隔が広がっていくため、同じ層数でも従来の畳み込みよりもはるかに広い入力範囲（受容野）をカバーできます。

### 参照: https://arxiv.org/pdf/1609.03499.pdf